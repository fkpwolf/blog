---
layout: post
title:  "Install Kubernetes by kubeadm"
date:   2017-2-28 14:20:33
categories:
  - cloud
  - install
typora-root-url: ../../../blog
---
### å®‰è£…
æ€»æ–‡æ¡£ <https://kubernetes.io/docs/setup/independent/install-kubeadm/>

å› ä¸º master ä¸Šé¢æœ‰å¤šä¸ª ipï¼Œæ‰€ä»¥ init çš„åŠ äº†å‚æ•°ï¼ˆåæ¥è¯æ˜è¿™å¾ˆæœºæ™ºï¼‰ï¼š

    kubeadm init --api-advertise-addresses 192.168.51.132

èŠ‚ç‚¹åŠ å…¥çš„æ—¶å€™ï¼š
```
[discovery] Failed to request cluster info, will try again: [Get http://192.168.51.132:9898/cluster-info/v1/?token-id=2ffc9b: dial tcp 192.168.51.132:9898: getsockopt: connection refused]
```
æ˜¾ç¤º pods çŠ¶æ€æ—¶çœ‹åˆ° romana æ˜¯ errorã€‚æ¢ä¸€ä¸ª netï¼š`kubectl apply -f https://git.io/weave-kube`ï¼Œè¿™æ¬¡è¿è¡Œ `kubectl get pods --all-namespaces` æ²¡æœ‰é”™è¯¯äº†ã€‚ç„¶åèŠ‚ç‚¹åŠ å…¥æ—¶å°±æ²¡é—®é¢˜äº†ã€‚

å†æ¬¡åŠ å…¥æ˜¯å¦‚æœtokenå¿˜è®°äº†ï¼Œ[è¦é‡æ–°äº§ç”Ÿ](https://stackoverflow.com/questions/40009831/cant-find-kubeadm-token-after-initializing-master/48691158#48691158)ã€‚
```
kubeadm token generate 
kubeadm token create <generated-token> --print-join-command --ttl=0 
```
å¸¸ç”¨å‘½ä»¤ï¼š 
```
kubectl get nodes 
kubectl get pods --all-namespaces 
kubectl describe nodes kubernete-1 
Capacity: 
alpha.kubernetes.io/nvidia-gpu:     0 
cpu:                         4 
memory:                    1883448Ki 
pods:                         110 
```
è¿™å° master ä¸»æœºä¸Šé¢æœ‰ gpuï¼Œä½†æ˜¯æ€ä¹ˆåœ¨ kvm èŠ‚ç‚¹é—´å…±äº«å‘¢ï¼Ÿä¸Šé¢çš„æ–‡å­—è¡¨ç¤ºç°åœ¨å·²ç»æœ‰æ–¹æ³•äº†ï¼Ÿåæ¥æ‰¾äº†å°èŠ‚ç‚¹ï¼Œä¸Šé¢æœ‰ GPUï¼Œä¸è¿‡è¿˜æ˜¯æ˜¾ç¤ºä¸å‡ºæ¥ï¼Œçœ‹æ¥ gpu è¿˜æ˜¯è¦[é¢å¤–é…ç½®äº†](https://insights.ubuntu.com/2017/02/15/gpus-kubernetes-for-deep-learning%E2%80%8A-%E2%80%8Apart-13/)ã€‚

ç°åœ¨å‘ç°è‡ªå·± create çš„ pod çŠ¶æ€éƒ½æ˜¯ creatingï¼Œ 
```
kubectl describe -n kube-system pod kubernetes-dashboard-3203831700-n2wd7
kubectl logs -n kube-system kubernetes-dashboard-7955b65d66-rc6ns 
Error syncing pod, skipping: failed to "SetupNetwork" for "kubernetes-dashboard-3203831700-n2wd7_kube-system" with SetupNetworkError: "Failed to setup network for pod \"kubernetes-dashboard-3203831700-n2wd7_kube-system(5fda6401-fd92-11e6-9832-525400232486)\" using network plugins \"cni\": unable to allocate IP address: Post http://127.0.0.1:6784/ip/e87f63d5e3fe2d7c9a8a50dfd816a46c98b94d1760ed424a972a4eb5df69d369: dial tcp 127.0.0.1:6784: getsockopt: connection refused; Skipping podâ€ 
```
ä»€ä¹ˆé¬¼ï¼Œè¿˜æ˜¯æ‹¿ä¸åˆ° IPï¼Ÿ 
```
[root@kubernete-1 vagrant]# kubectl get pods -n kube-system -o wide | grep weave-net
weave-net-7wwm6                         2/2       Running             2          3h        192.168.121.205   kubernete-1
weave-net-wsx32                         1/2       CrashLoopBackOff    25         3h        192.168.121.79    localhost.localdomain
```
émaster nodeçš„weaveæŒ‚äº†ï¼Œmasteræ˜¯å¥½çš„ã€‚
å¯èƒ½æ˜¯æˆ‘çš„æœºå™¨æœ‰å¤šä¸ªç½‘ç»œè®¾å¤‡çš„åŸå› ã€‚https://github.com/kubernetes/kubernetes/issues/34101 è¿™é‡Œè®¨è®ºäº†å¾ˆå¤šã€‚æ„Ÿè§‰è¿˜ä¸å¦‚ç›´æ¥ç”¨ https://github.com/clifinger/vagrant-centos-kubernetes è¿™ä¸ªã€‚

weave troubleshot æ–¹æ³• https://www.weave.works/docs/net/latest/kube-addon/ ã€‚log åœ¨ dashboard ä¸Šé¢ä¹Ÿå¯ä»¥æŸ¥çœ‹ã€‚
æˆ‘æŠŠä¸Šé¢clifinger çš„åº“é‡Œé¢çš„ install.sh çš„ "$1" == "-master" ä¸€æ®µèƒ¡ä¹±è¿è¡Œäº†ä¸€éï¼š
```
  kubectl -n kube-system get ds -l 'component=kube-proxy' -o json \
  | jq '.items[0].spec.template.spec.containers[0].command |= .+ ["--proxy-mode=userspace"]' \
  |   kubectl apply -f - && kubectl -n kube-system delete pods -l 'component=kube-proxy'
  cp /etc/kubernetes/admin.conf /shared
```
è¿™ä¸ª shell æ˜¯æ ¹æ® json path æ·»åŠ äº†ä¸€ä¸ªå‚æ•°åˆ° kube-proxy å‘½ä»¤ï¼Œåˆ é™¤åç³»ç»Ÿä¼šè‡ªåŠ¨é‡å»º kube-proxy podã€‚ç„¶åå›å®¶åƒé¥­ã€‚ç°åœ¨æ´—å®Œæ¾¡å‘ç°æ‰€æœ‰çš„ pod éƒ½åœ¨ running è¿è¡ŒçŠ¶æ€äº†ã€‚
https://dickingwithdocker.com/deploying-kubernetes-1-4-on-ubuntu-xenial-with-kubeadm/ è¿™é‡Œä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ï¼Œç”¨çš„ç±»ä¼¼çš„æ–¹æ³•è§£å†³ã€‚

### Dashboard

<https://github.com/kubernetes/dashboard> 
```
kubectl describe svc kubernetes-dashboard -n kube-system
Name:                   kubernetes-dashboard 
Namespace:              kube-system 
Labels:                 app=kubernetes-dashboard 
Selector:               app=kubernetes-dashboard 
Type:                   NodePort 
IP:                     10.100.11.211 
Port:                   <unset> 80/TCP 
NodePort:               <unset> 31755/TCP 
Endpoints:              10.44.0.40:9090 
Session Affinity:       None 
No events. 
```
ç„¶åå¯ä»¥å¤–ç½‘è®¿é—® [http://192.168.51.132:31755](http://192.168.51.132:31755/) ï¼ˆè¿™ä¸ªæ˜¯ master ip è¿˜æ˜¯ node IPï¼Ÿï¼‰ï¼ˆç°åœ¨éœ€è¦åŠ  httpsï¼Œè€Œä¸”éœ€è¦ä¿®æ”¹yamlè®©typeä¸º[nodePort](https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/)ï¼‰ 

é»˜è®¤ yaml éœ€è¦æ·»åŠ ä¸€è¡Œï¼š 
```
# ------------------- Dashboard Service ------------------- # 
kind: Service 
apiVersion: v1 
metadata: 
  labels: 
â€‹    k8s-app: kubernetes-dashboard 
  name: kubernetes-dashboard 
  namespace: kube-system 
spec: 
  type: NodePort 
  ports: 
â€‹    - port: 443 
â€‹      targetPort: 8443 
  selector: 
â€‹    k8s-app: kubernetes-dashboard 
```
è¿™æ ·ç«¯å£ç±»å‹å°±æˆäº† node portï¼Œç„¶åå°±å¯ä»¥ä¸éœ€è¦ Proxy ç›´æ¥ç«¯å£è®¿é—®ã€‚ 

dashboard åœ¨ node1èŠ‚ç‚¹ä¸Šï¼ŒæŸå¤©æˆ‘æ²¡æœ‰èµ·è¿™ä¸ªèŠ‚ç‚¹ã€‚è¿™ä¸ªæ—¶å€™ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºäº†ä¸€ä¸ª podï¼Œä½†æ˜¯æœ‰é”™è¯¯ ImagePullBackOffï¼Œnode1 ä¸Šé¢å¯¹åº”çš„ pod çŠ¶æ€ä¸º Unknownã€‚æˆ‘çŒœå¯èƒ½æ˜¯å› ä¸ºç°åœ¨ç³»ç»Ÿå”¯ä¸€å¯ä»¥ç”¨ï¼ˆé»˜è®¤ master æ˜¯ dedicated çš„ï¼‰çš„æ˜¯ä¸€ä¸ª arm rpi èŠ‚ç‚¹ã€‚ç„¶åæˆ‘åŠ å…¥ä¸€ä¸ª amd64èŠ‚ç‚¹ï¼Œç³»ç»Ÿå¡åœ¨è¿™é‡Œï¼Œå¹¶æ²¡æœ‰åœ¨æ–°èŠ‚ç‚¹ä¸Šè¯•ä¸€è¯•ã€‚kubectl delete -n kube-system po kubernetes-dashboard-3203831700-c44b8 åˆ é™¤ä¹‹ï¼ˆæ³¨æ„è¦åŠ  namespaceï¼‰ï¼Œç³»ç»Ÿé©¬ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ dashboard podã€‚ä½†æ˜¯ masterip:NodePort è¿˜æ˜¯æ— æ³•è®¿é—®ï¼Œnodeip:nodeportå¯ä»¥è®¿é—®ã€‚poweroff amd64 nodeã€‚kubectl get nodes é©¬ä¸Šæ˜¾ç¤ºä¸º NotReadyï¼Œä½† dashboard pod è¿˜æ˜¯ Runningï¼Œåªæ˜¯ kubectl describe svc kubernetes-dashboard -n kube-system é‡Œé¢çš„ Endpoints å·²ç»ä¸ºç©ºã€‚è¿‡äº†å¤§æ¦‚ 5 åˆ†é’Ÿï¼ŒçŠ¶æ€æ‰å˜ä¸º Unknownï¼Œç„¶åå¼€å§‹åˆ›å»ºæ–°çš„ podã€‚ 

kubectl delete -f kubernetes-dashboard.yaml åªèƒ½ç”¨è¿™ç§æ–¹å¼åˆ é™¤ï¼Œç‰¹ä¹ˆçš„ã€‚

1.7 æ·»åŠ äº†å¾ˆå¤šå®‰å…¨çš„ä¸œè¥¿ï¼Œç°åœ¨æˆ‘æ‰“å¼€ç½‘é¡µï¼Œå¾—åˆ°é”™è¯¯ï¼š
```
warningUser "system:bootstrap:91765f" cannot list roles.rbac.authorization.k8s.io at the cluster scope. (get roles.rbac.authorization.k8s.io)close
warningUser "system:bootstrap:91765f" cannot list clusterroles.rbac.authorization.k8s.io at the cluster scope. (get clusterroles.rbac.authorization.k8s.io)
```
<https://github.com/kubernetes/dashboard/wiki/Installation> Alternative setup è¿™é‡Œå…³é—­äº†å®‰å…¨è®¾ç½®ã€‚æŒ‰ç…§è¿™é‡Œé‡æ–°éƒ¨ç½²åè¿™æ¬¡ä¸è¦httpsäº†ï¼Œä½†æ˜¯è¿˜æ˜¯æŠ¥ä¸Šé¢ä¸€æ ·çš„é”™è¯¯ã€‚æˆ‘æ—¥ã€‚

<https://github.com/kubernetes/dashboard/wiki/Access-control> è¿™é‡Œçœ‹æ¥è¿˜è¦é…ç½®è®¿é—®çš„ bearer tokenï¼Œä½†æ˜¯è¿™ token æƒé™ç²’åº¦å¾ˆç»†ï¼Œåæ¥ç›´æ¥ç”¨Admin privilegesæ–¹å¼ï¼Œè¿™æ ·é¦–é¡µå°±å¯ä»¥ç‚¹å‡» skip äº†ã€‚èµ‹äºˆ dashboard admin æƒé™ä¹Ÿæ˜¯ç”¨ yaml æ–¹å¼ï¼Œå¾ˆå¥‡ç‰¹çš„äº¤äº’æ–¹å¼ã€‚è¿™ä¸ªæ˜¯ k8s æ‰€è°“çš„â€œ[å£°æ˜å¼ API](https://kubernetes.io/docs/tutorials/object-management-kubectl/declarative-object-management-configuration/)â€ï¼Œå…³æ³¨çš„æ˜¯ whatï¼Œè€Œä¸æ˜¯ how(å‘½ä»¤å¼)ã€‚å£°æ˜å¼ç›¸å½“äºéœ€æ±‚è¯´æ˜ä¹¦ï¼Œæ€ä¹ˆå®Œæˆæˆ‘ä¸å…³å¿ƒï¼›è€Œå‘½ä»¤å¼åˆ™æ˜¯äº‹æ— å·¨ç»†çš„æ§åˆ¶ã€‚

### Sock Demo 
```
[root@kubernete-1 vagrant]# kubectl describe svc front-end -n sock-shop 
Name:                   front-end 
Namespace:              sock-shop 
Labels:                 name=front-end 
Selector:               name=front-end 
Type:                   NodePort 
IP:                     10.97.42.96 
Port:                   <unset> 80/TCP 
NodePort:               <unset> 30001/TCP 
Endpoints:              10.44.0.33:8079 
Session Affinity:       None 
No events. 
```
åªæœ‰ `curl [http://10.44.0.33:8079](http://10.44.0.33:8079/)` è¿™ä¸ªå¯ä»¥ï¼Œmaster_ip:30001ä¸è¡Œå•Šã€‚http://192.168.51.132:30001/ è¿™ç§ host ä¸» ip åœ°å€å¯ä»¥ã€‚ 

micro service demo <https://github.com/microservices-demo/microservices-demo> è¿™ä¸ªå¥½å±Œï¼Œèƒ½åœ¨å¤šä¸ªå¹³å°ä¸Šéƒ¨ç½²ã€‚ 

<https://microservices-demo.github.io/docs/index.html> å‡ ç§éƒ¨ç½²æ–¹æ³•ç½‘ç»œç”¨çš„éƒ½æ˜¯ weaveã€‚ 

è¯¦ç»† DEMO è§£æè§ [Sock Shop Microservices Example](evernote:///view/14040673/s63/0a9b9b4f-0f69-4236-9bf5-eecde22a0b3f/0a9b9b4f-0f69-4236-9bf5-eecde22a0b3f/) 

### åŠ ä¸€ä¸ª Raspberry Pi èŠ‚ç‚¹ è¯•è¯•çœ‹

<https://blog.hypriot.com/getting-started-with-docker-on-your-arm-device/> pirate:hypriot 

å®‰è£…å¾ˆé¡ºåˆ©ã€‚k8s å¯¹ arm æ¶æ„æ”¯æŒçš„å¾ˆå®Œå–„ã€‚ä½†æ˜¯å…¶ weave è¿˜æ˜¯ CrashLoopBackOff çŠ¶æ€ï¼Œå’Œå‰é¢ä¸€æ ·çš„é—®é¢˜ã€‚ç”¨åŒæ ·çš„æ–¹æ³•è¿˜æ˜¯è€é—®é¢˜ï¼Œæ€ä¹ˆåŠã€‚ 
```
2017-03-02T07:41:48.109827025Z 2017/03/02 07:41:48 error contacting APIServer: Get https://10.96.0.1:443/api/v1/nodes: dial tcp 10.96.0.1:443: i/o timeout; trying with fallback: http://localhost:8080 
```
æ„Ÿè§‰å¦‚æœ master æœ‰å¤šä¸ªç½‘å¡ï¼Œéƒ¨ç½²ä¼šæœ‰å„ç§é—®é¢˜ï¼Œå³ä½¿æˆ‘åœ¨ init çš„æ—¶å€™æŒ‡å®šäº† network interface ä¹Ÿä¸€æ ·ã€‚å¥½å§ï¼Œé‡æ–°ç”¨ vagrant åšä¸ªï¼Œè¿™æ¬¡åšå¥½åæˆ‘åˆ é™¤ vagrant nat interfaceï¼Œç„¶åç”¨ virt-manager ç®¡ç†ã€‚ 

ç»“æœè¿™æ¬¡è¿ join éƒ½ä¸è¡Œï¼š 
```
[discovery] Failed to request cluster info, will try again: [Get http://192.168.51.113:9898/cluster-info/v1/?token-id=790772: dial tcp 192.168.51.113:9898: getsockopt: connection refused] 
```
æ¢æˆ `kubeadm init --api-advertise-addresses 192.168.51.113`ï¼Œå†æ¥è¿‡ã€‚ 

è¿™æ¬¡èŠ‚ç‚¹ join å°±å¯ä»¥äº†ï¼Œä¸ç”¨ä¿®æ”¹ kube-proxy çš„é…ç½®ã€‚å¯èƒ½æ˜¯ init æ—¶å€™åŠ äº† ip çš„åŸå› ï¼Œä¹Ÿå¯èƒ½æ˜¯æˆ‘å…ˆå…³äº†é˜²ç«å¢™é…ç½®ã€‚ç½‘ä¸Šçœ‹ centos é—®é¢˜æŒºå¤šã€‚ 
```
[root@k8s-master ~]# netstat -ltpn | grep 9898 
tcp6       0      0 :::9898                 :::*                    LISTEN      15516/kube-discover 
```
å¯ä»¥çœ‹åˆ°9898ç«¯å£ã€‚ä¸ºä»€ä¹ˆå‰é¢çš„è¿è¿™ä¸ªç«¯å£éƒ½æ²¡æœ‰ã€‚ 

rpi è¿˜æ˜¯ä¸€æ ·çš„é—®é¢˜ã€‚æˆ‘å¦å¤–æ‰¾äº†å° Ubuntu çš„ç‰©ç†æœºï¼Œå¯ä»¥åŠ å…¥ï¼Œweave pod ä¹Ÿæ˜¯å¥½å¥½çš„ï¼Œæ²¡æœ‰ä»»ä½•é—®é¢˜ã€‚ä»€ä¹ˆé¬¼å‘¢ï¼Ÿç„¶åæ ¹æ® Limitation é‡Œé¢æç¤ºï¼Œå‘ç°è¿™å° rpi ä¸Šé¢çš„ hostname -i è¿”å› 127.0.0.1ï¼Œæ”¹å¥½åè¿”å›çœŸå®å¤–ç½‘ ipï¼Œä½†é—®é¢˜ä¾æ—§ã€‚åæ¥å‘ç°è¿™èŠ‚ç‚¹ä¸Šé¢æ²¡æœ‰ kube-proxy podï¼Œæ²¡æœ‰è¿™ä¸ªä¹Ÿæœ‰å¯èƒ½ ping ä¸é€š10.96.0.0 çš„ ipã€‚ä¹Ÿæ²¡æœ‰ kube-proxy è¿›ç¨‹ã€‚docker ps åªæœ‰ 2 ä¸ªï¼Œæ­£ç¡®çš„æœ‰ 5 ä¸ªã€‚ä¹Ÿæœ‰å¯èƒ½æ˜¯ weave åˆ›å»ºå¥½äº†å†åˆ›å»º kube-proxyã€‚ç„¶åæˆ‘åœ¨ kubectl -n kube-system get ds -l 'component=kube-proxy' -o json è¿”å›é‡Œé¢å‘ç° kube-proxy "image": "gcr.io/google_containers/kube-proxy-amd64:v1.5.3â€, æ²¡æœ‰ ARM ç‰ˆæœ¬çš„ï¼Ÿ<https://console.cloud.google.com/kubernetes/images/list?location=GLOBAL&project=google-containers> è¿™ä¸ªä¸‹é¢æœ‰ ARM ç‰ˆæœ¬çš„ã€‚ä½†æ˜¯æˆ‘è§‰å¾—è¿™ä¸ª kube-proxy åªæ˜¯åœ¨ master åˆ›å»ºæ—¶ç”¨çš„é…ç½®ï¼Ÿ 

<https://github.com/luxas/kubernetes-on-arm> è¿™ä¸ªmaster å’Œ node éƒ½æ˜¯ ARM ç‰ˆæœ¬çš„ã€‚æ²¡æœ‰æ··åˆçš„äº†ï¼Ÿæœ‰ç‚¹ï¼Œä»–æœ‰å†™ã€‚ 

<https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md> è¿™é‡Œä¹Ÿæœ‰å†™ï¼Œä¸è¿‡ç½‘ç»œåªèƒ½ç”¨ flannelã€‚ä½†æ˜¯æˆ‘çœ‹ arm ä¸Šé¢å·²ç»è‡ªåŠ¨åˆ›å»ºäº† weave çš„ podï¼Œéš¾é“æ˜¯å‡çš„ï¼Ÿ 

æ¥ä¸‹æ¥éƒ¨ç½²ä¸€ä¸ªå°åº”ç”¨åˆ°è¿™ä¸ªèŠ‚ç‚¹ä¸Šã€‚Assigning Pods to Nodes <https://kubernetes.io/docs/user-guide/node-selection/>  

### Play on ARM

2 odroid, install Docker CE 17.03 recommended currently. xenial(Ubuntu 16.04) is currently supported version. Odroid-1t as master node. 

The connection to the server localhost:8080 was refused - did you specify the right host or port? ==> export KUBECONFIG=/etc/kubernetes/admin.conf  

ä½¿ç”¨ weave ç½‘ç»œã€‚join çš„æ—¶å€™æ²¡æœ‰é—®é¢˜ï¼Œä½†æ˜¯åˆ›å»º dashboard çš„æ—¶å€™å‡ºç°é—®é¢˜ï¼š 
```
Warning  FailedCreatePodSandBox  6m               kubelet, odroid    Failed create pod sandbox. 
Warning  FailedSync              6s (x4 over 6m)  kubelet, odroid    Error syncing pod 
Normal   SandboxChanged          5s (x4 over 6m)  kubelet, odroid    Pod sandbox changed, it will be killed and re-created. 
```
å›è¿‡å¤´æ¥çœ‹è¿™ä¸ªå…¶å®æ˜¯å› ä¸º dashboard deployment é‡Œé¢æŒ‡å®šç”¨ amd64 imageï¼Œæ¢æˆ arm å³å¯ã€‚ç°åœ¨ (k8s 1.13.0) æ­å»ºé›†ç¾¤å¾ˆæˆç†Ÿäº†ï¼Œä¸»è¦é—®é¢˜æ˜¯å¾ˆå¤šé•œåƒéƒ½æ²¡æœ‰ arm ç‰ˆæœ¬ã€‚

1.8.4 è¿˜è¦ disbale swapï¼Œç‰¹ä¹ˆçš„ã€‚(vm çš„ swap æ˜¯å…³é—­çš„ï¼Œç›´æ¥å°±å¯ä»¥ç”¨)ç„¶ååœ¨ amd64ä¸Šé¢åˆè¯•äº†ä¸‹ï¼Œè¿˜æ˜¯å„ç§é—®é¢˜ã€‚æˆ‘éœ€è¦å…¨æ–°çš„æœºå™¨æ¥ä¹ˆï¼Ÿè¿˜æ˜¯ kubeadm ä¸å¤ªç¨³å®šï¼Ÿ 

### x64 + arm 

<https://github.com/kubernetes/kubernetes/issues/64234> ä¸€è¿è¡Œkubeletå°±æŠ¥é”™ï¼Œè¦é™åˆ°ä½ç‰ˆæœ¬ã€‚ 

standard_init_linux.go:195: exec user process caused "exec format error" çœ‹æ¥è¿˜æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ä¸å¯¹ã€‚ 

gcr.io/google_containers/kube-proxy-amd64:v1.10.4 è¿™ä¸ªæ˜¯daemonsetåˆ›å»ºçš„ï¼Œä¸èƒ½æ”¹å…¶å®šä¹‰ï¼Œå› ä¸ºå…¶ä¹Ÿè´Ÿè´£å…¶ä»–çš„ x86 kube-proxyåˆ›å»ºã€‚ç›´æ¥ç¼–è¾‘podå®šä¹‰ï¼Œamd64->armã€‚å¥½äº†ã€‚ 

weaveç°åœ¨[æ”¯æŒarm](https://github.com/weaveworks/weave/issues/3276)äº†ï¼Œweaveworks/weave-kube:2.3.0ï¼Œç»Ÿä¸€åœ°å€éƒ½æ˜¯è¿™ä¸ªï¼ŒğŸ‘ã€‚ä¸Šé¢é—®é¢˜æ˜¯kube-proxyçš„bugï¼Ÿhub docker.com å¯ä»¥æ ¹æ®å®¢æˆ·ç«¯ç±»å‹è¿”å›å¯¹åº”Image? quay.io ä¸ä¼šç¼–è¯‘æ‰€æœ‰å¹³å°çš„ï¼Œä½†æ˜¯ docker hub ä¸æ”¯æŒ CI å§ï¼Œéƒ½æ˜¯å¼€å‘è€…è‡ªå·± push çš„ã€‚<https://hub.docker.com/r/giantswarm/tiny-tools/> è¿™ä¸ªæ˜¯automated buildçš„ã€‚ 

æ”¹æˆ gcr.io/google_containers/kube-proxy:v1.10.4ï¼Œè™½ç„¶æµè§ˆå™¨ç›´æ¥è®¿é—®å¯ä»¥æµè§ˆï¼Œä¸è¡Œã€‚ 

è¿™ä¸¤ä¸ªéƒ½æ˜¯daemonsetï¼Œå®ˆæŠ¤è¿›ç¨‹é›†ï¼Œè¿™ä¸ªéƒ½ä¼šè¿è¡Œåœ¨æ¯ä¸ªnode ä¸Šé¢ï¼Ÿ 

Change hostname, é‡å¯åå‘ç°ä¸¤ä¸ªpod éƒ½æ— æ³•èµ·æ¥ï¼Œthe server could not find the requested resource ( pods/log kube-proxy-4kwfd)ï¼Œé‡æ–°kubeadm reset & joinï¼Œå‘ç°åŸæ¥kubectl get nodesæ˜¾ç¤ºéƒ½æ˜¯å’Œhostname å…³è”èµ·æ¥çš„ã€‚ 

### Upgrade

kubeadm upgrade apply stable  <https://kubernetes.io/docs/tasks/administer-cluster/kubeadm-upgrade-1-9/> å®ƒä¼šè®©ä½ å…ˆå‡çº§ kubeadmï¼Œä¼¼ä¹ kubeadm ç‰ˆæœ¬æ˜¯å’Œ k8s ä¸€ä¸€è·Ÿéšçš„ã€‚ Upgrading your Static Pod-hosted control plane to version "v1.9.2"â€¦ æ¯ä¸ª client node ä¹Ÿè¦è‡ªå·±æ‰‹åŠ¨å‡çº§ä¹ˆï¼Ÿ 
```
[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets in turn. 
```
å¯ä»¥å•Šï¼è¿™éƒ½èƒ½å‡çº§ï¼ä¸è¿‡è¿™ kubelets æ˜¯å•¥ä¸œè¥¿ï¼Ÿ 

Join failed: 
```
[preflight] Running pre-flight checks. 
â€‹    [WARNING Hostname]: hostname "k8s-4" could not be reached 
â€‹    [WARNING Hostname]: hostname "k8s-4" lookup k8s-4 on 192.168.51.1:53: no such host 
[preflight] Some fatal errors occurred: 
â€‹    [ERROR CRI]: unable to check if the container runtime at "/var/run/dockershim.sock" is running: exit status 1 
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=â€¦` 
[root@k8s-4 centos]# kubeadm join 192.168.51.11:6443 --token 7c6192.tis6ip34uslrgi0u --discovery-token-ca-cert-hash sha256:9fc0dcbe36a1d3df41941ae63e34bbb3d2078e2e656efbf4beaae96dee275c48 --ignore-preflight-errors cri 
```
<https://mp.weixin.qq.com/s/8bW-Et9WHVgQ8O2JwJarUw> Kubernetes 1.11 ç»ˆäºç­‰åˆ°ä½ ã€‚ä¸»è¦æ˜¯ coredns å’Œ ipvs æ”¯æŒã€‚å…¶ä¸­ coredns å·²ç»ä½œä¸ºé»˜è®¤ç»„ä»¶ã€‚<https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs> è¿™é‡Œæœ‰äº›å¦‚ä½•åˆ‡æ¢çš„æ–¹æ³•ï¼Œè¿˜æŒºå¤æ‚ã€‚ 

å¯¹äº kubeadm å·²ç»å®‰è£…å¥½çš„ï¼Œè¿è¡Œ kubectl edit configmap kube-proxy -n kube-systemï¼Œç„¶åä¿®æ”¹é‡Œé¢ mode: â€œipvsâ€ã€‚ï¼ˆå¦‚æœ ipvs module æ­£å¸¸è¢« loadï¼Œè¿™é‡Œä¿®æ”¹å¯èƒ½ä¸æ˜¯å¿…é¡»çš„ï¼Œå› ä¸º v1.11 é»˜è®¤æ˜¯å¯ç”¨çš„ï¼‰é‡å¯åæŸ¥çœ‹ kube-proxy logï¼š 
```
â€‹    Failed to load kernel module ip_vs_wrr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules 
â€‹    can't determine whether to use ipvs proxy, error: IPVS proxier will not be used because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh ip_vs] 
```
æˆ‘çœ‹ /lib/modules å·²ç» mount äº†å•Šï¼ŒæŒ‰é“ç†æ¥è¯´å®¹å™¨é‡Œé¢çš„ä»£ç ä¼šæ“ä½œ host çš„ kernel moduleï¼Œä½†æ˜¯æ²¡æœ‰å‘ç”Ÿã€‚æˆ‘åœ¨ host é‡Œé¢æ‰‹å·¥ modprobe æ˜¯å¯ä»¥ä½†æ˜¯ä¸ä¼˜é›…ã€‚å®¹å™¨é‡Œé¢è¿è¡Œï¼š 
```
modprobe -v ip_vs_wrr 
insmod /lib/modules/3.10.0-693.11.6.el7.x86_64/kernel/net/netfilter/ipvs/ip_vs_wrr.ko.xz 
modprobe: ERROR: could not insert 'ip_vs_wrr': Exec format error 
```
host ä¸Šé¢è¿è¡Œå°±å¯ä»¥ã€‚å¦‚æœ host å·²ç» modprobe äº†ï¼Œä¸Šé¢ä¹Ÿä¸ä¼šæŠ¥é”™ã€‚[åŸå› åœ¨äº](https://github.com/kubernetes/kubernetes/issues/60869)ï¼šmodprobe in gcr.io/google-containers/kube-proxy-amd64:v1.9.2 is not compiled with CONFIG_MODULE_COMPRESS, so that loading compressed kernel modules on hosts is not supported. è¿™ä¸ªåªåœ¨ centos æ‰æœ‰è¿™ä¸ªé—®é¢˜ï¼Ÿéš¾é“åªæœ‰ centos ç”¨å‹ç¼©çš„å†…æ ¸æ¨¡å—ã€‚åªèƒ½è‡ªå·±å…ˆä¿®æ”¹ host é…ç½®ï¼š 
```
[root@k8s-1 containers]# cat /etc/modules-load.d/ipvs.conf 
ip_vs_rr 
ip_vs_wrr 
ip_vs_sh 
ip_vs 
```
ä½†æ˜¯æœ‰çš„èŠ‚ç‚¹ kube-proxy å¯åŠ¨å‡ºç°ä¸€å †é”™è¯¯ï¼š 
```
Failed to make sure ip set: KUBE-NODE-PORT-LOCAL-TCP bitmap:port inet 1024 65536 0-65535 Kubernetes nodeport TCP port with externalTrafficPolicy=local map[] 0xc420596440 exist, error: error creating ipset KUBE-NODE-PORT-LOCAL-TCP, error: exit status 2 
```
ipset ä¸æ”¯æŒ comment è¯­æ³•ï¼Œè¿™ä¸ªé—®é¢˜ä¸æ˜¯è¯´ 1.11.1 [å·²ç»è§£å†³äº†](https://sealyun.com/post/k8s-ipvs/)ä¹ˆï¼Ÿk8s.gcr.io/kube-proxy-amd64:v1.11.2 è¦æ¢æˆè¿™ä¸ªæœ€æ–°ç¨³å®šç‰ˆæœ¬ï¼Œç›´æ¥ä¿®æ”¹ kubectl edit pod kube-proxy-xxxï¼ˆè¿™ POD çš„å‚æ•°å®šä¹‰åœ¨å“ªé‡Œï¼Ÿ/etc/kubernetes/ ä¸‹é¢æ²¡æœ‰ï¼‰ã€‚è¿™ä¸ª ipvs å¦‚æœå·¥ä½œä¸æ­£å¸¸ï¼Œè¿ weave ä¹Ÿä¸æ­£å¸¸ã€‚ç°åœ¨ ipvsadm -ln ä¼šè¿”å›ä¸€å †ä¸œè¥¿ã€‚ 
```
[centos@k8s-1 ~]$ kubectl log kube-proxy-t8zj4 -n kube-system
log is DEPRECATED and will be removed in a future version. Use logs instead.
I0823 07:38:19.917429       1 server_others.go:183] Using ipvs Proxier.
W0823 07:38:19.941986       1 proxier.go:349] clusterCIDR not specified, unable to distinguish between internal and external traffic
W0823 07:38:19.942013       1 proxier.go:355] IPVS scheduler not specified, use rr by default 
I0823 07:38:19.942160       1 server_others.go:210] Tearing down inactive rules.
I0823 07:38:20.017820       1 server.go:448] Version: v1.11.2
I0823 07:38:20.039200       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072
I0823 07:38:20.039297       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0823 07:38:20.039371       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400
I0823 07:38:20.039411       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600
I0823 07:38:20.042981       1 config.go:202] Starting service config controller
I0823 07:38:20.043016       1 controller_utils.go:1025] Waiting for caches to sync for service config controller
I0823 07:38:20.043549       1 config.go:102] Starting endpoints config controller
I0823 07:38:20.043568       1 controller_utils.go:1025] Waiting for caches to sync for endpoints config controller
I0823 07:38:20.150279       1 controller_utils.go:1032] Caches are synced for service config controller
I0823 07:38:20.150367       1 controller_utils.go:1032] Caches are synced for endpoints config controller
```

1.11 æ”¹ç”¨ coredns åï¼Œpod ä¸€ç›´å¤„äº CrashLoopBackOffã€‚æŒ‰è¿™é‡Œè¯´ <https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/> å¦‚æœæ²¡æœ‰å®‰è£… CNI å‰ï¼Œcoredns ä¼šä¸€ç›´å¤„äº Pending çŠ¶æ€ï¼Œè¿™ä¸ªå¯¹çš„ä¸Šã€‚ä¸€å®‰è£… weave åé©¬ä¸Šå°± CrashLoopBackOffã€‚ 

Kubectl describe è¿”å›è¯´ 0/1 nodes are available: 1 node(s) were not ready. Kubectl log æ²¡æœ‰è¿”å›ã€‚åæ¥åœ¨ /var/log/containers/coredns.log é‡Œé¢å‘ç°åŸæ¥ kubectl logs è¿”å›çš„æ˜¯çœŸæ­£çš„ logï¼š 

standard_init_linux.go:178: exec user process caused "operation not permittedâ€ï¼Œ 

åŸæ¥æˆ‘è£… EFK çš„æ—¶å€™ä¿®æ”¹äº† Docker çš„é…ç½® /etc/sysconfig/dockerï¼š 
`OPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=falseâ€™ `å»æ‰å…¶ä¸­çš„ selinuxã€‚ 

ä»Šå¤©å‘ç° k8s-1 master ä¸Šé¢çš„ weave åå¤é‡å¯ï¼ˆä»¥å‰æ²¡æœ‰è¿‡ï¼Ÿæˆ‘æ²¡åšå•¥ä¿®æ”¹å•Šï¼‰ï¼Œæ—¥å¿—é‡Œé¢æœ‰ï¼š 
```
FATA: 2018/10/12 09:52:45.999563 [kube-peers] Could not get peers: Get https://10.96.0.1:443/api/v1/nodes: dial tcp 10.96.0.1:443: i/o timeout Failed to get peers 
```
ä¸èƒ½è®¿é—® api serverï¼Ÿæˆ‘å‘ç° k8s-1ä¸Šé¢å±…ç„¶æ²¡æœ‰ kube-proxy podï¼Ÿè¿™æ˜¯æ­£å¸¸çš„ä¹ˆï¼Ÿä¸ºä»€ä¹ˆkube-proxyæ²¡æœ‰èµ·æ¥å‘¢ï¼ŸDaemonSetä¸æ˜¯æ¯ä¸ªnodeéƒ½èµ·ä¸€ä¸ªä¹ˆ ï¼Œä½ çœ‹weaveå°±æ˜¯ã€‚ä¸å¯¹ï¼Œrook cephå°±æ²¡æœ‰ã€‚åŸæ¥ç°åœ¨é»˜è®¤DaemonSetæ˜¯ä¸åœ¨Master nodeä¸Šè¿è¡Œçš„ï¼Œweaveæœ‰å¦‚ä¸‹è°ƒåº¦ç­–ç•¥é…ç½®ï¼š 
```
â€‹        "tolerations": [ 
â€‹          { 
â€‹            "operator": "Exists", 
â€‹            "effect": "NoSchedule" 
â€‹          } 
â€‹        ] 
```
kube-proxy yamlä¹Ÿæœ‰ç±»ä¼¼ï¼Œä½†æ˜¯æ ¼å¼å’Œä¸Šé¢ä¸åŒï¼Œå¯èƒ½æ˜¯å‡çº§å¯¼è‡´ã€‚ä¿®æ”¹åç°åœ¨k8s-1ä¸Šé¢ä¹Ÿæœ‰ä¸€ä¸ªkube-proxy pod äº†ã€‚è¯¦ç»†çœ‹ <https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/>ã€‚ 

é¢˜å¤–è¯ï¼šå¦‚æœ api server æ‰€åœ¨çš„ node ä¸Šé¢æ²¡æœ‰ weaveï¼Œapi server å¦‚ä½•å’Œå…¶ä»–èŠ‚ç‚¹ä¸Šçš„ pod é€šä¿¡çŸ¥é“å…¶çŠ¶æ€ï¼Ÿpod ä¹‹é—´é€šä¿¡æ‰éœ€è¦ weaveï¼Œpod çŠ¶æ€æ˜¯ kubelet åé¦ˆçš„ã€‚ 

Dashboard pod ä¸æ˜¯å¤§é‡è®¿é—® api server ä¹ˆï¼ŸDashboard æ˜¯ in-cluster æ–¹å¼è®¿é—®ï¼Œå¹¶æ²¡æœ‰æŒ‡å®š api server ipï¼Œæ‰€ä»¥ï¼Œæˆ‘çŒœæ˜¯é€šè¿‡ kubelet æ¥åšçš„ä»£ç†ï¼Œè¿™é‡Œ kubelet å’Œ proxy åŠŸèƒ½æœ‰é‡åˆã€‚æ²¡æœ‰äº†kube-proxyï¼Œæ€ªä¸å¾—ç°åœ¨ä¸èƒ½é€šè¿‡<http://k8s-1:NodePort>æ¥è®¿é—®serviceäº†ã€‚ 

ä¸ºä»€ä¹ˆ dashboard pod ä¸€ç›´å¯ä»¥è®¿é—®api serverï¼Œè€Œ istio injectorå°±å¿…é¡»é€šè¿‡ kube-proxy è®¿é—® webhook å‘¢ï¼ŸIstio å¯ä»¥è¿è¡Œåœ¨ cluster å¤–éƒ¨ï¼Ÿ 

<https://kubernetes.io/docs/concepts/architecture/master-node-communication/> è¿™ä¸ªä¼¼ä¹ä¹Ÿæ²¡è®²æ¸…æ¥šã€‚ 

ä»Šå¤©å‘ç° etcd æ²¡æœ‰èµ·æ¥ï¼Œapi server container logï¼š `Unable to create storage backend: config. err (dial tcp 127.0.0.1:2379: connect: connection refused) ` æ²¡æœ‰ etcd logï¼Œdocker ps æ²¡æœ‰ etcdï¼Œæ­£å¸¸æƒ…å†µä¸‹æ˜¯æœ‰ä¸ªçš„ã€‚ journalctl --no-pager -u kubelet -n 4000 ä¼¼ä¹æ˜¯ç£ç›˜å‹åŠ›ï¼š
```
eviction_manager.go:156] Failed to admit pod weave-net-gb5l5_kube-system(2bec0ea4-b667-11e8-86d6-aa90b3cbae01) - node has conditions: [DiskPressure] 
```
åˆ é™¤äº›ä¸œè¥¿åå°±æ­£å¸¸äº†ï¼Œä¸çŸ¥é“æ˜¯ä¸æ˜¯æ ¹æœ¬åŸå› ã€‚æˆ–è€…æ˜¯ etcd å†™å…¥å‡ºç°é—®é¢˜ï¼Œå‰é¢æˆ‘æœ‰å¼ºåˆ¶åˆ é™¤ pvcã€‚ä¸ç®¡æ€æ ·ï¼Œæƒ…å†µæœ‰ç‚¹å“äººã€‚ 

kube-controller-manager èµ·ä¸æ¥ï¼Œfailedï¼Œæ— æ—¥å¿—ã€‚ 
```
[centos@k8s-1 containers]$ kubectl describe pod kube-controller-manager-k8s-1 -n kube-system 
The node was low on resource: ephemeral-storage. Container kube-controller-manager was using 60688Ki, which exceeds its request of 0. 
```
å…¶å‚æ•°é…ç½®åœ¨ï¼š 
```
â€‹    /etc/kubernetes/manifests/kube-controller-manager.yaml 
```
æ·»åŠ å€¼ ephemeral-storage: 100Miã€‚ä¿®æ”¹å systemctl restart kubelet ä¸èµ·ä½œç”¨ï¼Œreboot node ä¹Ÿæ²¡ç”¨ã€‚kubectl edit pod é‡Œé¢æ˜¾ç¤ºçš„è¿˜æ˜¯è€çš„å€¼ã€‚æˆ‘ç­‰äº†å¤§æ¦‚åŠä¸ªå°æ—¶å kube-controller-manager-k8s-1 æ­£å¸¸å¯åŠ¨äº†ï¼Œé…ç½®ä¹Ÿæ›´æ–°äº†ã€‚ä¸çŸ¥é“è¿™ä¸ªæ˜¯ kubelet è¿˜æ˜¯ kubeadm çš„æœºåˆ¶ã€‚ç°åœ¨åˆä» 200M å¢åŠ åˆ° 300Mï¼Œè¿™ä¸ªæ§åˆ¶å™¨åº”è¯¥æ˜¯æ— çŠ¶æ€çš„å•Šï¼Œä¸ºä»€ä¹ˆå è¿™ä¹ˆå¤šèµ„æºã€‚ 

å…³äºephemeral-storageï¼Œ<https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>. è¿™ä¸ªã€çŸ­æš‚ã€å­˜å‚¨ä¿å­˜çš„æ˜¯ kubeletâ€™s root directory (/var/lib/kubelet by default) and log directory (/var/log)ï¼Œè¿™ä¸ªè¦è¿™ä¹ˆå¤§ï¼Ÿ 

kube-controller-manager èµ·ä¸æ¥ä¼šå¯¼è‡´ç³»ç»Ÿä¸­ Statefulset åˆ›å»ºäº†ï¼Œä½†æ˜¯ pod/pvc éƒ½æ²¡æœ‰åˆ›å»ºï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•é”™è¯¯ï¼Œä½†æ˜¯ç•Œé¢ä¸Šæ²¡æœ‰ä»»ä½•æœ‰æ•ˆçš„è­¦å‘Šä¿¡æ¯ã€‚è¿™ä¸ªå¯ä»¥åšæˆ HA ä¹ˆï¼Ÿå¦åˆ™å½±å“å¤ªå¤§ã€‚æˆ–è€…å‘Šè­¦é‚£é‡Œèƒ½é‡ç‚¹ç›‘æ§ kube-system é‡Œé¢çš„ podã€‚ 

### Inside

How kubeadm Initializes Your Kubernetes Master <https://www.ianlewis.org/en/how-kubeadm-initializes-your-kubernetes-master>

![kubeadm-inside](/images/2017/kubeadm-inside.png)

### Troubleshooting

Dmesg åå¤æ˜¾ç¤ºï¼š 
```
[959776.150708] IPVS: Creating netns size=2048 id=77 
[959776.353964] weave: port 8(vethweple4c80f4) entered blocking state 
[959776.356498] weave: port 8(vethweple4c80f4) entered disabled state 
[959776.390464] device vethweple4c80f4 entered promiscuous mode 
[959776.456718] IPv6: ADDRCONF(NETDEV_UP): vethweple4c80f4: link is not ready 
[959776.504730] IPv6: ADDRCONF(NETDEV_CHANGE): vethweple4c80f4: link becomes ready 
[959776.507472] weave: port 8(vethweple4c80f4) entered blocking state 
[959776.509741] weave: port 8(vethweple4c80f4) entered forwarding state 
[959776.757383] IPv6: eth0: IPv6 duplicate address fe80::3c75:1eff:fea9:3828 detected! 
[959777.369007] weave: port 8(vethweple4c80f4) entered disabled state 
[959777.405632] device vethweple4c80f4 left promiscuous mode 
[959777.407870] weave: port 8(vethweple4c80f4) entered disabled state 
```
è¿™ä¸ªæ˜¯å› ä¸º IPVS è§„åˆ™å˜åŒ–çš„åŸå› ï¼Ÿ å…ˆ[ç¦ç”¨ ipv6 on centos](https://www.techrepublic.com/article/how-to-disable-ipv6-on-linux/): 
```
sysctl -w net.ipv6.conf.all.disable_ipv6=1 
sysctl -w net.ipv6.conf.default.disable_ipv6=0 
```
ä½†è¿™ä¸ªä¼¼ä¹æ˜¯ openvswitch kernel module å¿…é¡»çš„ï¼Œé‡å¯åè¿˜æ˜¯èƒ½çœ‹åˆ°ä¸Šé¢æ—¥å¿—ã€‚å…¶å®è¿™ä¸ª eth0 æ˜¯ä¸å­˜åœ¨çš„ä¸€ä¸ª interfaceï¼Œåªåœ¨ /etc/sysconfig/network-scripts é‡Œé¢å­˜åœ¨ï¼Œip addr é‡Œé¢æ²¡æœ‰ï¼Œä¸çŸ¥é“ä¸ºä½•è¿™é‡Œä¼šå‡ºç°ã€‚åˆ é™¤åé—®é¢˜ä¾ç„¶ï¼Œ[è¿™é‡Œ](http://www.ioremap.net/2015/01/31/docker-and-ipv6-eth0-ipv6-duplicate-address-detected/)è¯´è¿™æ˜¯ docker ä¸Šé¢ä¸€ä¸ªæœ‰åçš„é—®é¢˜ï¼Œå’Œ DAD stands for Duplicate Address Detection æœ‰å…³ç³»ã€‚Fedora 28 ä¸‹é¢æ²¡æœ‰è¿™ä¸ªé—®é¢˜ã€‚ 

[kubelet bootstrap æµç¨‹](https://lingxiankong.github.io/2018-09-18-kubelet-bootstrap-process.html)ï¼Œé¦–å…ˆä¼šå…ˆä» master node é‚£é‡Œæ‹¿åˆ° tokenï¼Œç„¶å work node ä¼šæ ¹æ® token æ‹¿åˆ°è¯ä¹¦ï¼Œå°”åè®¿é—®éƒ½ä¼šæ ¹æ®è¯ä¹¦æ¥è®¿é—® API Serverã€‚æœ‰äº† token å master node æ‰ä¼šä¿¡ä»» work node ç„¶åé¢å‘è¯ä¹¦ã€‚ 
```
openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem 
â€‹        Validity 
â€‹            Not Before: Jul 12 04:29:00 2018 GMT 
â€‹            Not After   : Jul 12 04:29:00 2019 GMT 
â€‹        Subject: O=system:nodes, CN=system:node:k8s-2-new 
```
è¿™ä¸ªæœ‰æ•ˆæœŸæœ‰ä¸€å¹´ï¼Œè¿™ä¹ˆé•¿ï¼åˆ°æœŸä¼šè‡ªåŠ¨ renew ç»­ç­¾ï¼Œè¿™ç§æœ‰äº† token åéƒ½æ˜¯è‡ªåŠ¨çš„ï¼Œå®‰å…¨æ€§æ¯”åªç”¨ token è¦é«˜ä¹ˆï¼Ÿ 

[ä½¿ç”¨ kubeadm æ­å»º multi-master k8s é›†ç¾¤æ€»ç»“](https://lingxiankong.github.io/2018-06-22-multi-master-k8s.html), æœ‰æä¾› Ansibleã€‚ 

è¯ä¹¦è¿‡æœŸçš„é—®é¢˜ï¼š
```
2018/12/07 10:57:09 Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: Get https://192.168.1.140:6443/version: x509: certificate has expired or is not yet valid
```
å¥‡æ€ªçš„æ˜¯é›†ç¾¤å†…éƒ¨çš„ dashboard æ­£å¸¸è¿è¡Œï¼Œin-cluster è®¿é—® api server ä¸æ ¡éªŒè¯ä¹¦ï¼Ÿè¿™ä¸ªé—®é¢˜æœ‰ä¸ª [kubeadm issue](https://github.com/kubernetes/kubeadm/issues/581) è§£å†³ï¼Œå°±æ˜¯å¤ªéº»çƒ¦äº†ã€‚å¯ä»¥è¯•è¯• [è¿™ä¸ª stackoverflow](https://stackoverflow.com/questions/46360361/invalid-x509-certificate-for-kubernetes-master) çš„æ–¹æ³•ã€‚