---
layout: post
title:  "GlusterFS"
date:   2018-9-11 14:20:33
typora-root-url: ../../../blog
---

### å®‰è£… Heketi

ä»Šå¤©æƒ³åˆ°å¦‚æœ 16GB å†…å­˜æœºå™¨å®‰è£…è¿™ä¸ªï¼Œä¼šä¸ä¼šæ¯”è¾ƒçœèµ„æºã€‚ è¿™ä¸ªæ¯” Ceph è½»ï¼Œå®‰è£…ä¹Ÿå®¹æ˜“ï¼Œä½†åªæ”¯æŒå—è®¾å¤‡ï¼Œå¯¹è±¡å’Œæ–‡ä»¶å­˜å‚¨éƒ½ä¸æ”¯æŒã€‚å¥½å¤„åœ¨ k8s storage æ”¯æŒèŒƒå›´å†…ã€‚
<https://docs.gluster.org/en/latest/Install-Guide/Overview/>
```
gluster volume info
Connection failed. Please check if gluster daemon is operational. 
systemctl start&enable glusterd.service 
[glusterd.c:1324:glusterd_init_var_run_dirs] 0-management: Unable to create directory /run/gluster/snaps ,errno = 13 [Permission denied] 
==>setenforce 0 
gluster volume create gv0 gluster-1:/export/sdb1/brick 
```
[Heketi](https://github.com/heketi/heketi) RESTful based volume management framework for GlusterFSï¼Œç‹¬ç«‹è¿è¡Œï¼Œæœ¬èº«æœ‰æ•°æ®åº“ç»´æŠ¤é›†ç¾¤çš„æ‹“æ‰‘ç»“æ„ï¼Œæœ‰ç‚¹åƒ ceph monitorï¼Œä½†æ˜¯åªæ˜¯æ§åˆ¶é¢æ¿ï¼Œæ•°æ®é¢æ¿æ˜¯ç›´æ¥è¿å„ä¸ª GlusterFS èŠ‚ç‚¹ï¼Œk8s glusterfs plugin é€šè¿‡è¿™ä¸ªæ¥å’Œ GlusterFS äº¤äº’ã€‚[è¿™ä¸ªä¸­æ–‡](https://www.cnblogs.com/breezey/p/8849466.html)æŒ‡å¯¼ä¸é”™ï¼ŒHeketi æ˜¯é€šè¿‡ ssh æ¥è¿œç¨‹ç®¡ç†å„ä¸ª GlusterFS èŠ‚ç‚¹ï¼Œå…¶æœ¬èº«æ˜¯ä¸€ä¸ª client-server æ¶æ„ï¼Œclient é€šè¿‡ REST API è¿æ¥åˆ° server æ¥åˆ›å»º GlusterFS Cluster å’Œå·ï¼Œè€Œ server åˆ™è¿æ¥çœŸæ­£çš„ GlusterFSã€‚

å®‰è£…æ–¹å¼æœ‰å¤šç§ï¼Œå¯ä»¥å®‰è£…åœ¨ k8s å†…éƒ¨ï¼Œä½†æ˜¯çœ‹äº†ä¸‹è¿˜æŒºéº»çƒ¦ï¼Œæˆ‘è¿˜æ˜¯å•ç‹¬èµ·ä¸ªè¿›ç¨‹å§ã€‚ åˆ›å»ºå·ï¼š
```
failed to create volume: failed to create volume: Failed to allocate new volume: No space 
./heketi-cli --server http://vm1:8080/ volume create --size=1 
Error: Failed to allocate new volume: No space 
./heketi-cli --server http://vm1:8080/ volume create --size=1 --durability=none 
```
è¿™ç§å¯ä»¥ï¼Œå‡å‡çš„ã€‚
```
$ ./heketi-cli --server http://192.168.1.121:8080/ volume delete 83ebfa75567b8b2138dd7df53a53c947 
Error: Unable to get snapshot information from volume vol_83ebfa75567b8b2138dd7df53a53c947: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain 
```
è¿™ä¸ªå‘½ä»¤æŠ¥é”™å¾ˆè¯¦ç»†ï¼Œssh æœ‰ç‚¹æã€‚ç°åœ¨æˆ‘æŠŠ heketi server å®‰è£…åœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šï¼Œç„¶å `ssh-copy-id -i heketi_key.pub new_node` æ¥æ‹·è´è¯ä¹¦åˆ°æ–°èŠ‚ç‚¹ä¸Šï¼Œè¿™æ · heketi å°±å¯ä»¥ç”¨è¯ä¹¦å…å¯†ç™»å½•æ‰€æœ‰ glusterfs èŠ‚ç‚¹ã€‚ 
```
fandeMac:bin fan$ ./heketi-cli --server http://vm1:8080/ device add --name="/dev/vdb1" --node "5ef3f8c98a2e7456db1a05f7c60088e1" 
Error: WARNING: xfs signature detected on /dev/vdb1 at offset 0. Wipe it? [y/n]: [n] 
  Aborted wiping of xfs. 
  1 existing signature left on the device. 
```
è¦å¯¹/dev/vdb é‡æ–°åˆ†åŒºï¼Œç»Ÿä¸€çº³å…¥ heketi ç®¡ç† 

    ./heketi-cli --server http://vm1:8080/ topology info 

ç°åœ¨å¯ä»¥æ˜¾ç¤ºçœŸå®çš„å¤§å°äº†ã€‚--durability=none åˆ›å»ºçš„ volume ä¹Ÿå¯ä»¥åˆ é™¤äº†ã€‚ 

### Kubernetes

å› ä¸ºé»˜è®¤ replica æ˜¯å¼€å¯çš„ï¼Œåªèƒ½ä½¿ç”¨ Distribute volume: volumetype: none è¿™ç§æ¨¡å¼äº†ï¼ŒStorageClass ä¹Ÿæ”¯æŒè¿™ä¸ªã€‚ 

ç°åœ¨ PV å¯ä»¥åˆ›å»ºäº†ï¼Œä½†æ˜¯ Pod æŠ¥é”™ï¼šmount: unknown filesystem type â€˜glusterfsâ€™exe ï¼Œè§£å†³åŠæ³•ï¼šèŠ‚ç‚¹ä¸Šå®‰è£…  `apt install glusterfs-client |  yum install glusterfs-fuse` ã€‚ç°åœ¨å¯ä»¥æ­£å¸¸ mount äº†ï¼Œä¸€åˆ‡æ­£å¸¸ã€‚ 

ä¸ºä»€ä¹ˆ GlusterFS å°±å¯ä»¥è¿™æ ·ï¼Œceph å´è¦ä¸€ä¸ªå•ç‹¬çš„ podï¼Ÿå¯èƒ½æ˜¯å› ä¸º gluster åªè¦ mount è¿™ä¸ªå¸¸è§å‘½ä»¤å°±å¤Ÿäº†ï¼ˆmount åˆ™éœ€è¦ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿçš„æ’ä»¶ï¼‰ï¼Œè¿™ä¸ªåœ¨å®¹å™¨ä¸­å°±å¯ä»¥è°ƒç”¨ï¼Ÿè€Œ ceph åˆ™éœ€è¦ rbd ç­‰ä¸“å±å‘½ä»¤ï¼Œè¿™ä¸ªå³ä½¿å®‰è£…åœ¨èŠ‚ç‚¹ä¸Šå®¹å™¨ä¸­ä¹Ÿæ— æ³•è°ƒç”¨ï¼Ÿæœ‰ä¸ª [feature](https://github.com/kubernetes/features/issues/278) Kubernetes can run mount utilities in pods instead on the hostï¼Œè¿™æ ·èŠ‚ç‚¹ä¸Šå°±ä¸ç”¨å®‰è£…ä»»ä½•é¢å¤–çš„åŒ…äº†ã€‚ä½†æ˜¯ä¸ºä»€ä¹ˆ Ceph è¿å®‰è£…é¢å¤–çš„åŒ…åéƒ½ä¸èƒ½å·¥ä½œï¼ŸçœŸæ˜¯è¾£é¸¡å•Šã€‚ å¦å¤–ä¸€ä¸ªåŸå› æ˜¯ ceph æ–‡ä»¶ç³»ç»Ÿ mount æ˜¯åœ¨å†…æ ¸å±‚ï¼Œè€Œ glusterfs æ˜¯**ç”¨æˆ·å±‚**ã€‚

ä½†æ˜¯åœ¨ä½¿ç”¨ volume çš„ k8s èŠ‚ç‚¹ä¸Šçœ‹ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼ˆdmesg, lsblk, lsmod, df -hâ€¦ï¼‰ï¼Œåæ¥å‘ç°ï¼š 
```
ubuntu@vm1:~$ mount | grep gluster 
192.168.1.121:vol_a2f74b885204c095191dbac9e8c8bd51 on /var/lib/kubelet/pods/7d812851-b62e-11e8-ae0a-428a7547569e/volumes/kubernetes.io~glusterfs/pvc-7cefa502-b62e-11e8-ae0a-428a7547569e type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) 
```
å› ä¸ºæˆ‘ç”¨çš„æ˜¯ Distribute volumeï¼Œæ‰€ä»¥è¿™é‡Œåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ã€‚è¿™ä¸ªæ˜¯åœ¨ç”¨æˆ·ç©ºé—´ mountï¼Œæ²¡æœ‰å†…æ ¸å‚ä¸ï¼Œä¼šæ¯” Ceph ç¨³å®šç‚¹ä¹ˆï¼Ÿè¿™ç§ mount æ–¹å¼å’Œ NFS æˆ–è€… Samba æœ‰ç‚¹åƒã€‚ 

æœ€åçš„ [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs) å®šä¹‰ï¼š 
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://192.168.51.11:8080"
  volumetype: none
```
Distribute volume: volumetype: noneï¼Œè¡¨ç¤ºæ²¡æœ‰å‰¯æœ¬ï¼Œå› ä¸ºæˆ‘åªæœ‰ä¸€ä¸ª glusterfs èŠ‚ç‚¹ã€‚ æ²¡æœ‰å¼€å¯è®¤è¯ï¼Œæ‰€ä»¥ä¸€åˆ‡éƒ½å¾ˆç®€å•ï¼Œä¸éœ€è¦ secretï¼Œè€Œä¸” heketi é»˜è®¤æ˜¯å…³é—­è®¤è¯çš„ã€‚ 

glusterfs provider ä¹Ÿä¼šåœ¨ k8s ä¸­åˆ›å»ºä¸€ä¸ª `gluster-dynamic-<claimname>` æœåŠ¡ï¼Œè¿™ä¸ªæœåŠ¡ç›´æ¥ç»‘å®šå…¨éƒ¨çš„ glusterfs èŠ‚ç‚¹ï¼Œè¿™ä¸ªè´Ÿè½½å‡è¡¡å¾ˆæœ‰ç”¨ï¼Œä½†æ˜¯ç»™è°ç”¨å‘¢ï¼Ÿä¸€èˆ¬ Pod éƒ½æ˜¯ç›´æ¥ mount pvã€‚å¦‚ä½•ç”¨ï¼Ÿä½¿ç”¨ fuse.glusterfs ç›´æ¥ mountï¼Ÿ 

Heketi èƒ½ç®¡ç†å¤šä¸ªclusterï¼Œå¤šä¸ªnodeï¼Œå¹¶éåªæ˜¯ç®€å•çš„ REST API æä¾›ï¼Œè€Œæ˜¯ä¸€ä¸ªæœ‰è‡ªå·±æ•°æ®åº“çš„å¯ä»¥ç®¡ç†å·çš„åˆ†é…çš„æœåŠ¡ã€‚è¿™æ · k8s æ‰å¯ä»¥åŠ¨æ€åˆ†é… Volumeã€‚å¦‚æœæœ‰å¤šä¸ª clusterï¼Œè€Œ StorageClass æ²¡æœ‰æŒ‡å®šï¼Œåˆ™ heketi ä¼šéå†å¤šä¸ª cluster æ‰¾åˆ°åˆé€‚çš„åœ°æ–¹ã€‚ 
```
./heketi-cli --server http://vm1:8080 topology info
./heketi-cli --server http://vm1:8080 volume list
./heketi-cli --server http://vm1:8080 volume delete foo
./heketi-cli --server http://vm1:8080 volume create --size=1
```
heketi è¿™ä¹ˆå¤šæ¥å£ï¼Œå¦‚æœèƒ½æä¾› web ui å°±æ–¹ä¾¿å¤šäº†ï¼Œä¸è¿‡ heketi ä¸Šé¢æ²¡äººç›¸åº”ï¼Œåªæœ‰è¿™ä¸ª <https://github.com/orachide/heketi-ui> ã€‚heketi ä¹Ÿä¸æä¾›ç›‘æ§ä¿¡æ¯å§ï¼Œè¿™ä¸ªåº”è¯¥å±äºå•†ä¸šäº§å“èŒƒå›´äº†ã€‚

å¸¸é©»è¿è¡Œå¯ä»¥ç”¨`nohup ./heketi --config=./heketi.json &`ï¼Œå¦‚æœè¦è½¬æ¢ä¸º systemd æœåŠ¡ï¼Œå¯ä»¥å‚è€ƒè¿™é‡Œ <https://github.com/heketi/heketi/blob/master/extras/systemd/heketi.service>ã€‚

<https://www.redhat.com/zh/technologies/storage/gluster> è¿™ä¸ªåŸæ¥æ˜¯çº¢å¸½çš„æŠ€æœ¯ã€‚è¿™ä¸¤ä¸ªäº§å“åœ¨ centos/fedora ä¸Šé¢éƒ½å¯ä»¥ç›´æ¥å®‰è£…ï¼Œé€‚é…æ›´å¥½ã€‚ 

**æ€»çš„æ¥è¯´**ï¼Œæ¯” ceph ç®€å•å¤šäº†ï¼šserver é…ç½®ç®€å•ï¼Œk8s é‡Œé¢ä¹Ÿç®€å•ï¼Œç”³è¯·åˆ é™¤ pv éƒ½å¾ˆå¿«ï¼Œä¹Ÿä¸éœ€è¦å…ˆæ secretã€‚heketi ä¹Ÿä¸º k8s åšäº†é€‚é…ã€‚è€Œ ceph ç°åœ¨è§‰å¾—æ›´å¤šæ˜¯åœ¨ openstack ç”Ÿæ€åœˆé‡Œé¢ã€‚ 

Heketi å±…ç„¶åœ¨ centos é‡Œé¢æ²¡åŒ…å®‰è£…ï¼Œfedora æœ‰ï¼Œubuntu æ²¡æœ‰ã€‚ç®€å•æ–¹æ³• `sudo nohup ./heketi --config=./heketi.json > log &`ã€‚æ„Ÿè§‰ fedora æ‰æ˜¯å¥½ï¼Œcentos 7 kernel æœ‰ç‚¹è€ï¼Œcloud image ä¹Ÿå¤§å¤šäº†ã€‚ 

ä»Šå¤©å‘ç° Mongodb èµ·ä¸æ¥ï¼Œæ—¥å¿—æœ‰ï¼š 

    Error executing 'postInstallation': Group '2001' not found 

çœ‹äº†åŠå¤©ï¼Œå‘ç°æ˜¯ GlusterFS çš„é—®é¢˜ã€‚<https://github.com/helm/monocular/issues/419> <https://github.com/helm/charts/issues/2488> æ–°çš„ mongodb-4.0.6 chart å·²ç»ä¿®å¤ã€‚ [åŸå› åœ¨è¿™é‡Œ](https://github.com/bitnami/bitnami-docker-drupal/issues/88)ï¼Œå¥½åƒæ˜¯æ–‡ä»¶ç³»ç»Ÿæƒé™çš„é—®é¢˜ã€‚

### Inside

å…·ä½“å­˜å‚¨ç±»å‹åˆ†ç±» <https://docs.gluster.org/en/v3/Quick-Start-Guide/Architecture/>  Distributed, Replicated, Distributed Replicated, Stripedã€‚ å›¾æ–‡å¹¶èŒ‚ï¼Œå†™çš„ä¸é”™ã€‚

å’Œ ceph ä¸€æ ·çš„åœ¨äºå¯¹äºç£ç›˜æ ¼å¼ç”¨çš„éƒ½æ˜¯ xfsï¼Œæ€»ç£ç›˜åˆ†åŒºæ‹“æ‰‘æ˜¯ LVMï¼Œä¸‹é¢ LV åˆ†åŒºæ˜¯å„ä¸ª xfs æ ¼å¼åˆ†åŒºã€‚LVM æ˜¯ç®¡ç†å¤šä¸ªç£ç›˜çš„å·¥å…·ã€‚ 

åŸºäº GlusterFS å®ç° Docker é›†ç¾¤çš„åˆ†å¸ƒå¼å­˜å‚¨ <https://www.ibm.com/developerworks/cn/opensource/os-cn-glusterfs-docker-volume/index.html> "GlusterFS å€ŸåŠ© TCP/IP æˆ– InfiniBand RDMA ç½‘ç»œå°†ç‰©ç†åˆ†å¸ƒçš„å­˜å‚¨èµ„æºèšé›†åœ¨ä¸€èµ·ï¼Œä½¿ç”¨å•ä¸€å…¨å±€å‘½åç©ºé—´æ¥ç®¡ç†æ•°æ®ã€‚GlusterFS åŸºäºå¯å †å çš„ç”¨æˆ·ç©ºé—´è®¾è®¡ï¼Œå¯ä¸ºå„ç§ä¸åŒçš„æ•°æ®è´Ÿè½½æä¾›ä¼˜å¼‚çš„æ€§èƒ½ã€‚" 

ä½†æ˜¯ä¹Ÿè¦å¤šä¸ªæœºå™¨ï¼Œè¿™ä¸æ˜¯ç°æœ‰é¸¡è¿˜æ˜¯è›‹çš„é—®é¢˜äº†ã€‚ovirt ä¹Ÿæ”¯æŒè¿™ä¸ªï¼Œæ¯•ç«Ÿéƒ½æ˜¯çº¢å¸½ä¸€å®¶çš„ã€‚

ã€GlusterFS æ¶æ„ä¸­æ²¡æœ‰å…ƒæ•°æ®æœåŠ¡å™¨ç»„ä»¶ï¼Œè¿™æ˜¯å…¶æœ€å¤§çš„è®¾è®¡è¿™ç‚¹ï¼Œå¯¹äºæå‡æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½ã€å¯é æ€§å’Œç¨³å®šæ€§éƒ½æœ‰ç€å†³å®šæ€§çš„æ„ä¹‰ã€‚ã€å’Œ Ceph ç±»ä¼¼ï¼Œä½†æ˜¯ Ceph æœ‰ monitorï¼Œè¿™ä¸ªå¦‚ä½•ç®¡ç†æ•°æ®çš„åˆ†å¸ƒï¼Ÿæ ¹æ®å·çš„ç±»å‹æ¥ï¼Ÿæ¯”å¦‚å¦‚æœæ˜¯ Replica volume ç±»å‹çš„ï¼Œè‚¯å®šæ˜¯æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰äº†ã€‚ ä½†æ˜¯ heketi å‘¢ï¼Ÿè¿™ä¸ªä¸æ˜¯å•ç‚¹æ•…éšœä¹ˆï¼Ÿå¦‚æœ heketi æŒ‚äº†ï¼Œvolume çº§åˆ«çš„ meta ä¿¡æ¯åº”è¯¥è¿˜åœ¨ï¼Œè¿™æœ¬èº«å°±ç”± glusterfs èŠ‚ç‚¹ç»´æŠ¤ï¼Œcluster/node çº§åˆ«çš„å¯èƒ½å°±ä¸å­˜åœ¨äº†ã€‚

å¦‚æœä¸€ä¸ªå·åˆ†å¸ƒåœ¨å¤šä¸ªç£ç›˜ä¸Š(replica >2)ï¼Œsamba å®¢æˆ·ç«¯è¯»å–åªèƒ½è®¾ç½®ä¸€ä¸ª IP å§ï¼Œå¦‚ä½•æé«˜æ€§èƒ½ï¼Ÿ 

Volume é‡Œé¢æœ‰å±æ€§ï¼š 
```
heketi-cli topology info  
Mount: 192.168.51.130:vol_573e4b4f72cecb867cbddb75260dab34 
Mount Options: backup-volfile-servers=192.168.51.187 
```
ä½†æ˜¯ k8s mount node ä¸Šé¢æ²¡æœ‰ä½¿ç”¨è¿™ä¸ª backup serverï¼š 
```
[centos@k8s-2-new ~]$ mount | grep gluster 
192.168.51.130:vol_573e4b4f72cecb867cbddb75260dab34 on /var/lib/kubelet/pods/7f062c23-f906-11e8-ac1b-3eede03e70f1/volumes/kubernetes.io~glusterfs/pvc-7ed0a1a3-f906-11e8-ac1b-3eede03e70f1 type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) 
```
å¦‚ä½•è§¦å‘æ¥æµ‹è¯•è¿™ä¸ªå‰¯æœ¬ä¿æŠ¤æœºåˆ¶ï¼Ÿ[Setting Up GlusterFS Client](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Clients/)ï¼Œæœ‰ä¸ªå‚æ•° backupvolfile-serverï¼Œå½“ä¸» server æŒ‚æ‰ï¼Œä¼šè¿æ¥ç¬¬äºŒä¸ªï¼Œå’Œæ€§èƒ½æ²¡å…³ç³»ï¼ŒHA çš„ã€‚ k8s glusterfs provider bug? 

å¦‚æœåœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œ `ps aux|grep gluster`ï¼Œä¼šå‘ç°å¯¹äºæ¯ä¸ª PVCï¼ˆå¯¹ gluster æ˜¯ä¸€ä¸ª volumeï¼‰ éƒ½è¿è¡Œäº†ä¸€ä¸ª glusterfsd è¿›ç¨‹ï¼Œå¦‚æœåœ¨ k8s ä¸‹é¢ï¼Œå¯ä»¥åšæˆä¸€ä¸ª podã€‚

### å®¹é‡ç¿»å€çš„é—®é¢˜

ä¸€ä¸ª PVï¼Œåœ¨ gluserfs server ä¸Šé¢æœ‰ä¸¤ä¸ªç£ç›˜ vg_tmeta & vg_tdataï¼Œå¯¼è‡´å ç”¨å®¹é‡ç¿»äº†å€ã€‚gluster volume info è¿è¡Œçœ‹ Type: Distributeã€‚heketi-cli topology infoï¼š 
```
Devices:
        Id:af07f63706290336a9e68f68ee8447ba   Name:/dev/vdb1           State:online    Size (GiB):89      Used (GiB):84      Free (GiB):5       
            Bricks:
                Id:03a421ebc0f7a68d138d9fa8afa67739   Size (GiB):8       Path: /var/lib/heketi/mounts/vg_af07f63706290336a9e68f68ee8447ba/brick_03a421ebc0f7a68d138d9fa8afa67739/brick
                Id:5f2a4e8ac83577fe65a53265aa9ee892   Size (GiB):8       Path: /var/lib/heketi/mounts/vg_af07f63706290336a9e68f68ee8447ba/brick_5f2a4e8ac83577fe65a53265aa9ee892/brick
                Id:6fa247bbe01d2e2656b33eade73e8131   Size (GiB):8       Path: /var/lib/heketi/mounts/vg_af07f63706290336a9e68f68ee8447ba/brick_6fa247bbe01d2e2656b33ee73e8131/brick
```
ä¸Šé¢æ˜¾ç¤ºæœ‰ 3 ä¸ªå·ï¼Œä½†æ˜¯ Used æ˜¾ç¤ºå·²ç»ç”¨äº† 84Gï¼Œ ä½†æ˜¯ `lsblk -f` å´æœ‰ 6 ä¸ªç£ç›˜ï¼Œvg_tmeta & vg_tdataï¼š 
```
[fedora@gluster-1 ~]$ sudo vgdisplay
  --- Volume group ---
  VG Name               vg_af07f63706290336a9e68f68ee8447ba
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  162
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                6
  Open LV               3
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               89.87 GiB
  PE Size               4.00 MiB
  Total PE              23007
  Alloc PE / Size       6188 / 24.17 GiB
  Free  PE / Size       16819 / <65.70 GiB
  VG UUID               9WZzBd-CJlr-MWWD-3qRO-z4gD-e0Za-dysaKt
```
è¿™ä¸ªæ˜¾ç¤ºæ˜¯æ­£ç¡®çš„ï¼Œç”¨äº† 24 GBã€‚lvdisplay æ˜¾ç¤ºæœ‰ 6 ä¸ªé€»è¾‘å·ã€‚å†æ¬¡åˆ›å»ºä¸€ä¸ªæœ‰çŠ¶æ€ helm chartï¼Œ`pvc pendingï¼ŒFailed to allocate new volume: No space`ã€‚ å‡çº§ heketi åˆ° 9.0 ç‰ˆæœ¬ï¼Œåˆ é™¤æ‰€æœ‰ PVï¼Œtopology info ä¹Ÿæ˜¾ç¤ºå·²ç»åˆ é™¤ï¼Œä½†æ˜¯` Size (GiB):89 Used (GiB):60  Free (GiB):29`ï¼Œè¿™æ—¶ä¸ºä½•ï¼ŸåŸæ¥è¦è¿è¡Œ `heketi-cli device resync <id_device>`ï¼Œè¿™æ ·ç£ç›˜å¤§å°å°±åŒæ­¥äº†ã€‚ ç„¶åå†æ¬¡å®‰è£…å›æœ‰çŠ¶æ€ chartï¼Œgluster vm è¿˜æ˜¯æ˜¾ç¤ºä¸¤ä¸ªï¼štmeta & tdataï¼Œä½†æ˜¯ `heketi-cli topology info` æ˜¾ç¤ºå·²ç»æ˜¯æ­£å¸¸äº†ã€‚ä¸çŸ¥é“æ˜¯ç‰ˆæœ¬é—®é¢˜è¿˜æ˜¯éœ€è¦è¿è¡Œ resync å‘½ä»¤ã€‚ 

### volume æ— æ³•åˆ é™¤çš„é—®é¢˜
å…ˆæ˜¯åœ¨ k8s ä¸­æ— æ³•åˆ é™¤ï¼Œ`heketi-cli volume delete id` æ˜¾ç¤º errorï¼Œä½†æ˜¯æ²¡æœ‰æœ‰ç”¨ä¿¡æ¯ã€‚åæ¥å‘ç°ä¸€ä¸ªèŠ‚ç‚¹æœ‰ mount fuse.glusterfsï¼Œumount ä¹‹ï¼Œä½†å¹¶æ²¡æœ‰ç”¨ã€‚å…ˆæäº† [issue](https://github.com/heketi/heketi/issues/1538)ã€‚ç°åœ¨å‘ç°å‘é€ delete å‘½ä»¤æ—¶æœåŠ¡å™¨ç«¯æ—¥å¿—éƒ½æ˜¾ç¤ºï¼š

    [negroni] Started GET /volumes/eab0e046b9cdb9a07572d18549be64f2
    [negroni] Completed 200 OK in 7.151436ms

ä½¿ç”¨ https://putsreq.com æ¥æŸ¥çœ‹å‘½ä»¤å‘é€çš„ REST è¯·æ±‚æ˜¯å¦æ˜¯ DELETEï¼Œè¿”å›`Error: server did not provide a message (status 404: Not Found)`ï¼Œåº”è¯¥æ˜¯ URL æœ‰åŠ å‚æ•°çš„ç¼˜æ•…ï¼š`req, err := http.NewRequest("DELETE", c.host+"/volumes/"+id, nil)`ï¼Œè€Œ pustreq å’Œ httpbin éƒ½ä¸æ”¯æŒ wildcard matchingï¼Œå¯èƒ½æ˜¯å®‰å…¨è€ƒè™‘ï¼Œåæ¥å‘ç° Postman æœ¬èº«å¸¦æœ‰è¿™ä¸ªåŠŸèƒ½ï¼Œæ£€æŸ¥åå‘ç° heketi-cli ç¡®å®å‘é€çš„ DEL å‘½ä»¤ã€‚
æ‰‹å·¥ç”¨`gluster volume delete id`å‘½ä»¤åˆ é™¤å·ï¼ŒæˆåŠŸã€‚ä½†æ˜¯ heketi è¿™è¾¹è¿˜æ˜¯æ˜¾ç¤ºåŸæ¥çš„å·ï¼Œheketi db åªæ˜¯å•å‘çš„æ•°æ®æµï¼Ÿk8s å†…éƒ¨çš„ PV å¯ä»¥è‡ªåŠ¨åˆ é™¤ã€‚ç¥­å‡ºå‘½ä»¤`tcpdump port 8080 -nA`ï¼Œå¯¹æ¯”ä¸¤è€…è¯·æ±‚ï¼Œå‘ç°é—®é¢˜åœ¨`./heketi-cli --server http://192.168.51.187:8080/ volume list` çš„è¯·æ±‚ä¸º `DEL //volume/id`ï¼Œè¿™æ—¶ä¼šè¿”å› `301 Moved Permanently` å¹¶é™„ä¸Šæ­£ç¡®çš„ URL `/volume/id`ï¼Œcli æ‹¿åˆ°æ–°çš„ URL åç»Ÿä¸€é‡å‘ GET è¯·æ±‚ã€‚DELETE is not a safe methodï¼Œæ‰€ä»¥ 301 è·³è½¬æ—¶ä¼šè½¬æ¢ä¸º GETï¼Œè¿™ä¸ªæ˜¯ golang http çš„[è¡Œä¸º](https://github.com/golang/go/issues/13994)ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ GET å‘½ä»¤å¯ä»¥æˆåŠŸçš„åŸå› ã€‚

ä¸ºä»€ä¹ˆä¼šè¿”å› 301 å‘¢ï¼ŸåŸæ¥ http.ServeMux åœ¨é‡åˆ° url é‡Œé¢å¸¦å¤šä½™çš„ slash çš„æ—¶å€™ä¼šåšè¿™ä¸ªå¤„ç†ã€‚

è°ƒè¯•å‘ç°å…¶è¯·æ±‚è¿‡ç¨‹æŒºæœ‰è¶£ï¼Œæ¯”å¦‚åˆ›å»ºä¸€ä¸ª volumeï¼š
```
-> POST /volumes
<- 202 Accepted. Location: /queue/qid
-> GET /queue/qid
<- 200 OK
...keep polling
-> GET /queue/id
<- 303 See Other. Location: /volumes/vid
-> GET /volumes/vid
<- 200 OK JSON {...}
```

### èŠ‚ç‚¹é‡æ–°ä¸Šçº¿
ä¸€ä¸ªèŠ‚ç‚¹å‡ºç°é—®é¢˜ï¼Œæ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œåªèƒ½æŠŠ /etc/fstab é‡Œé¢å…¨éƒ¨æ¸…é™¤æ‰å¯ä»¥ï¼Œå¯èƒ½æ˜¯ç¡¬ç›˜é—®é¢˜ã€‚`heketi-cli device remove` è¿”å› Device must be offline before remove operation is performedï¼Œå¥½çš„ï¼Œé‚£å°±å…ˆ`heketi-cli device disable`ï¼Œå†æ¬¡ remove çš„æ—¶å€™ï¼Œè¿”å›é”™è¯¯ Failed to remove device, error: Cannot replace brick xxx as only 1 of 2 required peer bricks are onlineã€‚æˆ‘çš„è®¾ç½®æ˜¯æ¯ä¸ª brick ä¸¤ä¸ªå‰¯æœ¬ï¼Œé›†ç¾¤æ€»å…±æœ‰ 3 ä¸ªèŠ‚ç‚¹ï¼Œè¿™æ ·é—®é¢˜èŠ‚ç‚¹ä¸Šé¢çš„ brick å¯ä»¥è¿ç§»åˆ°å…¶ä»–æ­£å¸¸èŠ‚ç‚¹å•Šï¼Ÿé›†ç¾¤èµ„æºæ˜¯å¤Ÿçš„å‘€ã€‚è¿™ä¸ªæ—¶å€™è¿è¡Œ`heketi-cli volume info`æŸ¥çœ‹ brick åˆ†å¸ƒï¼š
```
Volume Id: 80a20c260f4ca7488651a1eeadf7e6e1
Cluster Id: 596da7375a1ea5cedd395289cf1f2074
Mount: 192.168.51.130:vol_80a20c260f4ca7488651a1eeadf7e6e1
Mount Options: backup-volfile-servers=192.168.51.187,192.168.51.142
Durability Type: replicate
Distribute Count: 1
Replica Count: 2
```
192.168.51.130 å°±æ˜¯é—®é¢˜èŠ‚ç‚¹ï¼Œå› ä¸ºæ¯ä¸ª volume éƒ½æ˜¯ mount åˆ°è¿™ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¯¼è‡´è¿™ä¸ªèŠ‚ç‚¹è¯»å†™ç¹å¿™è€Œæœ€ç»ˆç£ç›˜å‡ºç°é”™è¯¯ï¼Ÿä¸ºä»€ä¹ˆæœ‰çš„åº”ç”¨æ˜¯æ­£å¸¸çš„å‘¢ï¼Ÿä½¿ç”¨äº†`Mount Options`ä½œä¸º failoverï¼ŸæŸ¥çœ‹å„ä¸ªk8sèŠ‚ç‚¹ä¸Šé¢æŒ‚è½½çš„å·ï¼Œä½¿ç”¨å‘½ä»¤`mount | grep`ï¼Œç¡®å®æ²¡æœ‰ 192.168.51.130 èŠ‚ç‚¹çš„äº†ï¼Œä¸é”™çš„ğŸ‘ï¼Œä¸çŸ¥é“è¿™ä¸ª failover æ˜¯åœ¨ä½•æ—¶åˆ‡æ¢çš„ã€‚`device disable`çš„æ—¶å€™ï¼Ÿ

æ—¢ç„¶è¿™äº›å·è¿˜åœ¨åŠæ­£å¸¸å·¥ä½œï¼ˆè™½ç„¶åªæœ‰ä¸€ä¸ªå‰¯æœ¬ï¼‰ï¼Œæ¢å¤å°±æœ‰äº›æ„ä¹‰äº†ã€‚åˆ° GlusterFS èŠ‚ç‚¹ä¸Šé¢è¿è¡Œ[å‘½ä»¤](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)`gluster volume info`ï¼š
```
Volume Name: vol_80a20c260f4ca7488651a1eeadf7e6e1
Type: Replicate
Volume ID: 92cf2b06-9bcc-4798-925b-b97f4f006ebf
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: 192.168.51.130:/var/lib/heketi/mounts/vg_479769750407fdd6b9a8be8ae0f4868                                                               1/brick_dcdd21ca860756660a442e841d6533bf/brick
Brick2: 192.168.51.142:/var/lib/heketi/mounts/vg_33e01e645079271d8d8b4b4b0484791                                                               9/brick_f195342426ad2f481564a378a897c040/brick
Options Reconfigured:
performance.client-io-threads: off
nfs.disable: on
transport.address-family: inet
```
è¿™é‡Œé¢å†…å®¹æ›´å¤šï¼Œæ˜¾ç¤ºäº† bricks åˆ†å¸ƒï¼ŒGlusterå®˜æ–¹çš„æ–‡æ¡£[Replace faulty brick](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#replace-faulty-brick)è®²è¿°äº†å¦‚ä½•æ›¿æ¢ brickï¼Œä¼¼ä¹æ­£æ˜¯æˆ‘éœ€è¦çš„ï¼Œåªæ˜¯æ­¥éª¤æœ‰ç‚¹å¤šï¼Œ`heketi-cli device remove`ä¼¼ä¹å°±æ˜¯ç®€åŒ–æ“ä½œï¼Œè¿™ä¸ª[issue](https://github.com/heketi/heketi/issues/1630)å’Œæˆ‘é—®é¢˜ä¸€æ ·ğŸ˜…ï¼Œæ˜¯ä¸ª[å·²çŸ¥é—®é¢˜](https://github.com/heketi/heketi/pull/1653) - Fixed migration logic for replica < 3 volumesã€‚æŒ‰ç…§æ“ä½œ master åˆ†æ”¯é‡æ–°ç¼–è¯‘ï¼Œè¿è¡Œåé—®é¢˜ä¾ç„¶ï¼Œåæ¥æ‰æ„è¯†åˆ°è¿™ä¸ªæ˜¯æœåŠ¡å™¨ç«¯çš„ä¿®æ”¹ã€‚ä¸Šä¼ åˆ° server åè¿è¡Œ heketi-cli å‘½ä»¤ä¼šå‡ºé”™ï¼šError: Invalid JWT token: Token missing iss claimï¼Œ[åŸæ¥](https://github.com/heketi/heketi/issues/1664)è¦åŠ ä¸Šè®¤è¯æ‰å¯ä»¥ï¼š
```
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY="My Secret"
```
å¯†ç åœ¨ /etc/heketi/heketi.json é‡Œé¢å¯ä»¥æ‰¾åˆ°ã€‚ç„¶åå†æ¬¡è¿è¡Œ`heketi-cli device remove`ï¼Œç­‰ä¸€æ®µæ—¶é—´å°±æˆåŠŸäº†ã€‚remove ä¹‹åè¿è¡Œ`heketi-cli device delete`ï¼Œå†æ¬¡é”™è¯¯ï¼š
```
Error: Failed to delete device /dev/sda1 with id 479769750407fdd6b9a8be8ae0f48681 on host 192.168.51.130:   Volume group "vg_479769750407fdd6b9a8be8ae0f48681" not found
```

å‡ºç°é—®é¢˜æ—¶å€™ï¼Œä¸Šé¢æ“ä½œåº”è¯¥æ˜¯è‡ªåŠ¨ï¼Œè€Œä¸æ˜¯æ‰‹å·¥æ¥ç»´æŠ¤ã€‚`backup-volfile-servers=192.168.51.187,192.168.51.142` è¡¨é¢å·²ç»è‡ªåŠ¨ re-balance äº†ä¹ˆï¼Ÿ[Rebalancing Volumes](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#rebalancing-volumes) è¿™ä¸ªæŒºè¯¦ç»†ã€‚

### Odroid HC1

Ubuntu 18.04.1 LTSï¼Œä¸¤å°æœºå™¨ï¼Œodroid-1 & odroid-2ï¼Œå„å¸¦ä¸€æœºæ¢°ç¡¬ç›˜ï¼Œå„åƒå…†ç½‘çº¿æ¥äº¤æ¢æœºï¼Œheketi å®‰è£…åœ¨ odroid-1ã€‚

`apt install glusterfs-server lvm2 thin-provisioning-tools` 

å¦‚æœä»¥å‰å®‰è£…ï¼Œå…ˆ vgdisplay/vgremoveæ¸…é™¤å·²æœ‰ lvm åˆ†åŒºï¼Œå¦åˆ™ add device æ—¶å€™è¯´ç£ç›˜æ²¡æœ‰åˆå§‹åŒ–ã€‚ 

    heketi-cli --server "http://192.168.51.187:8080" node add --management-host-name 192.168.51.130 --storage-host-name 192.168.51.130 --zone 1 --cluster foo 

è¿™é‡Œå…¨éƒ¨ç”¨ IPï¼Œå¦åˆ™å„ç§é—®é¢˜ã€‚ 

å­˜å‚¨(nas)æ˜¯è¿™ä¸ªæ¿å­æœ€å¥½ç”¨é€”äº†ï¼Œè£… k8s é›†ç¾¤ä½œä¸ºè®¡ç®—èŠ‚ç‚¹çš„è¯å†…å­˜å¤ªå°ã€‚è€Œä¸”è¿™ä¸ªæ¿å­ä¹…ç»è€ƒéªŒï¼Œè™½ç„¶æ€§èƒ½ä¸€èˆ¬ï¼Œä½†ç¨³å®šä½¿ç”¨ï¼ŒOdroid ä¹Ÿåœ¨ä¸æ–­æ›´æ–°å›ºä»¶ï¼Œè¿™ä¸ªå¯¹äº ARM æ¿å­æ˜¯å¾ˆéš¾å¾—çš„ã€‚

å¦‚ä½•æµ‹è¯•ä¸ªæ€§èƒ½ï¼Ÿ[Accessing Gluster volume via SMB Protocol](https://docs.gluster.org/en/latest/Administrator%20Guide/Accessing%20Gluster%20from%20Windows/)ï¼ŒæŒ‰ç…§è¿™é‡Œå»ºç«‹ samba å…±äº«æœåŠ¡ã€‚å•ç‹¬çš„ä½¿ç”¨ Samba æŒ‚è½½ gluster volume å¹¶ä¸èƒ½å®ç°High Availability,æœ‰äº†CTDBå°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜äº†ã€‚[ä½¿ç”¨ctdb+samba+glusterfsæ­å»ºNASé›†ç¾¤ç³»ç»Ÿ](https://segmentfault.com/a/1190000003005106)è¿™ä¸ªé…ç½®æŒºå¤æ‚ã€‚

ä½¿ç”¨åŠå¹´åï¼Œå‘ç°è¿™ä¸ªæ¿å­å„ç§é—®é¢˜ï¼šæœ‰æ—¶å€™ç¡¬ç›˜å‡ºç°é”™è¯¯ï¼ˆfstab æ— æ³•æŒ‚è½½ï¼‰ï¼Œæœ‰æ—¶å€™ç³»ç»Ÿ SD å¡é”™è¯¯ï¼Œæ•´ä¸ªé—®é¢˜ç³»ç»Ÿå˜æˆåªè¯»çš„ã€‚æ€»ä½“æ¥è¯´ä¸é€‚åˆé•¿æ—¶é—´ã€æŒç»­çš„ç£ç›˜ IO æ“ä½œï¼Œå› ä¸º SD å¡æ²¡æœ‰ S.M.A.R.T æ ¡éªŒï¼Œå†…å­˜ä¹Ÿæ²¡æœ‰ ECCï¼Œå‡ºé”™æ¦‚ç‡æ¯”è¾ƒé«˜ã€‚