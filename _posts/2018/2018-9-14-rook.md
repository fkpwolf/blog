---
layout: post
title:  "æµ‹è¯•ä¸‹ Rook"
date:   2018-9-14 14:20:33
categories:
  - cloud
  - db
typora-root-url: ../../../blog
---

CNCFé¦–ä¸ªäº‘åŸç”Ÿå­˜å‚¨é¡¹ç›®â€”â€”ROOKï¼ŒåŸºäºæˆç†Ÿçš„ Cephã€‚ 

<https://rook.io/docs/rook/v0.8/ceph-quickstart.html>

Design Doc <https://github.com/rook/rook/tree/master/design> 

![](/images/2018/rook-arch1.png)





![](/images/2018/rook-arch2.png)

Ref <https://rook.io/docs/rook/v0.8/> 

æ€ä¹ˆä¿è¯ç£ç›˜çš„è®¿é—®æ€§èƒ½å‘¢ï¼Ÿä¸è¿‡ Ceph éƒ¨ç½²æ¯”è¾ƒå¤æ‚ï¼Œè¿™æ ·ç»Ÿä¸€ç®¡ç†ä¼¼ä¹ä¹Ÿæœ‰ä¼˜åŠ¿ï¼Œæ›´é‡è¦çš„æ˜¯å¯ä»¥ç›‘æ§èµ·æ¥ï¼Œä¹Ÿå¯ä»¥æ›´å®¹æ˜“çš„ä¼¸ç¼©ã€‚æ€»çš„æ¥è¯´ï¼Œå¦‚æœæ˜¯ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œåˆ™è¿ç§»åˆ° k8s ä¸Šæ˜¯æœ‰ä¼˜åŠ¿çš„ã€‚ è¿™é‡Œçœ‹ volume plugin ä¼šç›´æ¥åˆ†åŒºå’Œ Mount ç£ç›˜ï¼Ÿå¦‚æœè¿™æ ·å°±å’Œè‡ªå·±å®‰è£… Ceph æ€§èƒ½å·®åˆ«ä¸å¤§äº†ã€‚ è¿™é‡Œè¿æ¥ç£ç›˜æ–¹å¼å°±æœ‰å¤šç§äº†ã€‚ä¸Šé¢æ˜¯ç›´æ¥ mount èŠ‚ç‚¹ä¸Šé¢çš„ç£ç›˜ï¼Œä½†æ˜¯ä¸€èˆ¬æ¶æ„å­˜å‚¨å’Œè®¡ç®—æ˜¯åˆ†å¼€çš„ï¼Œå­˜å‚¨ä¸€èˆ¬éœ€è¦è¾ƒå¤šçš„ç£ç›˜ï¼Œæœºæ¶æ ¼å±€å’Œè®¡ç®—ä¸åŒã€‚å¦‚æ­¤éœ€è¦éƒ¨ç½²ä¸€ä¸ªå•ç‹¬çš„ k8s å­˜å‚¨é›†ç¾¤äº†ï¼Ÿ 

åŸæ¥å®ƒç”¨çš„æ˜¯ FlexVolume è¿™ç§æ‰©å±•ï¼Œ<https://rook.io/docs/rook/v0.8/flexvolume.html> [Flexvolume](https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md) ä¼¼ä¹åªæ˜¯ä¸€ç§è®© kubelet ç›´æ¥è¿è¡Œçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ”¾åœ¨ /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ä¸‹é¢ï¼Œè¿™äº›ä¸æ˜¯å®¹å™¨é•œåƒã€‚ä½¿ç”¨è¿™äº›æ–‡ä»¶å¯ä»¥ç›´æ¥å¯¹èŠ‚ç‚¹ä¸Šçš„ç£ç›˜æ“ä½œï¼Œæ¯”å¦‚åˆ†åŒºå’ŒæŒ‚è½½ï¼Œä¼¼ä¹å’Œæ¯ä¸ªèŠ‚ç‚¹ä¸Šå®‰è£… ceph-common ä¸€æ ·ï¼Œä½†æ˜¯è¿™ä¸ªæ˜¯ç”± k8s æ¥ç®¡ç†å®‰è£…å’Œæ›´æ–°ï¼Œä¸ç”¨æ‰‹å·¥å¹²é¢„ã€‚ä¸ºä»€ä¹ˆä¸æ”¾ pod é‡Œé¢ï¼Ÿpod æ— æ³•è®¿é—®ç£ç›˜ï¼Ÿä¸å¯¹å•Šï¼Œosd server æ˜¯ç›´æ¥è®¿é—®çš„ã€‚é—®é¢˜æ˜¯ osd server èƒ½ç›´æ¥è®¿é—®èŠ‚ç‚¹çš„ç£ç›˜æ¯”å¦‚/dev/vdb ä¹ˆï¼Ÿ 
```shell
[centos@k8s-4 ~]$ sudo ls /var/lib/rook/osd0 -lh
æ€»ç”¨é‡ 44K
lrwxrwxrwx 1 root root   58 9æœˆ  27 07:07 block -> /dev/disk/by-partuuid/8f0c90b3-e76f-47f7-940f-79165b5f54c1
lrwxrwxrwx 1 root root   58 9æœˆ  27 07:07 block.db -> /dev/disk/by-partuuid/9c5ad55d-764e-4425-a688-7d16171629d1
lrwxrwxrwx 1 root root   58 9æœˆ  27 07:07 block.wal -> /dev/disk/by-partuuid/d1dc01e5-e5eb-45c6-a50e-0b11d05a3a99
[centos@k8s-4 ~]$ lsblk --output NAME,PARTLABEL,PARTUUID
NAME   PARTLABEL       PARTUUID
vda                    
â””â”€vda1                 
vdb                    
â”œâ”€vdb1 ROOK-OSD0-WAL   d1dc01e5-e5eb-45c6-a50e-0b11d05a3a99
â”œâ”€vdb2 ROOK-OSD0-DB    9c5ad55d-764e-4425-a688-7d16171629d1
â””â”€vdb3 ROOK-OSD0-BLOCK 8f0c90b3-e76f-47f7-940f-79165b5f54c1
```
è¿™æ ·å°±å¾ˆæ¸…æ¥šäº†ï¼Œå®¹å™¨è¿˜æ˜¯ä»¥æ–‡ä»¶çš„å½¢å¼æ¥è®¿é—®å®¿ä¸»æœºï¼Œè€Œä¸Šé¢çš„ç¬¦å·è¿æ¥åº”è¯¥æ˜¯ç”± rook flexvolume æ¥å®Œæˆçš„ã€‚UNIX ä»»ä½•çš†æ–‡ä»¶çš„å“²å­¦æœç„¶å¼ºå¤§ã€‚è¿™æ ·å°±èƒ½ç†è§£ä¸ºä»€ä¹ˆé‡å»º ceph cluster æ—¶ä¸ºä»€ä¹ˆè¦åˆ é™¤/var/lib/rook ç›®å½•äº†ã€‚

å…¶ quickstart é»˜è®¤ç”¨çš„æ˜¯ä¸»æœºç£ç›˜æ–‡ä»¶ç³»ç»Ÿï¼šIf you are using dataDirHostPath to persist rook data on kubernetes hosts, make sure your host has at least 5GB of space available on the specified path.  ä¸ºä»€ä¹ˆä¸æä¾›ç›´æ¥ mount ä¸»æœºä¸Šé¢çš„ç£ç›˜ /dev/sdb è¿™ç§æ–¹å¼ï¼Ÿ<https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html#storage-selection-settings> deviceFilter * sdb: Only selects the sdb device if foundï¼Œç›´æ¥è¿™æ ·å°±å¯ä»¥ï¼Ÿè¿™ä¸ªæ˜¯èŠ‚ç‚¹ä¸Šé¢çš„ç£ç›˜å§ï¼Œå®¹å™¨èƒ½ç›´æ¥è®¿é—®ç£ç›˜ï¼Ÿå¦‚æœæŸä¸ªèŠ‚ç‚¹æ²¡æœ‰ç£ç›˜ï¼Œé‚£è¿™ä¸ªä¸Šé¢ä¹Ÿä¸ä¼šå®‰è£… Cephï¼ŸOperator è¿™ä¹ˆæ™ºèƒ½ï¼Ÿæˆ‘å¯ä»¥æä¸€ä¸ªæœ‰å¤šä¸ªç£ç›˜çš„è£¸æœºç„¶ååŠ å…¥å·²æœ‰é›†ç¾¤ã€‚ 

æŒ‰ç…§ <https://rook.io/docs/rook/v0.8/ceph-quickstart.html> å®‰è£…ä¹‹ã€‚é»˜è®¤ç£ç›˜æ—¶å®‰è£…åœ¨ host æœºå™¨ä¸Šçš„ /var/lib/rookï¼Œæˆ‘ 3 ä¸ªèŠ‚ç‚¹ï¼Œä¼šæœ‰ 3 ä¸ª osd podï¼Œä¸€ä¸ª mgr podï¼Œ3 ä¸ª monitor podï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªã€‚æ•´ä¸ªé€Ÿåº¦æŒºå¿«çš„ï¼Œæ¯”è‡ªå·±æ•´ ceph é›†ç¾¤å¿«å¤šäº†ã€‚mgr pod å°±æ˜¯è‡ªå¸¦çš„ Dashboard web uiã€‚ceph version 12.2.7 luminous (stable)ï¼Œoperatorï¼Œæœç„¶æ¯”æˆ‘ Ceph é—¨å¤–æ±‰æ›´ä¸“ä¸šã€‚Dashboard å¯å®æ—¶åˆ·æ–°æ•°æ®ï¼Œç”¨çš„æ˜¯è½®è®­ï¼Œæ„Ÿè§‰ç›‘æ§èµ·æ¥æ¯”å‘½ä»¤è¡Œè¦æ–¹ä¾¿ã€‚ä¸è¿‡çœ‹ä¸åˆ° pool ä¸‹é¢çš„å¯¹åº” pvc çš„å·ã€‚ 

ç°åœ¨æ·»åŠ ç£ç›˜ï¼Œå…ˆç»™ k8s-4 èŠ‚ç‚¹åˆ†é…ä¸€ä¸ªæ–°ç¡¬ç›˜ï¼Œç„¶åè®¾ç½® cluster.yaml useAllDevices: trueï¼Œè¿™ä¼šå¯¹èŠ‚ç‚¹ä¸Šæ‰€æœ‰çš„éæ“ä½œç³»ç»Ÿçš„ç£ç›˜åˆ†åŒºã€‚kubectl apply æ²¡å˜åŒ–ï¼Œåªèƒ½å…¨åˆ é™¤åé‡å»ºã€‚ä½†æ˜¯è¿™æ¬¡ä¸€ä¸ª osd server ä¹Ÿæ²¡æœ‰ã€‚osd-prepare-k8s-4 ä¸Šæœ‰æ—¥å¿—ï¼š 
```
2018-09-13 09:15:57.545604 I | exec: Running command: lsblk /dev/vdb --bytes --pairs --output NAME,SIZE,TYPE,PKNAME 
2018-09-13 09:15:57.548315 I | exec: Running command: udevadm info --query=property /dev/vdb1 
2018-09-13 09:15:57.550359 I | sys: non-rook partition: 
2018-09-13 09:15:57.550378 I | exec: Running command: udevadm info --query=property /dev/vdb 
2018-09-13 09:15:57.552510 I | cephosd: skipping device vdb that is in use (not by rook). fs: , ownPartitions: false 
2018-09-13 09:15:57.562849 I | cephosd: configuring osd devices: {"Entries":{}} 
```
ä¸èƒ½è‡ªå·±åˆ†åŒºï¼Ÿå¯èƒ½æ˜¯ä¸ºäº†ä¿æŠ¤æ•°æ®ã€‚å¥½å§ï¼Œæˆ‘åˆ é™¤ /dev/vdb1 åˆ†åŒºï¼Œç„¶åé‡å»ºé›†ç¾¤ã€‚ä¸ºä»€ä¹ˆæ¯æ¬¡éƒ½è¦é‡å»ºï¼Ÿç°åœ¨æœ‰ä¸€ä¸ª osd server äº†ï¼å¥½è€¶ï¼æœ‰äº†è¿™ä¸ª operatorï¼Œé‡å»º ceph é›†ç¾¤çœŸæ˜¯å¥½æ–¹ä¾¿ï¼è¿›å…¥åˆ° k8s-4ï¼Œ/dev/vdb è¢«åˆ†æˆäº† 3 ä¸ªåŒºï¼Œä½†æ˜¯æ²¡æœ‰çœ‹åˆ°é‚£ä¸ªå®¹å™¨æŒ‚è½½äº†è¿™äº›åˆ†åŒºã€‚å¯¹äºæ›¾ç»åŠ å…¥ ceph è¿‡çš„ç£ç›˜ï¼Œå†æ¬¡åŠ å…¥é›†ç¾¤æ—¶å¯ä»¥ä¸ç”¨åˆ é™¤å·²æœ‰åˆ†åŒºã€‚ 

`kubectl apply -f storageclass.yaml` åˆ›å»º StorageClassï¼š 
```yaml
apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
â€‹    size: 1
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
```
ç›¸å½“ç®€å•ï¼Œä¸ç”¨é…ç½® IP ä¹‹ç±»çš„ï¼Œå’Œ Openstack ç¯å¢ƒä¸‹ cinder æŒ‚è½½ç±»ä¼¼ã€‚ç„¶åå°±å¯ä»¥ `helm chart postgresql`åˆ›å»º äº†ã€‚åœ¨ postgresql pod æ‰€åœ¨çš„èŠ‚ç‚¹ä¸Šé¢å¯ä»¥çœ‹åˆ° dmesg é‡Œé¢æœ‰ï¼š 
```shell
libceph: mon1 10.100.238.135:6790 session established 
rbd: rbd0: capacity 8589934592 features 0x1 
[centos@k8s-2-new ~]$ lsblk 
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT 
sda      8:0    0  366K  0 disk 
vda    252:0    0   20G  0 disk 
â””â”€vda1 252:1    0   20G  0 part / 
rbd0   251:0    0    8G  0 disk /var/lib/kubelet/pods/13be1c5a-b73b-11e8-86d6-aa90b3cbae01/volumes/ceph.rook.io~rook-ceph-system/pvc-139b6b86-b73b-11e8-86d6-aa90b3cbae01 
```
ç”¨çš„å’Œæˆ‘è‡ªå·±æ­å»º Ceph å·®ä¸å¤šçš„å·¥å…·åº“ï¼Œä¸çŸ¥é“è¿™æ¬¡ä¼šä¸ä¼šå‡ºç° libceph socket closed å¯¼è‡´èŠ‚ç‚¹é«˜è´Ÿè½½çš„é—®é¢˜ã€‚ceph ç‰ˆæœ¬ä¸åŒï¼Œprovisioner ä¹Ÿä¸åŒï¼Œå¸Œæœ›ä¸ä¼šå‡ºç°å§ã€‚ï¼ˆæƒ³ä¸åˆ°å›°æ‰°ä¸€æ—¶çš„é—®é¢˜è¿™ä¹ˆè§£å†³ï¼ŸğŸ˜‚ï¼‰ 

ä¸Šé¢çš„ 10.100.238.135 æ˜¯å»ºç«‹åœ¨ k8s-4ä¸Šé¢ service çš„ ClusterIPï¼Œè¿™ä¸ªé…ç½®æ²¡æœ‰å†™åœ¨ StorageClass é‡Œé¢ï¼Œæˆ‘çŒœæ˜¯ provisioner åšçš„è‡ªåŠ¨é…ç½®ã€‚ 

æ€ä¹ˆé€šè¿‡ ceph-common å‘½ä»¤æŸ¥çœ‹ä¸Šé¢çš„é›†ç¾¤å‘¢ï¼Ÿ<https://rook.io/docs/rook/v0.8/toolbox.html> è€ƒè™‘åˆ°äº†ï¼Œä¸é”™ã€‚`kubectl create -f toolbox.yaml` ä¸€è¡Œä»£ç å°±æå®šï¼ˆexamples/kubernetes/ceph è¿™ä¸ªæ ·ä¾‹ä»£ç åšçš„ä¸é”™ï¼‰ã€‚æ–‡æ¡£æœ‰é—®é¢˜ï¼Œè¿æ¥ pod çš„åå­—ä¸å¯¹ã€‚è¿™æ ·å°±å¯ä»¥éªŒè¯åˆ é™¤ pv å Ceph é‡Œé¢ pool/pv æ˜¯å¦ä¹Ÿåˆ é™¤ï¼šç¡®å®åˆ é™¤äº†ï¼ 

éªŒè¯ä¸‹ç¨³å®šæ€§ï¼šé‡å¯å”¯ä¸€çš„ osd èŠ‚ç‚¹ k8s-4ï¼Œç»“æœæŒ‚äº†ï¼šqcow2: Image is corrupt; cannot be opened read/writeã€‚éš¾é“ ceph æŠŠç³»ç»Ÿç›˜ç»™ä¿®æ”¹äº†ï¼Ÿä¹Ÿä¿®å¤ä¸å¥½ï¼Œæ²¡åŠæ³•ï¼Œåªèƒ½é‡å»ºã€‚é‡å»ºåå› ä¸ºæˆ‘ hostname æ²¡æœ‰æ”¹ï¼Œk8s-4 ä¸Šé¢è‡ªåŠ¨è¿è¡Œäº†ä¸€ä¸ª osd serverï¼Œä½†æ˜¯æœ‰é”™è¯¯ï¼š 

    bluestore(/var/lib/rook/osd0/block) _read_bdev_label failed to open /var/lib/rook/osd0/block: (2) No such file or directory 

å¯èƒ½ operator å·²ç»è®°å½•äº† k8s-4 ä¸Šé¢æœ‰ä¸€ä¸ªç£ç›˜ï¼Œå¹¶ä¸”å·²ç» prepare è¿‡ï¼Œæ‰€ä»¥è®¤ä¸ºè¿™ä¸ªç›®å½•ä¸‹åº”è¯¥å¯¹åº”çš„é…ç½®æ–‡ä»¶ã€‚æˆ‘è§‰å¾—é‡æ–° repare ä¸‹åº”è¯¥å¯ä»¥ï¼Œç°åœ¨ä¸çŸ¥é“æ–¹æ³•ï¼Œæ‰‹å·¥åˆ é™¤ rook ç›®å½•ï¼Œé‡å¯ k8s-4ã€‚å¾—é‡å»ºé›†ç¾¤äº†ã€‚ 
```shell
[centos@k8s-1 ceph]$ kubectl delete -f cluster.yaml
namespace "rook-ceph" deleted
serviceaccount "rook-ceph-cluster" deleted
role.rbac.authorization.k8s.io "rook-ceph-cluster" deleted
rolebinding.rbac.authorization.k8s.io "rook-ceph-cluster-mgmt" deleted
rolebinding.rbac.authorization.k8s.io "rook-ceph-cluster" deleted
cluster.ceph.rook.io "rook-cephâ€ deleted
```
åˆ é™¤å¡åœ¨æœ€åä¸€æ­¥ï¼Œ[å¼ºåˆ¶åˆ é™¤](https://github.com/kubernetes/kubernetes/issues/60807)åé‡æ–°å®‰è£…ä¼šæ˜¾ç¤º cluster.ceph.rook.io "rook-cephâ€ unchangedï¼Œè¿™ä¼šå¯¼è‡´åé¢ pod éƒ½ä¸ä¼šåˆ›å»ºå‡ºæ¥ã€‚ 
```shell
[centos@k8s-1 ceph]$ kubectl get clusters.ceph.rook.io  -n rook-ceph
NAME        AGE
rook-ceph   18h
[centos@k8s-1 ceph]$ kubectl delete clusters.ceph.rook.io rook-ceph -n rook-ceph
cluster.ceph.rook.io "rook-ceph" deleted
```
ä¹Ÿæ˜¯å¡åœ¨è¿™ä¸€æ­¥ã€‚ä¼¼ä¹ CRD åˆ é™¤å®¹æ˜“å‡ºç°è¿™ç§é—®é¢˜ï¼Œ[ceph-teardown](https://rook.io/docs/rook/v0.8/ceph-teardown.html) å·²ç»è®°å½•äº†ï¼Œè¦åˆ é™¤ CRD çš„ finalizerã€‚ 

ç„¶åé—®é¢˜ä¾æ—§ã€‚åªèƒ½ `kubectl delete -f operator.yaml` å†é‡å»ºã€‚ 

æ—¢ç„¶åœ¨ k8s ä¸­ç®¡ç† Cephï¼Œä¸€ä¸ªé—®é¢˜å°±æ˜¯åŠ¨æ€ç®¡ç†ã€‚æ¯”å¦‚å¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°çš„ç£ç›˜åæŠŠè¿™ä¸ªç£ç›˜åŠ å…¥åˆ° ceph é›†ç¾¤ä¸­ï¼Œè¿™äº›éƒ½èƒ½é€šè¿‡æ“ä½œ CRD è€Œä¸ç”¨äº†è§£ ceph çŸ¥è¯†ä¹ˆï¼Ÿæ·»åŠ æ–°ç£ç›˜åï¼Œpod çš„ ceph monitor ä¼šåŒæ­¥æ›´æ–°ä¹ˆï¼Ÿ[è¿™é‡Œ](https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html)æè¿°è¯´ï¼šå¦‚æœ useAllNodes è®¾ç½®ä¸º falseï¼ŒNodes can be added and removed over time by updating the Cluster CRD 

åˆ›å»ºä¸€ä¸ªæ–°èŠ‚ç‚¹ï¼Œå¹¶æŠŠå…¶åŠ å…¥åˆ° k8s ä¸­ã€‚æ£€æµ‹ç³»ç»Ÿï¼Œå‘ç° rook-ceph-system ä¸‹å·²ç»æœ‰ rook-ceph-agent & rook-discover pod è¿è¡Œäº†ï¼ˆéƒ½æ˜¯å®ˆæŠ¤è¿›ç¨‹é›†ï¼‰ï¼Œrook-ceph namespace ä¸‹é¢å¹¶æ²¡æœ‰æ–°çš„ ceph-monitor podã€‚æ–°èŠ‚ç‚¹å·²ç» load libceph rbd kernel moduleï¼Œæ˜¯ rook åŠ çš„ï¼Ÿæ–°èŠ‚ç‚¹ä¸Šè¿˜æ²¡æœ‰å®‰è£… ceph-common åŒ…ã€‚ç„¶åæ–°èŠ‚ç‚¹å…³æœºåŠ å…¥æ–°ç£ç›˜ï¼Œå¯åŠ¨ã€‚rook-discover æ—¥å¿—å·²ç»èƒ½å‘ç°æ–°ç£ç›˜ /dev/vdbï¼Œä½†æ˜¯æ²¡æœ‰åˆ›å»ºå¯¹åº”çš„ osd podã€‚æˆ‘é›†ç¾¤ useAllNodes ä¸º trueï¼Œåº”è¯¥ä¸ºè‡ªåŠ¨æ·»åŠ å•Šã€‚è¿™æ—¶å€™æˆ‘å‘ç°æ–°èŠ‚ç‚¹é‡Œé¢ rook-agent é‡Œé¢æœ‰å¥½å¤šé”™è¯¯ï¼š 

    github.com/rook/rook/vendor/github.com/rook/operator-kit/watcher.go:76: Failed to list *v1beta1.Cluster: Get https://10.96.0.1:443/apis/ceph.rook.io/v1beta1/clusters?resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout 

å…³é—­æ–°èŠ‚ç‚¹ï¼ˆfedoraï¼‰ä¸Šé¢çš„é˜²ç«å¢™ï¼Œè¿˜æ˜¯è€æ ·å­ã€‚å‘½ä»¤è¡Œä¸‹ curl -k url æœ‰é”™è¯¯ï¼Œå…¶ä»–è€èŠ‚ç‚¹éƒ½æ²¡é—®é¢˜ã€‚dmesg ä¸‹é¢æœ‰å¦‚ä¸‹é”™è¯¯ï¼š 

    overlayfs: unrecognized mount option "context="system_u:object_r:container_file_t:s0:c514" or missing value 

æ¢æˆ centosï¼Œæ²¡æœ‰ä¸Šé¢é”™è¯¯ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰ osd podã€‚è¿™ä¸ª Q&A [OSD pods are not created on my devices](https://github.com/rook/rook/blob/master/Documentation/common-issues.md#osd-pods-are-not-created-on-my-devices) å†™äº†äº›åŸå› ï¼Œä¸è¿‡éƒ½å’Œæˆ‘çš„ä¸ä¸€æ ·ï¼Œä¸è¿‡è¿˜æ˜¯å®‰è£…èµ·è§£å†³åŠæ³•åˆ é™¤ rook-ceph-operator podï¼Œè¿™æ ·ä¼šæœ‰ä¸€ä¸ªæ–°çš„ osd-prepareï¼Œä½†æ˜¯å…¶è¿è¡Œé”™è¯¯ï¼š 
```
2018-09-18 06:28:51.356903 I | partition vda: Could not create partition 2 from 1181696 to 43124735 
2018-09-18 06:28:51.356947 I | partition vda: Unable to set partition 2's name to 'ROOK-OSD13-DB'! 
2018-09-18 06:28:51.356953 I | partition vda: Error encountered; not saving changes. 
failed to configure devices. failed to config osd 13. failed format/partition of osd 13. failed to partion device vda. failed to partition /dev/vda. Failed to complete 'partition vda': exit status 4. 
```
æ‰¾äº†ä¸‹ï¼ŒåŸæ¥ ceph éœ€è¦ç£ç›˜è‡³å°‘ 30GBï¼Œè¿™ä¸ªé”™è¯¯æç¤ºä¹Ÿå¤ªéšæ™¦äº†å§ï¼ceph æœç„¶æ˜¯å¤è€çš„ä¸œè¥¿ã€‚è¿™äº›é”™è¯¯æ˜¯ ceph æŠ¥å‡ºæ¥çš„ï¼Œè¿™ä¸ªæ–°èŠ‚ç‚¹ä¸Šé¢æ²¡æœ‰å®‰è£… ceph-common å®¢æˆ·ç«¯ã€‚æé«˜ç£ç›˜å®¹é‡ä¹‹å osd pod å°±åˆ›å»ºäº†ã€‚å°±æ˜¯è¿™ä¸ªå¿…é¡»è¦åˆ é™¤ operator æ‰èƒ½ç”Ÿæ•ˆæœ‰ç‚¹è¹©è„šã€‚å¦‚æœæˆ‘ä¸€æ—©å°±é¢„å¤‡ 30gb å°±æ²¡è¿™ä¸ªé—®é¢˜ï¼Ÿä¸å¯èƒ½å§ï¼Œå› ä¸ºä¸é‡å»º operator podï¼Œè¿ prepare pod éƒ½ä¸ä¼šå‡ºæ¥ã€‚è¿™ä¸ªæ–°èŠ‚ç‚¹ä¸Šé¢ monitor å¹¶æ²¡æœ‰ï¼Œå¯èƒ½è§‰å¾— 3 ä¸ªå°±å¤Ÿäº†ã€‚ 

ç„¶å poweroff è¿™ä¸ªèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¸Šçš„ osd pod æ²¡æœ‰è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹ï¼ŒçŠ¶æ€ä¸ºã€æ­¤å®¹å™¨æ­£åœ¨ç­‰å¾…ä¸­ã€ï¼Œè¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºç£ç›˜å·²ç»ä¸åœ¨äº†ã€‚Ceph Dashboard ä¸­ä»æ˜¾ç¤ºä¸º HEALTH_OKã€‚  

ç„¶å kubectl delete nodeï¼Œè¿™ä¸ªæ—¶å€™ pod ä¸º ã€æ­¤å®¹å™¨ç»„å·²ç»å‡ºé”™ã€0/5 nodes are available: 1 node(s) were not ready, 1 node(s) were out of disk space, 4 node(s) didn't match node selector. OSD å‰¯æœ¬é›†ä¸Šæœ‰å¾ˆå¤šé€‰æ‹©å™¨ï¼šapp: rook-ceph-osdï¼›ceph-osd-id: 15ï¼›pod-template-hash: 1499787393ï¼›rook_cluster: rook-cephï¼Œä½†æ˜¯æˆ‘çœ‹å„ä¸ªèŠ‚ç‚¹ä¸Šé¢éƒ½æ²¡æœ‰è¿™äº›æ ‡ç­¾ï¼Œä¸çŸ¥é“æ˜¯æ€ä¹ˆåŒ¹é…çš„ã€‚Ceph Dashboard æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚ 

é‡å»º operator podï¼Œæ²¡å•¥ç”¨ï¼Œk8s èµ„æºéƒ½è¦æ‰‹å·¥åˆ é™¤ã€‚Ceph Dashboard è¿˜æ˜¯æ˜¾ç¤ºå·²ç»åˆ é™¤çš„ OSD Serverã€‚ 

Fedora é‡æ–°è¯•ä¸€ä¸‹ï¼Œä½¿ç”¨ç³»ç»Ÿ docker-ceï¼ŒçŒœæµ‹ rook-agent ç½‘ç»œä¸é€šåº”è¯¥æ˜¯ ipvs é—®é¢˜ï¼ˆè¦å‡çº§ kube-proxyï¼‰ï¼Œç°åœ¨ agent æ²¡æœ‰ timeout çš„é—®é¢˜äº†ã€‚å†è¿™ä¸ªèŠ‚ç‚¹ä¸Šæµ‹è¯•ä¸Šé¢åŠŸèƒ½ï¼Œä¸€æ ·é—®é¢˜ã€‚æäº†ä¸ª [issue](https://github.com/rook/rook/issues/2134)ã€‚ 

ç¬¬äºŒå¤©æ¥å‘ç° mon pod å·²ç»è¿ç§»åˆ°æ–°èŠ‚ç‚¹ä¸Šï¼Œè‡ªåŠ¨è¿ç§»å·²é è¿‘ osd podï¼Ÿæ€»æ•°è¿˜æ˜¯ 3 ä¸ªã€‚ä½†æ˜¯æœ‰é”™è¯¯ï¼šrook error: unknown flag: --config-dirï¼Œå…¶ä»– mon pod éƒ½å¥½å¥½çš„å•Šï¼Œå¥‡æ€ªã€‚åˆ° hub.docker.com æ£€æŸ¥åæŠŠé•œåƒç‰ˆæœ¬ä» master æ”¹ä¸º v0.8.2 å°±å¯ä»¥äº†ï¼Œè¿™å¸®äººå¼€å‘çæã€‚ 

æˆ‘ 30GB ç¡¬ç›˜ï¼Œceph è‡ªåŠ¨åˆ†åŒºåä¸ºï¼š 
```
#         Start          End    Size  Type            Name
1         2048      1181695    576M  Linux filesyste ROOK-OSD0-WAL
2      1181696     43124735     20G  Linux filesyste ROOK-OSD0-DB
3     43124736     62914526    9.4G  Linux filesyste ROOK-OSD0-BLOCK
```
ä¸ºä»€ä¹ˆblockåªæœ‰ 10Gbï¼Œå“ªé‡Œ[é…ç½®](https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html)ï¼ŸdatabaseSizeMB & journalSizeMBï¼Œå»æ‰è¿™äº›æˆ‘ä¿®æ”¹çš„å‚æ•°ï¼Œé‡æ–°éƒ¨ç½²ï¼ˆä¸éœ€åˆ é™¤å·²æœ‰åˆ†åŒºï¼‰ï¼Œç°åœ¨ DB åˆ†åŒºåªæœ‰ 1Gã€‚ 

å•ç‹¬æ·»åŠ æŸä¸ªç£ç›˜è¿˜[ä¸æ”¯æŒ](https://github.com/rook/rook/issues/1686)ï¼Œcurrently in Rook v0.7, there is only official support for adding and removing entire nodes (with all their disks and directories). 

ä¸€ä¸ªèŠ‚ç‚¹ä¸Šå¦‚æœ‰ç£ç›˜è¦åŠ å…¥ Cephï¼Œåˆ™èµ·ä¸€ä¸ª pod å³å¯ã€‚æ„Ÿè§‰æœ‰ç‚¹ä¸è¸å®ï¼šå¦‚æœè¿™ä¸ª pod killed äº†å‘¢ï¼Ÿå½“ç„¶ k8s liveness ä¼šå¸®å¿™èµ·ä¸€ä¸ª podï¼Œæ–°å»º pod ä¹Ÿéœ€è¦å…¶åŠ å…¥å·²æœ‰çš„ Ceph clusterï¼Œè¿™ä¸ªè¿‡ç¨‹å¦‚æœå¾ˆå®Œç¾åˆ™æ¯”è‡ªå·±ç»´æŠ¤ä¸€ä¸ª ceph é›†ç¾¤å¯é æ€§æ›´å¥½ã€‚è¿™ä¸ªæ˜¯ failed over ä¹ˆï¼ŸèŠ‚ç‚¹ä¸Šå¦‚æœèµ·å¤šä¸ª pod æ¥æä¾› HA å¯ä»¥ä¹ˆï¼Ÿæœ‰çŠ¶æ€çš„æ²¡æ³•è¿™ä¹ˆå¹²å§ã€‚ 

è¿è¡Œä¸€æ®µæ—¶é—´åï¼ŒèŠ‚ç‚¹è¿˜æ˜¯å¤±å»ååº”ï¼Œlibceph socket closed (con state connecting)ï¼Œå¥½çš„ä¸€ç‚¹åœ¨äºæ—¥å¿—åå‡ºå¹¶ä¸å—ï¼ŒCPU å ç”¨ç‡ä¹Ÿä¸é«˜ï¼Œä½†æ˜¯ç³»ç»Ÿè¿˜æ˜¯å¤±å»å“åº”ï¼Œå¾—å¼ºåˆ¶é‡å¯ã€‚å¯èƒ½åŸå› åœ¨äºæˆ‘å·²ç»åˆ é™¤äº† ceph clusterï¼Œå®ƒä¸ºäº†ä¿è¯æ•°æ®å®Œæ•´ï¼Œå†…æ ¸ä¸è®©é‡å¯ã€‚ç°åœ¨ä¼¼ä¹ä¸å°‘èŠ‚ç‚¹æ— æ³•å‘½ä»¤è¡Œé‡å¯ã€‚ 

æ²¡æœ‰è‡ªåŠ¨åˆ›å»ºçš„ [issue](https://github.com/rook/rook/issues/2134) æ— äººè§£ç­”ï¼Œè‡ªå·±æ¢ç´¢ã€‚ 
```
<== pkg/operator/ceph/cluster/osd/spec.go makeJob ä¼šåˆ›å»º prepare Job 
<== pkg/operator/ceph/cluster/osd/osd.go startProvisioningï¼Œè¿™ä¸ªæ–¹æ³•ä¼šéå† cluster.Storage.Nodes æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ²¡çœ‹åˆ°æœ‰æ¡ä»¶çš„åˆ›å»º job 
<== pkg/operator/ceph/cluster/osd/osd.go Start 
<== pkg/operator/ceph/cluster/cluster.go createInstance 
<== pkg/operator/ceph/cluster/controller.go onAdd & handleUpdate ( handleUpdate å®ä¸º onUpdate) 
```
è¿™äº› on æ–¹æ³•éƒ½æ˜¯ watch "kind: rook.Cluster" èµ„æºäº†å§ã€‚åªç›‘å¬ Cluster èµ„æºå½“ç„¶ä¸ä¼šåœ¨æ·»åŠ æ–°èŠ‚ç‚¹çš„æ—¶å€™è§¦å‘ä»¥ä¸Šè¿‡ç¨‹äº†ï¼æ€ªä¸å¾—æœ‰åœ°æ–¹å»ºè®®ç›´æ¥åˆ é™¤ Operator podï¼Œè¿™ç§æœ‰ç‚¹ç²—é²ã€‚å¯ä»¥ edit clusterï¼ŒuseAllNodes:falseï¼Œç„¶ååŠ ä¸Šæ–°èŠ‚ç‚¹çš„ ipï¼Œè¿™æ ·å¯ä»¥å§ã€‚ä¸çŸ¥é“è¿™æ ·æ˜¯å¦ä¼šå¯¹å·²æœ‰çš„èŠ‚ç‚¹ä¼šé€ æˆå½±å“ã€‚ 

ä¿®æ”¹ useAllNodes: falseï¼Œç„¶ååœ¨ nodes ä¸‹é¢å®šä¹‰çš„ä¸€ä¸ªèŠ‚ç‚¹ k8s-4 ã€‚éƒ¨ç½²é›†ç¾¤åï¼Œä¸€åˆ‡æ­£å¸¸ï¼Œç°åœ¨åªæœ‰ä¸€ä¸ª OSD serverã€‚ä¿®æ”¹ cluster.yamlï¼ˆkubectl edit é‡Œé¢æ²¡æœ‰èŠ‚ç‚¹éƒ¨åˆ†ï¼‰ï¼Œåœ¨åŠ å…¥ä¸€ä¸ªæ–°çš„èŠ‚ç‚¹ k8s-new-nodeï¼ŒOSD server éƒ½èƒ½é¡ºåˆ©åˆ›å»ºå‡ºæ¥ã€‚Operator æœ‰å¦‚ä¸‹æ—¥å¿—ï¼š 
```
2018-09-27 07:12:00.214724 I | op-cluster: update event for cluster rook-ceph 
2018-09-27 07:12:00.214800 I | op-cluster: The list of nodes has changed 
2018-09-27 07:12:00.214811 I | op-cluster: update event for cluster rook-ceph is supported, orchestrating update now 
2018-09-27 07:12:00.260502 I | op-mon: start running mons 
2018-09-27 07:12:00.265528 I | cephmon: parsing mon endpoints: a=10.110.87.214:6790,b=10.110.186.133:6790,c=10.97.189.6:6790 
2018-09-27 07:12:00.265684 I | op-mon: loaded: maxMonID=2, mons=map[b:0xc4209c76c0 c:0xc4209c7740 a:0xc4209c7640], mapping=&{Node:map[c:0xc420318840 a:0xc4203187e0 b:0xc420318810] Port:map[]} 
2018-09-27 07:12:00.276099 I | op-mon: saved mon endpoints to config map map[data:a=10.110.87.214:6790,b=10.110.186.133:6790,c=10.97.189.6:6790 maxMonId:2 mapping:{"node":{"a":{"Name":"k8s-2-new","Hostname":"k8s-2-new","Address":"192.168.51.12"},"b":{"Name":"k8s-3-new","Hostname":"k8s-3-new","Address":"192.168.51.13"},"c":{"Name":"k8s-4","Hostname":"k8s-4","Address":"192.168.51.14"}},"port":{}}] 
2018-09-27 07:12:00.276785 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config 
2018-09-27 07:12:00.277239 I | cephconfig: copying config to /etc/ceph/ceph.conf 
2018-09-27 07:12:00.277466 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph 
2018-09-27 07:12:00.289078 I | op-mon: Found 1 running nodes without mons 
2018-09-27 07:12:00.289120 I | op-mon: ensuring mon rook-ceph-mon-a (a) is started 
2018-09-27 07:12:00.394787 I | op-mon: mon a running at 10.110.87.214:6790 
2018-09-27 07:12:00.466938 I | op-mon: mon b running at 10.110.186.133:6790 
2018-09-27 07:12:09.280754 I | op-osd: 2 of the 2 storage nodes are valid 
2018-09-27 07:12:09.280775 I | op-osd: checking if orchestration is still in progress 
2018-09-27 07:12:09.283427 I | op-osd: start provisioning the osds on nodes, if needed 
2018-09-27 07:12:09.310899 I | op-osd: avail devices for node k8s-4: [{Name:vdb FullPath: Config:map[]}] 
2018-09-27 07:12:09.315175 I | op-osd: Removing previous provision job for node k8s-4 to start a new one 
2018-09-27 07:12:09.335221 I | op-osd: batch job rook-ceph-osd-prepare-k8s-4 still exists 
2018-09-27 07:12:11.341452 I | op-osd: batch job rook-ceph-osd-prepare-k8s-4 deleted 
2018-09-27 07:12:11.351147 I | op-osd: osd provision job started for node k8s-4 
2018-09-27 07:12:11.455374 I | op-osd: avail devices for node k8s-new-node: [{Name:vdb FullPath: Config:map[]} {Name:sda FullPath: Config:map[]} {Name:vda FullPath: Config:map[]}] 
2018-09-27 07:12:11.466879 I | op-osd: osd provision job started for node k8s-new-node 
2018-09-27 07:12:11.466905 I | op-osd: start osds after provisioning is completed, if needed 
2018-09-27 07:12:11.475870 I | op-osd: osd orchestration status for node k8s-4 is starting 
2018-09-27 07:12:11.475917 I | op-osd: osd orchestration status for node k8s-new-node is starting 
2018-09-27 07:12:11.475927 I | op-osd: 0/2 node(s) completed osd provisioning 
2018-09-27 07:12:12.519123 I | op-osd: osd orchestration status for node k8s-4 is computingDiff 
2018-09-27 07:12:12.652414 I | op-osd: osd orchestration status for node k8s-4 is orchestrating 
2018-09-27 07:12:12.665979 I | op-osd: osd orchestration status for node k8s-4 is completed 
2018-09-27 07:12:12.666037 I | op-osd: starting 1 osd daemons on node k8s-4 
2018-09-27 07:12:12.684588 I | op-osd: deployment for osd 0 already exists. updating if needed 
2018-09-27 07:12:12.688043 I | op-k8sutil: updating deployment rook-ceph-osd-0 
2018-09-27 07:12:14.713141 I | op-k8sutil: finished waiting for updated deployment rook-ceph-osd-0 
2018-09-27 07:12:14.713224 I | op-osd: started deployment for osd 0 (dir=false, type=) 
2018-09-27 07:12:14.897416 I | op-osd: osd orchestration status for node k8s-new-node is computingDiff 
2018-09-27 07:12:15.126273 I | op-osd: osd orchestration status for node k8s-new-node is orchestrating 
2018-09-27 07:12:30.533344 I | op-osd: osd orchestration status for node k8s-new-node is completed 
2018-09-27 07:12:30.533379 I | op-osd: starting 1 osd daemons on node k8s-new-node 
2018-09-27 07:12:30.564077 I | op-osd: started deployment for osd 1 (dir=false, type=) 
2018-09-27 07:12:30.596881 I | op-osd: 2/2 node(s) completed osd provisioning 
2018-09-27 07:12:30.596940 I | op-osd: checking if any nodes were removed 
2018-09-27 07:12:30.645332 I | op-osd: processing 0 removed nodes 
2018-09-27 07:12:30.645360 I | op-osd: done processing removed nodes 
2018-09-27 07:12:30.645369 I | op-osd: completed running osds in namespace rook-ceph 
2018-09-27 07:12:30.645518 I | exec: Running command: ceph osd unset noscrub --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/956465413 
2018-09-27 07:12:31.559860 I | exec: noscrub is unset 
2018-09-27 07:12:31.560148 I | exec: Running command: ceph osd unset nodeep-scrub --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/041764000 
2018-09-27 07:12:32.695130 I | exec: nodeep-scrub is unset 
2018-09-27 07:12:32.695290 I | op-cluster: Done creating rook instance in namespace rook-ceph 
2018-09-27 07:12:32.707479 I | op-cluster: succeeded updating cluster in namespace rook-ceph 
2018-09-27 07:12:32.707735 I | op-cluster: update event for cluster rook-ceph 
2018-09-27 07:12:32.707772 I | op-cluster: update event for cluster rook-ceph is not supported 
2018-09-27 07:12:32.710587 I | op-cluster: update event for cluster rook-ceph 
2018-09-27 07:12:32.710663 I | op-cluster: update event for cluster rook-ceph is not supported 
```
ä¿®æ”¹ cluster.yamlï¼Œåˆ é™¤ name: "k8s-new-nodeâ€ï¼Œosd pod ç­‰ä¼šå°±ä¼šè¢«åˆ é™¤ï¼Œä¸é”™ã€‚ 

æŒ‰è®¾è®¡æ¥è¯´ï¼Œoperator ç¡®å®åªéœ€ç›‘å¬ CRD èµ„æºçš„å®šä¹‰å°±å¯ä»¥ï¼Œæ¯•ç«Ÿè¿™ä¸ªæè¿°å®šä¹‰æ‰€æœ‰çš„å†…å®¹ã€‚ä½†æ˜¯å¦‚ä½•å¤„ç† **useAllNodes:true**ï¼Ÿåœ¨ä»£ç é‡Œé¢ç›‘å¬èŠ‚ç‚¹å˜åŒ–å½“ç„¶å¯ä»¥ï¼Œä½†å¦‚æ­¤ä¸€æ¥ï¼Œç›‘å¬çš„ä¸œè¥¿å°±å¤šçš„å»äº†ï¼Œèƒ½å¦ç”¨å£°æ˜å¼çš„æ–¹æ³•å†™åœ¨ CRD æè¿°é‡Œé¢ï¼Ÿè¿™æ ·ç›‘å¬çš„è®¢é˜…å’Œå–æ¶ˆéƒ½ç”±å¤–éƒ¨å¤„ç†ã€‚ 

"kind: Clusterâ€ æŒ‡çš„æ˜¯ ceph clusterï¼Œæ‰€ä»¥å­—é¢ä¸Šç¡®å®å’Œ k8s cluster ä¸åŒï¼Œè¿™ç‚¹è¦æ³¨æ„ã€‚ 

åŠ å…¥æ–°èŠ‚ç‚¹æ²¡æœ‰ç›‘å¬è¿˜å¯ä»¥è§£é‡Šï¼Œå·²åŠ å…¥çš„èŠ‚ç‚¹æŒ‚èµ·æ˜¯å¦ç›‘å¬å‘¢ï¼Ÿå¦‚æœæ²¡æœ‰è¿™ä¸ªå¯ç”¨æ€§å¤§æ‰“æŠ˜æ‰£å•Šã€‚ 

åªç›‘å¬ CRD å°±è¶³å¤Ÿçš„è¯ï¼Œè¿˜è¦ discover & agent pod åšä»€ä¹ˆå‘¢ï¼Ÿå…¶å®è¿™ä¸¤ä¸ªå®ˆæŠ¤è¿›ç¨‹å¯ä»¥åšåˆ° ã€watch k8s clusterã€åŒæ ·æ•ˆæœã€‚ã€‚ã€‚ 

è¿™ä¸ªç°åœ¨åœ¨ 0.9 milestone ä¸­ <https://github.com/rook/rook/milestone/10> 

Cluster åˆ é™¤éš¾çš„é—®é¢˜åå¤å‡ºç°ï¼Œ[ceph-teardown](https://rook.io/docs/rook/v0.8/ceph-teardown.html) å†™äº†è¦åˆ é™¤ CRD çš„ finalizerã€‚ä»Šå¤©æŸ¥äº†ä¸‹æ ¹æºï¼Œåˆ é™¤æ—¶ operator æœ‰æ—¥å¿—ï¼š 
```
2018-09-27 06:17:08.606342 I | op-cluster: cluster rook-ceph has a deletion timestamp 
2018-09-27 06:17:08.616067 I | op-cluster: waiting for volume attachments in cluster rook-ceph to be cleaned up. Retrying in 2s. 
2018-09-27 06:17:36.696588 W | op-cluster: exceeded retry count while waiting for volume attachments for cluster rook-ceph to be cleaned up. vols: [{TypeMeta:{Kind:Volume APIVersion:rook.io/v1alpha2} ObjectMeta:{Name:pvc-8cb3ff34-bc95-11e8-86d6-aa90b3cbae01 GenerateName: Namespace:rook-ceph-system SelfLink:/apis/rook.io/v1alpha2/namespaces/rook-ceph-system/volumes/pvc-8cb3ff34-bc95-11e8-86d6-aa90b3cbae01 UID:8ebfb7f1-bc95-11e8-86d6-aa90b3cbae01 ResourceVersion:10884495 Generation:1 CreationTimestamp:2018-09-20 05:25:16 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[] OwnerReferences:[] Initializers:nil Finalizers:[] ClusterName:} Attachments:[{Node:k8s-3-new PodNamespace:default PodName:toned-waterbuffalo-redis-master-0 ClusterName:rook-ceph MountDir:/var/lib/kubelet/pods/8cb71bfa-bc95-11e8-86d6-aa90b3cbae01/volumes/ceph.rook.io~rook-ceph-system/pvc-8cb3ff34-bc95-11e8-86d6-aa90b3cbae01 ReadOnly:false}]}] 
2018-09-27 06:17:36.700940 E | op-cluster: failed to remove finalizer cluster.ceph.rook.io from cluster rook-ceph. Operation cannot be fulfilled on clusters.ceph.rook.io "rook-ceph": the object has been modified; please apply your changes to the latest version and try again 
```
è¿™ä¸ªç›®å½•ç¡®å®å­˜åœ¨ï¼Œä½†æ˜¯ Redis å·²ç»ä¸åœ¨äº†ã€‚å¯èƒ½æ˜¯æˆ‘ä¸Šæ¬¡å¼ºåˆ¶åˆ é™¤æ—¶ç•™ä¸‹çš„ç—•è¿¹ã€‚è¿™ä¸ª operator æœ‰è®°å½•æ¯æ¬¡åˆ›å»ºçš„ pvï¼Ÿä½†æ˜¯ä¸æ˜¯åªæœ‰ Cluster è¿™ä¸ª resource ä¹ˆï¼Ÿ 
```shell
[centos@k8s-1 ceph]$ kubectl get volumes.rook.io --all-namespaces
NAMESPACE          NAME                                       AGE
rook-ceph-system   pvc-8cb3ff34-bc95-11e8-86d6-aa90b3cbae01   7d
```
æœç„¶è¿˜æœ‰å¦å¤–ä¸€ä¸ª CRDã€‚ä¸Šé¢ç›®å½•å¯ä»¥ç›´æ¥åˆ é™¤ï¼Œç„¶ååˆ é™¤ volume crdã€‚åˆ é™¤ cluster.ceph.rook.io ä¾ç„¶æœ‰é”™è¯¯ï¼š 

    op-cluster: failed to remove finalizer cluster.ceph.rook.io from cluster rook-ceph. Operation cannot be fulfilled on clusters.ceph.rook.io "rook-ceph": the object has been modified; please apply your changes to the latest version and try again 

ä¸€èˆ¬å­˜å‚¨æ˜¯å•ç‹¬éƒ¨ç½²çš„ï¼Œæœ‰è‡ªå·±çš„æ§åˆ¶ç½‘ç»œå’Œæ•°æ®ç½‘ç»œã€‚è¿™ç§è®© Ceph è¿è¡Œåœ¨è®¡ç®—é›†ç¾¤å†…çš„æ–¹å¼æ˜¯å¦ç¨³å®šå‘¢ï¼Ÿç”Ÿäº§ç¯å¢ƒå¦‚ä½•å¤„ç†ï¼ŸæŸ¥äº†ä¸‹ï¼Œå¯ä»¥ç”¨ nodeSelector è®© pod è¿è¡Œåœ¨ç‰¹å®šçš„èŠ‚ç‚¹ä¸Šï¼Œä½†æ˜¯æ²¡æœ‰æ‰¾åˆ°æ–¹æ³•è®©èŠ‚ç‚¹åªè¿è¡Œ Ceph ç›¸å…³ pod è€Œè¿›è¡Œéš”ç¦»ã€‚ä½¿ç”¨ [admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) å‡†å…¥æ§åˆ¶å™¨ PodNodeSelector å¯ä»¥å®šåˆ¶æ§åˆ¶ã€‚è¿˜æ˜¯ç®—äº†ï¼Œå³ä¾¿çº¦æŸäº†æˆ‘çŒœè¿™ç§æ··åˆéƒ¨ç½²çš„æ–¹å¼ç¨³å®šæ€§ä¹Ÿæ²¡æé«˜å¤šå°‘ã€‚å•ç‹¬çš„éƒ¨ç½²ä¸€ä¸ªæ–°çš„é›†ç¾¤ï¼Ÿé‚£è®¡ç®—é›†ç¾¤é‡Œé¢çš„ StorageClass å°±å¾—ä½¿ç”¨æ ‡å‡†çš„ Ceph brd è¿æ¥ï¼ˆé…ç½®ä¸€å † monitorï¼‰ï¼Ÿ 

rook-ceph namespace å·²ç»æš´éœ²å‡ºäº† rook-ceph-mon-[a,b,c] ä¸‰ä¸ª serviceï¼ŒClusterIPã€‚è¿™ä¸ªç½‘ç»œæ˜¯ weave ç½‘ç»œï¼Œä¸çŸ¥é“æ€§èƒ½å¦‚ä½•ã€‚æœ‰ä¸ª[é…ç½®](https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html) hostNetworkï¼Œuses network of the hosts instead of using the SDN below the containers. <https://github.com/rook/rook/issues/561> hostnetwork support 

è€ƒè™‘åˆ°å®¹å™¨ç½‘ç»œçš„å¤æ‚å’Œå„ç§ policy æ§åˆ¶ï¼ŒCeph è¿™ç§å¤§æ•°æ®é‡çš„ä¼¼ä¹ä¸é€‚åˆå®¹å™¨åŒ–ï¼Œé€‚åˆçš„åœ°æ–¹åœ¨äºç›‘æ§å’Œæ¢å¤ã€‚ å¦‚æœåº”ç”¨å’Œ Ceph åœ¨åŒä¸€é›†ç¾¤ï¼Œåˆ™ç½‘ç»œé—®é¢˜ä¼¼ä¹å¹¶ä¸å¤§ã€‚

### å…¶ä»–å­˜å‚¨

è¿™ä¸ª Rook ç°åœ¨å·²ç»æ”¯æŒå¾ˆå¤šå­˜å‚¨ï¼šCeph, CockroachDB, Minio, Cassandra, EdgeFS 

è¿™ä¸ªå¦‚æœæ”¯æŒ Gluster ä¼¼ä¹æ˜¯å¾ˆå¥½çš„ï¼Œæ—¢ç„¶éƒ½æœ‰å¯¹ç£ç›˜æœ‰æ”¯æŒã€‚ [æš‚æ—¶ä¸æ”¯æŒ](https://github.com/rook/rook/issues/2044)ï¼Œè¯´æ˜¯ heketi çš„å­˜å‚¨æ¨¡å‹åªèƒ½è®©ä¸€ä¸ªè¿›ç¨‹æ‰“å¼€ã€‚ è™½ç„¶ Gluster å·²ç»å¤Ÿç®€å•ï¼Œä½†æ˜¯æˆ‘çœ‹äº†ä¸‹å…¶ Samba æ–¹æ¡ˆæŒºå¤æ‚ï¼ŒçœŸä¸æƒ³é…ç½®ï¼Œæˆ‘åªæ˜¯æƒ³è¯•è¯•è€Œå·²ï¼Œè‡ªå·±é…ç½®è‚¯å®šæœ‰å¾ˆå¤šå‘ã€‚åœ¨æ·±å…¥äº†è§£ä¸€ä¸ªä¸œè¥¿å‰å¹¶å‡†å¤‡çœŸæ­£å»ç”¨å‰ï¼Œä¸ºä»€ä¹ˆè¦è¶Ÿè¿™äº›å‘å‘¢ï¼Ÿè€Œä¸”è¿™äº›å‘ä¼šè®©äººçŸ¥éš¾è€Œé€€ã€‚è€Œè¿™äº›ä¹Ÿæ˜¯å¼€æºè™½ç„¶ç«çƒ­ä¹Ÿæœ‰å…¶ç—›ç‚¹ï¼Œäº‘æœåŠ¡å‚å•†çš„ PaaS çš„å¼€ç®±å³ç”¨ä¼˜åŠ¿ä¾¿åœ¨è¿™é‡Œã€‚å¦‚æœè¯´ IaaS åªæ˜¯æ–¹ä¾¿äº†è¿ç»´ï¼ŒPaaS é¢å‘çš„åˆ™æ˜¯å¼€å‘è€…ã€‚ 

EdgeFS <https://rook.io/docs/rook/v0.9/edgefs-storage.html> <http://edgefs.io/> <https://itnext.io/edgefs-cluster-with-rook-in-google-cloud-885227625b9b> Geo-transparentï¼Œæˆ‘ç†è§£æ˜¯å¤šäº‘ç¯å¢ƒçš„ä¸åŒçš„æ•°æ®ä¸­å¿ƒçš„ db åˆèµ·æ¥ä¸ºä¸€ä¸ªç»Ÿä¸€çš„ dbã€‚è¿™ä¸å’Œ CockroachDB ä¸€æ ·äº†ä¹ˆã€‚ 

ä»æœ€æ—©å¯¹ Rook çš„æ€€ç–‘ï¼Œåˆ°ç°åœ¨ Rook çš„è“¬å‹ƒå‘å±•ï¼ŒçœŸæ˜¯æœ‰è¶£çš„äº‹æƒ…ã€‚ <https://github.com/rook/rook/releases> çœ‹çœ‹æ¯ä¸ª release çš„ feature ä¹Ÿæ˜¯ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…ã€‚ 

å•ç‹¬å‡çº§cephï¼Œ [What version of Ceph would you like with that?](https://blog.rook.io/what-version-of-ceph-would-you-like-with-that-8275f487cdae)ï¼Œ[Decoupling the Ceph version](https://github.com/rook/rook/blob/master/design/decouple-ceph-version.md)  