---
layout: post
title:  "Helm / Monocular"
date:   2018-4-19 14:20:33
typora-root-url: ../../../blog
---
### å®‰è£…
- <https://github.com/kubernetes-helm/monocular>  
- KubernetesåŒ…ç®¡ç†å™¨Helmç°åœ¨ç”±CNCFæ‰˜ç®¡ <http://www.infoq.com/cn/news/2018/06/helm-hosted-cncf> 
- Google Charts <https://github.com/helm/charts> 
- helm upgrade jxing stable/nginx-ingress å‡çº§å·²æœ‰ chart 
0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 node(s) didn't have free ports for the requested pod ports. ç«¯å£æ²¡æœ‰é‡Šæ”¾ï¼Œå‡çº§ç­–ç•¥å¯¼è‡´ï¼Ÿ 

```
[fedora@kong-ct5vbhkzjgv2-master-0 ~]$ helm ls 
Error: configmaps is forbidden: User "system:serviceaccount:kube-system:default" cannot list configmaps in the namespace "kube-system" 
[fedora@kong-ct5vbhkzjgv2-master-0 ~]$ kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default 
clusterrolebinding "add-on-cluster-adminâ€ created 
```
<https://github.com/helm/helm/issues/2687>

nginx-ingress æ˜¯åšä½•è€Œç”¨ï¼Ÿä¼¼ä¹æ˜¯ä½œä¸ºå‰ç«¯ä»£ç†ã€‚ 

if "helm install stable/nginx-ingress", then I can get Ingress Ip in "kubectl get ingress" soon. But can't access it. If "helm install stable/nginx-ingress --set controller.hostNetwork=true" which is for Kubeadm, no IP and I get IP directly from Dashboard and can access it. After long time, this way can get IP also. 

nginx-ingress is an Ingress controller that uses ConfigMap to store the nginx configuration. nginx é…ç½®åœ¨ ui-vhost è¿™ä¸ª configmap é‡Œé¢ï¼Œç„¶åè¢«monocular/deployment/monocular/templates/ui-deployment.yaml ä½¿ç”¨å’Œ mountï¼Œä½†æ˜¯ ui server è¦ nginx é…ç½®æœ‰ä½•ç”¨å‘¢ï¼Ÿè¿™ä¸ªé…ç½®éœ€è¦å‘Šè¯‰ nginxå•Šã€‚nginxæ˜¯å¦‚ä½•åŠ¨æ€è·å–è¿™ä¸ªé…ç½®çš„ï¼Ÿ 

monocular/deployment/monocular/templates/ingress.yaml  å®šä¹‰äº† Ingress meta ä¿¡æ¯ï¼Œç„¶å nginx-ingress controllerä¼šè·å–è¿™ä¸ªå…¬å…±çš„ä¿¡æ¯ã€‚ingress controller ä¼šæ¶ˆåŒ–è¿™ä¸ª rules ä¿¡æ¯å¹¶å°†å…¶è½¬æ¢ä¸ºè‡ªå·±å®ç°æ‰€éœ€çš„é…ç½®ã€‚ 

After deploy, error. mongodb pod say "Unable to mount volumes for pod "anxious-hog-mongodb-mvx69_default(e9e)": timeout expired waiting for volumes to attach/mount for pod "default"/"anxious-hog-mongodb-mvx69". list of unattached/unmounted volumes=[data]" 

And There is a pending PV claim request. It need 8Gi storage. So just enlarge my PV? Yes. After enlarge. The pending claim turn to Green. I search monocular code and didn't find PV claim description. But the error pods didn't refresh/re-create automatically.  `helm delete/install`. 

Re-create ceph pool: sudo rbd create real-pool/doo --size 10096, map, format, mount, umount. 

mongodb installed on vm3. On vm3, run `journalctl --no-pager -u kubelet -n 100` to check recent 100 log. And looks vm3 didn't configure ceph. Add it to make sure `sudo rbd list` running successfully.  No. I have defined secretRef. 

Ceph å¤ªéº»çƒ¦ï¼Œæ”¹æˆ NFS çš„ PVï¼Œè¿™é‡Œ helm è¿˜è¦ReadWriteOnceæ¨¡å¼ï¼Œç‹¬å å¼çš„ï¼Ÿç„¶å claim å°±æˆç»¿è‰²çš„äº†ã€‚ç„¶åä¸ç”¨é‡æ–°å‘å¸ƒ monocularï¼Œå®ƒä¼šè‡ªå·±åå¤å°è¯•ï¼Œç„¶åæˆ‘åœ¨ Nfsç›®å½•ä¸‹å°±å¯ä»¥çœ‹åˆ° conf/mongodb.conf æ–‡ä»¶ã€‚è¿™ä¸ª PV ä¼¼ä¹æ¯æ¬¡ monocular install åå°±ä¸èƒ½ç”¨äº†ï¼Œåªèƒ½åˆ é™¤åé‡å»ºã€‚åæ¥åœ¨åå¤ monocular install è¿‡ç¨‹ä¸­ï¼Œæˆ‘åˆ é™¤äº† nfs ä¸‹é¢çš„æ–‡ä»¶ï¼Œç„¶å mongo pod å°±æŠ¥é”™è¯´ conf æ‰¾ä¸åˆ°ã€‚éš¾é“è¿™ä¸ª pod ä¸çŸ¥é“æˆ‘å·²ç»åˆ é™¤äº†ä¹ˆï¼Ÿ 

Error executing 'postInstallation': Path '/bitnami/mongodb/conf' does not exists. æ¢ä¸€ä¸ª nfs åå­—è¿˜æ˜¯ä¸è¡Œï¼Œåæ¥å‘ç° nfs æ ¹ç›®å½•ä¸‹é¢æœ‰ä¸ª .initialized æ–‡ä»¶ï¼Œæˆ‘æ—¥ï¼Œè¿™æ ·æ¥åˆå§‹åŒ–æœ‰ç‚¹lowå•Šã€‚åˆ é™¤ monogo podï¼Œconf data æ–‡ä»¶å°±è‡ªåŠ¨äº§ç”Ÿäº†ã€‚ 

æ€»çš„æ¥è¯´ï¼Œapi server ä¸‹è½½ charts æœ‰ç‚¹æ¼«é•¿ï¼Œæ•´ä¸ªè¿‡ç¨‹ä¹Ÿæœ‰ç‚¹æ¼«é•¿ã€‚ 

api pods åœ¨è¿ä¸Š mongo db åï¼Œä¼šä»https://kubernetes-charts.storage.googleapis.comä¸‹è½½å¤§é‡ä¸œè¥¿ï¼Œç„¶åå­˜åˆ° mongo ä¸­å»ã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰ç‚¹æ¼«é•¿ï¼Œè¿‡ç¨‹ä¸­ pod æ˜¾ç¤ºä¼šæœ‰é”™è¯¯ï¼Œè¿™ä¸ªä¸å¤§å¥½ã€‚ç„¶åè®¿é—® UI æ—¶ï¼Œkubectl get ingress æ²¡æœ‰æ˜¾ç¤º IPï¼Œéšä¾¿æ‰¾äº†ä¸ªç•Œé¢å¯ä»¥è®¿é—®ï¼Œä½†æ˜¯æ‰€æœ‰/api/å¼€å¤´çš„éƒ½è¿”å›404ï¼Œä¼¼ä¹è¿™ä¸ª Proxy æ²¡æœ‰è½¬å‘ã€‚"helm install stable/nginx-ingress --set controller.hostNetwork=true" è¿™ç§æ–¹æ³•åˆšå¼€å§‹ä¹Ÿæ˜¯404é”™è¯¯ï¼Œè¿‡ä¸€æ®µæ—¶é—´åokäº†ï¼Œkubectl get ingress ä¹Ÿä¼šè¿”å›IPã€‚å¦å¤–ä¸€ç§å®‰è£… nginx-ingressçš„æ–¹æ³•ä¸€ç›´404ã€‚ Run`wget localhost:8081/v1/charts`  in API Pod terminal can return json. On 404 cluster, this API works too. So something was wrong in Ingress. 

DEBUG. Nginx-ingress-controller æœ‰å¦‚ä¸‹ logï¼š 
```
192.168.18.2 - [192.168.18.2] - - [20/Apr/2018:07:55:25 +0000] "GET /assets/icons/menu.svg HTTP/2.0" 200 215 "https://192.168.51.13/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36" 33 0.001 [default-affable-worm-monocular-ui-80] 10.40.0.2:8080 215 0.001 200 
192.168.18.2 - [192.168.18.2] - - [20/Apr/2018:07:55:26 +0000] "GET /api/v1/charts HTTP/2.0" 404 19 "https://192.168.51.13/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36" 27 0.376 [default-affable-worm-monocular-api-80] 10.40.0.4:8081 19 0.376 404 
```
10.40.0.4:8081 è¿™ä¸ªIP æ‰€åœ¨ API POD æˆ‘è¿›å…¥å‘½ä»¤è¡Œï¼Œ`wget localhost:8081/v1/charts`å¯ä»¥è·å–ã€‚åœ¨nginx-ingress controller å‘½ä»¤è¡Œä¸‹ï¼Œç›´è¿api pod å¯ä»¥ï¼Œä½†æ˜¯ curl -k [https://localhost:/api/v1/charts](https://localhost/api/v1/charts) ä¸æˆåŠŸã€‚ç›´æ¥æŸ¥çœ‹ /etc/nginx/nginx.conf å‘ç°é‡Œé¢çš„å†…å®¹å¥½çš„åçš„ç›¸å·®å¾ˆå¤§ï¼š 
```
 upstream default-exacerbated-parrot-monocular-api-80 { 
        # Load balance algorithm; empty for round robin, which is the default
        least_conn;
        keepalive 32;
        server 10.40.0.11:8081 max_fails=0 fail_timeout=0;
        server 10.32.0.13:8081 max_fails=0 fail_timeout=0;
    }
    server {
        location ~* ^/api/(?<baseuri>.*) {
            set $proxy_upstream_name "default-exacerbated-parrot-monocular-api-80";
            # In case of errors try the next upstream server before returning an error
            proxy_next_upstream                     error timeout invalid_header http_502 http_503 http_504;
            rewrite /api/(.*) /$1 break;
            rewrite /api/ / break;
       proxy_pass http://default-exacerbated-parrot-monocular-api-80;
    }
```

é”™è¯¯çš„é‡Œé¢æ²¡æœ‰ä¸Šé¢çš„ rewriteï¼Œè¡¨ç°å’Œè¿™ä¸ªä¸€æ¨¡ä¸€æ ·ï¼š<https://github.com/kubernetes-helm/monocular/issues/422> ã€‚ä½†æ˜¯æˆ‘å¥½çš„æ˜¯ç¬¬äºŒå¤©éƒ¨ç½²çš„ï¼Œç¬¬äºŒå¤©çš„ Docker image å°±ä¿®å¤äº†è¿™ä¸ªé—®é¢˜ï¼Ÿå°è¯•æŒ‰ç…§ä¸Šé¢åŠ ä¸Šrewriteçš„å®šåˆ¶ï¼Œç„¶åå°±OKäº†ã€‚å¯èƒ½åçš„é›†ç¾¤ä¸Šé¢åˆ é™¤ image cache å¯èƒ½ä¼šå¥½å§ã€‚ 

åˆ é™¤å·²å®‰è£…çš„ helmsï¼Œå…¶åˆ é™¤åè¿”å›chartsåˆ—è¡¨ï¼Œè¿™ä¸ªæœ‰ç»†ç²’åº¦çš„æ§åˆ¶ã€‚ 
```
[root@k8s-1 centos]# kubectl get service 
NAME                                       TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE 
coiled-fox-nginx-ingress-controller        LoadBalancer   10.109.246.144   <pending>     80:30827/TCP,443:30194/TCP   22d 
coiled-fox-nginx-ingress-default-backend   ClusterIP      10.103.27.158    <none>        80/TCP                       22d 
[root@k8s-1 centos]# curl  10.103.27.158 
default backend - 404 
```
default-backend æ˜¯ä¸€ä¸ªfallbackçš„serverã€‚é‚£ç³»ç»Ÿä¸­åªèƒ½ç”±ä¸€ä¸ªingress-controllerä¹ˆï¼Ÿæ—¢ç„¶monocularå®šä¹‰çš„ingresså¹¶æ²¡æœ‰åˆ¶å®šç”±é‚£ä¸ªcontrolleræ¥è½¬å‘ï¼Œè¿™ä¸ªæ˜¯ä»»æ„æŒ‡æ´¾çš„ï¼Ÿæˆ‘è‡ªå·±åˆéƒ¨ç½²äº†ä¸€ä¸ªï¼Œhelm install stable/nginx-ingress --set controller.hostNetwork=trueã€‚è¿™æ¬¡åˆ†é…åˆ°å¦å¤–ä¸€ä¸ªip(å¯èƒ½è¿™ä¸ªæ˜¯controller.hostNetwork=trueçš„æ•ˆæœ)ã€‚å°è¯•å‘å¸ƒåº”ç”¨ï¼Œæ„Ÿè§‰è¿™ä¸¤ä¸ªLBå…¶å®æ˜¯ä¸€ä¸ªå•Šã€‚ 

### å¤šæ¬¡é‡å¯çš„é—®é¢˜
![monocular-restart](/images/2018/monocular-restart.png)
å’Œingressæœ‰å…³ <https://github.com/kubernetes-helm/monocular/issues/444> Helm å®‰è£…ä¼šæŒ‚èµ·çš„é—®é¢˜ <https://github.com/kubernetes/helm/issues/2224#issuecomment-356344286> 

åˆå§‹åŒ–çš„æ—¶å€™å¤šæ¬¡é‡å¯çš„é—®é¢˜ <https://github.com/kubernetes-helm/monocular/issues/450>ï¼Œå› ä¸ºåœ¨ä¸‹è½½å¤§é‡ chartsã€‚è€Œä¸”æœ‰ä¸¤ä¸ª api serverï¼Œè¯´æ˜æœ‰ HA äº†ï¼Œä½†ä¸ºä»€ä¹ˆå¥½çš„é‚£ä¸ªä¸æœåŠ¡å‘¢ï¼Ÿ 

Readiness probe failed: Get http://10.32.0.32:8081/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 

æˆ‘è¯•äº†ä¸‹ï¼Œcurl http://10.32.0.32:8081/healthz -v è¿”å› 200ï¼Œlog é‡Œé¢ä¹Ÿå¯ä»¥çœ‹åˆ°ã€‚ 

çœ‹ä»£ç å·²ç»åšäº† go channelï¼Œç›¸åº”ä¸åŠæ—¶è€Œå·²ï¼Ÿmonocular/src/api/data/cache/cache.go Refresh() æ–¹æ³•åˆ·æ–°æ•°æ®ï¼Œè¿™ä¸ªé—®é¢˜ä¸å¥½é‡æ–°å•Šã€‚ 

æœ‰æ—¶å€™é”™è¯¯åˆä¸º Liveness probe failed: Get http://10.40.0.50:8081/healthz: read tcp 10.40.0.0:50584->10.40.0.50:8081: read: connection reset by peer 
```
time="2018-06-26T05:56:59Z" level=info msg="Configuration bootstrap init" configFile="/monocular/config/monocular.yaml" 
time="2018-06-26T05:56:59Z" level=info msg="Configuration file found!" 
time="2018-06-26T05:56:59Z" level=info msg="Loading CORS from config file" 
time="2018-06-26T05:56:59Z" level=info msg="Loading repositories from config file" 
time="2018-06-26T05:56:59Z" level=info msg="Configuration bootstrap finished" 
time="2018-06-26T05:56:59Z" level=info msg="Using cache directory" path="/monocular/repo-dataâ€ â€”â€”>è¿™é‡Œç­‰äº†åŠå¤© 
time="2018-06-26T05:57:22Z" level=info msg="Deprecated chart skipped" name=telegraf... 
time="2018-06-26T05:57:22Z" level=info msg="Deprecated chart skipped" name=kubed 
time="2018-06-26T05:57:23Z" level=info msg="Local cache missing" name=dex version=0.1.0 
time="2018-06-26T05:57:23Z" level=info msg="Downloading metadata" dest="/monocular/repo-data/stable/dex/0.1.0/chart.tgz" source="https://kubernetes-charts.storage.googleapis.com/dex-0.1.0.tgz" 
time="2018-06-26T05:57:23Z" level=info msg="Storing in cache" path="/monocular/repo-data/stable/dex/0.1.0/README.md" 
time="2018-06-26T05:57:23Z" level=error msg="Error on DownloadAndExtractChartTarball" error="open /tmp/chart653157453/dex/README.md: no such file or directory" 
time="2018-06-26T05:57:27Z" level=info msg="Deprecated chart skipped" name=burrow... 
time="2018-06-26T05:57:27Z" level=info msg="Deprecated chart skipped" name=kubewatch 
time="2018-06-26T05:57:28Z" level=info msg="Storing in cache" path="/monocular/repo-data/incubator/fluentd/0.1.4/README.md" 
time="2018-06-26T05:57:28Z" level=error msg="Error on DownloadAndExtractChartTarball" error="open /tmp/chart801321288/fluentd/README.md: no such file or directory" 
time="2018-06-26T05:57:28Z" level=warning msg="authentication is disabled" error="no signing key, ensure MONOCULAR_AUTH_SIGNING_KEY is set" 
time="2018-06-26T05:57:28Z" level=info msg="Started Monocular API server" addr=":8081" 
```
[negroni] 2018-06-26T05:57:33Z | 200 | 130.925Âµs | 10.40.0.44:8081 | GET /healthz â€”>å¼€å§‹å“åº”probeness 

è¿™ä¸ªæ˜¯ container å¯åŠ¨çš„æ—¥å¿—ï¼Œè€—æ—¶è¾ƒé•¿ï¼Œè¿™ä¸ªæ—¶å€™é‡å¯è¿˜æƒ…æœ‰å¯åŸï¼Œä½†æ˜¯èµ·æ¥ååº”è¯¥å°±ä¸€ç›´èƒ½å“åº” /healthz äº†å•Šï¼Ÿ/healthz å’Œ update log æœ‰æ—¶å€™æ˜¯ç©¿æ’è¿›è¡Œçš„ï¼Œä¼¼ä¹update å¹¶æ²¡æœ‰block /healthzã€‚ 
![efk-log-1](/images/2018/efk-log-1.png)

ä»ä¸Šé¢çœ‹ï¼Œ17 * 24 = 408ï¼Œé‚£æ¯ä¸ªä¸€å°æ—¶é‡å¯ä¸€æ¬¡äº†ï¼Ÿè¿™ä¸ªå’Œ main.go é‡Œé¢è®¾ç½®çš„ cacheRefreshInterval æ˜¯ä¸€è‡´çš„ã€‚ 

cache.go.Refresh é‡Œé¢å·²ç»åšäº† go channelï¼Œè¯´æ˜æ˜¯å¼‚æ­¥çš„ï¼Œä¸ºä»€ä¹ˆä¸å“åº” /healthz å‘¢ï¼Ÿ 

![efk-log-2](/images/2018/efk-log-2.png)

ä¸Šå›¾æ˜¯ chart åˆæ¬¡å®‰è£…æ—¶çš„ EFKï¼Œå¯ä»¥çœ‹åˆ°å‡†ç¡®çš„æ¯éš” 5 åˆ†é’Ÿåé‡å¯ä¸€æ¬¡ã€‚ 
```
"livenessProbe": { 
â€‹          "httpGet": { 
â€‹            "path": "/healthz", 
â€‹            "port": 8081, 
â€‹            "scheme": "HTTP" 
â€‹          }, 
â€‹          "initialDelaySeconds": 180, 
â€‹          "timeoutSeconds": 10, 
â€‹          "periodSeconds": 10, 
â€‹          "successThreshold": 1, 
â€‹          "failureThreshold": 3 
â€‹        } 
```
è¿™ä¸ªåˆèµ·æ¥æ˜¯ 180 + (10 + 10) *3ï¼Œæ²¡æœ‰ 300 ç§’å•Šï¼Ÿå·®ä¸å¤šå§ã€‚ 

å‰é¢é—´éš”çŸ­æ˜¯/healthzï¼Œåé¢çš„æ¯æ¬¡éƒ½æ˜¯ä¸Šé¢çš„å‡ ä¸ª logï¼Œé—´éš” 5 åˆ†é’Ÿã€‚è¿™æ—¶å€™æ˜¯ Readiness probe failed çš„ã€‚æ¯æ¬¡éƒ½åœ¨é‡å¯ï¼Ÿåœ¨æˆåŠŸç›¸åº” /healthz å‰æœ‰æ¬¡ç¡®å®åœ¨æ‹‰å– chart çš„æ—¥å¿—ï¼Œç„¶åå°±é©¬ä¸Šæ¢å¤ /healthz äº†ã€‚ 

åœ¨ master node ä¸Šé¢è¿è¡Œ journalctl --no-pager -u kubelet æ˜¾ç¤ºäº†ä¸€å † monocular çš„æ—¥å¿—ï¼Œå…¶ä¸­ï¼š 
```
  Container {Name:monocular Image:bitnami/monocular-apiâ€¦ is dead, but RestartPolicy says that we should restart it. 
```
è¿™ä¸ªå‡ºç°é—´éš”ä¸º 16 ç§’ï¼Œprobe çš„é—´éš”ï¼Ÿ 

æŒ‰ç…§ Started Monocular API server è¿‡æ»¤ï¼Œç¡®å®æ˜¯æ¯éš”ä¸€å°æ—¶æˆåŠŸé‡å¯ä¸€æ¬¡ã€‚ 

å°è¯•ä¿®æ”¹å…¶æºä»£ç ï¼šmonocular/src/api, make bootstrapï¼Œéœ€è¦å…ˆè¿è¡Œè¿™ä¸ªï¼Œå¦åˆ™ go ä¼šæŠ¥æ‰¾ä¸åˆ°ä¾èµ–çš„é”™è¯¯ã€‚ç„¶åè¿è¡Œ src/api ä¸‹é¢è¿è¡Œ make docker-buildï¼Œç¼–è¯‘æˆåŠŸçš„æ˜¯ linux æ ¼å¼ã€‚è¿™ä¸ªæ˜¯è¿è¡Œåœ¨Docker é‡Œé¢ï¼Œä»–åº”è¯¥ä¼šæŠŠä¾èµ–ä¿å­˜åœ¨ Docker Volume é‡Œé¢ã€‚ 

é»˜è®¤çš„ push URL åœ°å€æ˜¯ bitnami/monocular-api:latestï¼Œç°åœ¨æ”¹ä¸º fkpwolf/monocular-api:x.x on docker hub.  

è¿™ä¸ªmonocularåˆ†ä¸ºui, api, prerenderï¼Œåˆå§‹åŒ–chart cacheæ˜¯prerenderåšçš„ï¼Ÿä½†æ˜¯æˆ‘çœ‹åˆ°å¾ˆå¤šchartçš„ä¸‹è½½æ˜¯åœ¨apié‡Œé¢åšçš„ï¼Œä½†æ˜¯apiåˆä¸¤ä¸ªå•Šï¼Œè¿™ä¸ªä¸å¼•å‘ç«äº‰ï¼Ÿcache.go é‡Œé¢æœ‰ä¸ªé” sync.RWMutexï¼Œä½†å¹¶æ²¡æœ‰è§£å†³é—®é¢˜ã€‚è€Œä¸”è¿™é”ä¸æ˜¯å¤šä¸ª api å®¹å™¨é—´çš„åˆ†å¸ƒå¼é”å§ï¼Ÿ 

![monocular-restart-2](/images/2018/monocular-restart-2.png)

ä¸Šé¢ä¸¤ä¸ªAPI server æ˜¾ç¤º probe å¤±è´¥ï¼Œå› ä¸ºå®ƒåœ¨æ‹‰æ•°æ®ã€‚é‚£ k8s ä¸ºä»€ä¹ˆä¸æŠŠå®ƒç»™ kill äº†å‘¢ï¼Ÿå¥½åƒè™½ç„¶ä¼š probe failedï¼Œä½†æ˜¯ container è¿˜æ˜¯ä¸€ç›´è¿è¡Œã€‚è¿˜æ˜¯è™½ç„¶ kill ä½†æ˜¯ç´¢å¼•å·²ç»å»ºå¥½å°±ä¸ç”¨é‡æ–°åˆ›å»ºï¼Ÿæœç„¶ï¼Œä¸€ä¼šå°± CrashLoopBackOff äº†ã€‚

ä¸ºä»€ä¹ˆæ²¡æœ‰å“åº”/healthz å‘¢ï¼Ÿéš¾é“ goroutine æ²¡æœ‰åˆ‡æ¢ï¼Ÿ[One goroutine trapï¼šwhen would Goroutine context switch?](https://medium.com/@genchilu/one-goroutine-trap-when-would-goroutine-context-switch-f91b9a20b8e8) Go é‡Œé¢çš„å¹¶è¡Œå¹¶éå¤šçº¿ç¨‹ï¼Œè€Œæ˜¯ä¸€ä¸ªçº¿ç¨‹é‡Œé¢çš„è½®æ¢è¿è¡Œï¼Œåªæœ‰å½“ IO æ“ä½œçš„æ—¶å€™æ‰ä¼šåˆ‡æ¢ï¼Œæ¯”å¦‚ http, print ç­‰ç­‰ã€‚ 

æ‰‹å·¥å†™äº†ä¸ª /refresh rest apiï¼Œdisable foreground repository refresh so I can invoke my REST in clear env. Soon I found when I invoke /refresh, pod just restart!!! So kubelet didnâ€™t know reason and just know it was dead. In dmesg: 
```
[147701.430156] SELinux: mount invalid.  Same superblock, different security settings for (dev mqueue, type mqueue)
[147913.262269] monocular invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=-998
[147913.264347] monocular cpuset=docker-5d3a4c6e75105b3ef0bdfd215ebec12e9a2d32341f369e75125954cb3eeed903.scope mems_allowed=0
[147913.266552] CPU: 0 PID: 26049 Comm: monocular Tainted: G               ------------ T 3.10.0-693.11.6.el7.x86_64 #1
[147913.268829] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
[147913.270941] Call Trace:
[147913.271944]  [<ffffffff816a5ea1>] dump_stack+0x19/0x1b
[147913.273406]  [<ffffffff816a1296>] dump_header+0x90/0x229
[147913.274871]  [<ffffffff816b9bff>] ? apic_timer_interrupt+0xff/0x240
[147913.276481]  [<ffffffff81187be6>] ? find_lock_task_mm+0x56/0xc0
[147913.278070]  [<ffffffff811f34c8>] ? try_get_mem_cgroup_from_mm+0x28/0x60
[147913.279611]  [<ffffffff81188094>] oom_kill_process+0x254/0x3d0
[147913.281107]  [<ffffffff811f71e6>] mem_cgroup_oom_synchronize+0x546/0x570
[147913.282787]  [<ffffffff811f6660>] ? mem_cgroup_charge_common+0xc0/0xc0
[147913.284431]  [<ffffffff81188924>] pagefault_out_of_memory+0x14/0x90
[147913.286000]  [<ffffffff8169f65e>] mm_fault_error+0x68/0x12b
[147913.287382]  [<ffffffff816b3a21>] __do_page_fault+0x391/0x450
[147913.288920]  [<ffffffff816b3bc6>] trace_do_page_fault+0x56/0x150
[147913.290461]  [<ffffffff816b325a>] do_async_page_fault+0x1a/0xd0
[147913.291854]  [<ffffffff816af928>] async_page_fault+0x28/0x30
[147913.293325] Task in /kubepods.slice/kubepods-podbee65b57_7e71_11e8_9ced_d24398b14524.slice/docker-5d3a4c6e75105b3ef0bdfd215ebec12e9a2d32341f369e75125954cb3eeed903.scope killed as a result of limit of /kubepods.slice/kubepods-podbee65b57_7e71_11e8_9ced_d24398b14524.slice
[147913.298709] memory: usage 233472kB, limit 233472kB, failcnt 3028
[147913.300052] memory+swap: usage 233472kB, limit 9007199254740988kB, failcnt 0
[147913.301744] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0
[147913.303218] Memory cgroup stats for /kubepods.slice/kubepods-podbee65b57_7e71_11e8_9ced_d24398b14524.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB
[147913.447142] Memory cgroup stats for /kubepods.slice/kubepods-podbee65b57_7e71_11e8_9ced_d24398b14524.slice/docker-bf8a40c68a97ea71175febbf2551262a881b3dd519b37a31e8e2837c2f743531.scope: cache:0KB rss:44KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB active_file:0KB unevictable:0KB
[147913.492743] Memory cgroup stats for /kubepods.slice/kubepods-podbee65b57_7e71_11e8_9ced_d24398b14524.slice/docker-5d3a4c6e75105b3ef0bdfd215ebec12e9a2d32341f369e75125954cb3eeed903.scope: cache:0KB rss:233428KB rss_huge:2048KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:233428KB inactive_file:0KB active_file:0KB unevictable:0KB
[147913.544835] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name
[147913.546739] [24389]     0 24389      253        1       4        0          -998 pause
[147913.548605] [24518]     0 24518    66805    60921     134        0          -998 monocular
[147913.550548] Memory cgroup out of memory: Kill process 26074 (monocular) score 54 or sacrifice child
[147913.552593] Killed process 24518 (monocular) total-vm:267220kB, anon-rss:232620kB, file-rss:11064kB, shmem-rss:0kB
[147919.170462] SELinux: mount invalid.  Same superblock, different security settings for (dev mqueue, type mqueue)
```

But I found log in EFK:  
```
â€‹    kube-state-metrics The expected resources are &{map[cpu:{0.106000000 DecimalSI} memory:{251658240.000000000 BinarySI}] map[cpu:{0.106000000 DecimalSI} memory:{251658240.000000000 BinarySI}]} 
â€‹    kube-state-metrics Resources are within the expected limits. 
```
Then Edit the monocular Deployment. Default is : 
```
â€‹            "resources": { 
â€‹              "limits": { 
â€‹                "cpu": "100m", 
â€‹                "memory": "128Mi" 
â€‹              }, 
â€‹              "requests": { 
â€‹                "cpu": "100m", 
â€‹                "memory": "128Mi" 
â€‹              } 
â€‹            } 
```
YAML define it? Or k8s default? I change to 228M. Still OOM. Change all(request & limit since I didnâ€™t know difference) to 428M, same Issue. Keep request to 428. Change limit to 928. OK. 

I change chartâ€™s buffered channel to 4. Original is len(charts). Set memory to 228. No OOM. But get error: 
```
  Readiness probe failed: Get http://10.40.0.44:8081/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers) 
  The node was low on resource: imagefs. 
  Container runtime did not kill the pod within specified grace period. 
```
And k8s create the third pod at this time! So the cluster think this time need to create new pods. RS setting is 2. 
```
â€‹    /dev/vda1                 8.0G      4.9G      3.1G  62% /monocular 
â€‹    /dev/vda1                 8.0G      4.9G      3.1G  62% /dev/termination-log 
â€‹    /dev/vda1                 8.0G      4.9G      3.1G  62% /monocular/config 
```
Oh, 8G is just my KVM virtual disk size. [Try to set](https://github.com/kubernetes/kubernetes/issues/63126) â€œsizeLimit": â€œ1Giâ€ of emptyDir. The download size > 1GB? Should not possible. Same issue. Later after downloaded all, I checked and found the whole size is just 240MB. 

Add a 20G vm k8s-4. How to make the monocular-api run the k8s-4? [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/): 
```
â€‹    "nodeSelector": { 
â€‹        "kubernetes.io/hostname": "k8s-4" 
â€‹    } 
```
Test again. After reboot 2 times, service OK. dmesg didnâ€™t show oom. Didnâ€™t know why reboot. Now send /refresh REST call again, to see how it update when all charts has been cached. Very fast. No error. Why it happened so frequently in past? Need restore(image & config) and verify again. 

ch := make(chan chanItem, 4 /*len(charts)*/). Charts are 2000+. I change to 4. Later I found that "Counting semaphore, 25 downloads max in parallelâ€. So my changes is useless? 

For frontend refresh, need as a Go routine. After do this, now a new pod just 2 restart to download all charts. 

Q: When do Refresh(), didnâ€™t inset new data to DB? 

åœ¨prerenderé‡Œé¢è¿›å…¥å‘½ä»¤è¡Œï¼š 
```
root@erstwhile-deer-monocular-prerender-d556b95c9-k8n6x:/app# ps aux                                                 
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND 
root         1  0.0  0.0   4212   340 ?        Ss   May31   0:06 tini -- node server.js 
root        12  0.0  0.6 886444 26076 ?        Sl   May31   0:00 node server.js 
root        18  0.0  0.6 919228 26444 ?        Sl   May31   0:00 /opt/bitnami/node/bin/node /app/server.js 
root        28  0.0  0.8 1321720 33960 ?       Sl   May31   0:09 /app/node_modules/phantomjs-prebuilt/lib/phantom/bin/phantomjs --load-im 
root        42  0.0  0.0  20260  1960 ?        Ss+  15:46   0:00 bash 
root        46  0.0  0.0  20260  1952 ?        Ss   15:46   0:00 bash 
root        49  0.0  0.0  17492  1144 ?        R+   15:46   0:00 ps aux 
```
All [Tini](https://github.com/krallin/tini) does is spawn a single child (Tini is meant to be run in a container), and wait for it to exit all the while reaping zombies and performing signal forwarding. If you are using Docker 1.13 or greater, Tini is included in Docker itself. ä¾èµ–å…³ç³»ä¸ºï¼š 
```
â€‹    bitnami/node -> bitnami/minideb -> [tiny](https://github.com/bitnami/minideb-extras)  
```
è¿™æ ·è¿½è¸ªä¸€ä¸ªå®¹å™¨æœ‰ç‚¹ç¹çï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•ã€‚ 

"PhantomJSçš„åŠŸèƒ½ï¼Œå°±æ˜¯æä¾›ä¸€ä¸ªæµè§ˆå™¨ç¯å¢ƒçš„å‘½ä»¤è¡Œæ¥å£ï¼Œä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€ä¸ªâ€œè™šæ‹Ÿæµè§ˆå™¨â€ï¼Œé™¤äº†ä¸èƒ½æµè§ˆï¼Œå…¶ä»–ä¸æ­£å¸¸æµè§ˆå™¨ä¸€æ ·ã€‚"ä»–ç”¨è¿™ä¸ªåšå•¥ï¼Ÿ 

root@erstwhile-deer-monocular-prerender-d556b95c9-k8n6x:/app# cat README.md 

Prerender Service [![Stories in Ready](https://badge.waffle.io/prerender/prerender.png?label=ready&title=Ready)](https://waffle.io/prerender/prerender) 

Google, Facebook, Twitter, Yahoo, and Bing are constantly trying to view your website... but they don't execute javascript. That's why we built Prerender. Prerender is perfect for AngularJS SEO, BackboneJS SEO, EmberJS SEO, and any other javascript framework. 

Behind the scenes, Prerender is a node server from [prerender.io](http://prerender.io) that uses phantomjs to create static HTML out of a javascript page. We host this as a service at [prerender.io](http://prerender.io) but we also open sourced it because we believe basic SEO is a right, not a privilege! 

It should be used in conjunction with [these middleware libraries](#middleware) to serve the prerendered HTML to crawlers for SEO. Get started in two lines of code using [Rails](https://github.com/prerender/prerender_rails) or [Node](https://github.com/prerender/prerender-node). 

<https://prerender.io/> æœ‰è°ä¼šçˆ¬è¿™ä¸ªmonocularçš„ç«™ç‚¹å‘¢? 

æœ‰æ—¶å€™ä¸èƒ½å®‰è£…ï¼Œè¯´æ˜¯è¿™ä¸ªå·²ç»æœ‰äº†ï¼š 
```
helm ls --all  // åˆ—å‡ºå…¨éƒ¨ 
helm del --purge monocular 
```
ä¸ºä»€ä¹ˆç•™ä¸‹ cacheï¼Ÿè¿™ä¸ªæœ‰ä»€ä¹ˆç”¨å‘¢ï¼Ÿcache = chart + valueï¼Œä¿å­˜äº† valueã€‚ 
```
helm del $(helm ls  --short --deleted) --purge // purge æ¸…ç©ºå·²ç»åˆ é™¤çš„ 
```
### inside

[How Helm Uses ConfigMaps to Store Data](http://technosophos.com/2017/03/23/how-helm-uses-configmaps-to-store-data.html)ï¼ŒåŸæ¥è¿™ä¹ˆåœŸï¼Ÿä¸ºä»€ä¹ˆä¸ç”¨ä¸“é—¨å­˜å‚¨ï¼Ÿå› ä¸ºä½œä¸ºä¸€ä¸ªåŒ…ç®¡ç†å™¨ï¼Œå¦‚æœä¾èµ–å¤ªå¤šå°±æ˜¯å…ˆæœ‰é¸¡è¿˜æ˜¯å…ˆæœ‰è›‹çš„é—®é¢˜äº†ã€‚å› ä¸º etcd å¤§å°é™åˆ¶ï¼Œæ¯ä¸ª ConfigMap è¢«é™åˆ¶åœ¨1M ä»¥ä¸‹ï¼Œæ‰€ä»¥é‡Œé¢æ•°æ®åšæˆäº† base64çš„ã€‚å¦‚æ­¤ä¸€æ¥ etcd æœ‰è¢«æåè¶‹åŠ¿å•Šã€‚kubectl get configmap -n kube-system -l "OWNER=TILLERâ€ è¿™æ ·æ˜¾ç¤ºæ‰€æœ‰çš„ releaseï¼Œè¿™æ˜¯ç”¨ label æ¥ç»„å»ºå…³ç³»å•Šã€‚æ¯ä¸ª release = Chart + Valuesï¼Œè¿™æ ·æ¯”å¦‚æ ¹æ® release name å°±èƒ½è®¡ç®—å‡º release æ‰€æœ‰çš„åˆ›å»ºå‡ºæ¥çš„èµ„æºï¼Ÿè¿™ä¸ªç®—æ³•æœ‰ç‚¹å°å¤æ‚å§ã€‚

![helm-inside](/images/2018/helm-inside.png)

<https://blog.openshift.com/wp-content/uploads/Monocular-on-OpenShift.pdf>  

Helm åªæ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼ŒTiller æ˜¯ä¸€ä¸ªæœåŠ¡ï¼Œä½†æ˜¯ä¸€èˆ¬æœ‰æ²¡æœ‰åŠæ³•é€šè¿‡ REST æ¥è®¿é—®ä¹‹ï¼Œåªèƒ½é€šè¿‡ Helm Clientã€‚<https://banzaicloud.com/blog/helm-rest-api/> 3rd å®ç°ã€‚ 

<https://github.com/helm/community/blob/master/helm-v3/000-helm-v3.md> ä¸åˆ†å®¢æˆ·ç«¯ / æœåŠ¡å™¨ç«¯çš„å•æœåŠ¡æ¶æ„â€”â€”æ— éœ€ Tillerï¼Œæ•°æ®å­˜åœ¨ CRD é‡Œé¢ã€‚åªè¦ä¿æŒç®€å•ï¼Œå…¶è¿˜æ˜¯èƒ½å’Œ operator ç«äº‰çš„ã€‚[æ•æˆ‘ç›´è¨€ï¼Œå¯¹Helmå¤§å®¶è¿˜æ˜¯è¦ä¸‰æ€è€Œåç”¨](https://mp.weixin.qq.com/s/V-ahe76SjEd1LGIE2Vyx4g)ã€‚ 

[åˆæ¢äº‘åŸç”Ÿåº”ç”¨ç®¡ç†ï¼ˆäºŒï¼‰: ä¸ºä»€ä¹ˆä½ å¿…é¡»å°½å¿«è½¬å‘ Helm v3](https://www.infoq.cn/article/JL6H7bgTy7X*1usthCP0)

### æ·»åŠ å‚æ•°é…ç½®åŠŸèƒ½

Roadmap é‡Œé¢è¯´è¦åˆ° 1.0 æ‰ä¼šæœ‰è¿™ä¸ªåŠŸèƒ½ Configure values on install 

é¦–å…ˆè¦è·å¾— values.yaml å†…å®¹ã€‚Release æ¯”å¦‚å®‰è£…åº”ç”¨ï¼Œæ˜¯åœ¨ api/handlers/releases/releases.go é‡Œé¢åšçš„ï¼Œå…¶è°ƒç”¨ api/data/helm/client/client.goï¼Œåè€…å°±æ˜¯ä¸Šé¢å›¾ä¸­çš„ Helm Clientï¼Œé€šè¿‡ gRPC å¯ä»¥ç›´æ¥å®‰è£…ä¸€ä¸ªæœ¬åœ°çš„ Chart tar å®‰è£…åŒ…ã€‚æœ€ç»ˆé€šè¿‡ helm go client æ¥ gRPC è°ƒç”¨ï¼ˆapi/data/helm/releases/releases.goï¼‰ï¼Œå…¶ä»–è¯­è¨€å°±æ²¡æ³•äº†ã€‚ 

å¯¹äº Chartï¼Œåˆ™æ˜¯ç›´æ¥è®¿é—®(api/handlers/charts/charts.go)æœ¬åœ°çš„ Chart tar å†…å®¹ï¼Œå…¶è°ƒç”¨ chartsImplementation ä¹Ÿå°±æ˜¯ api/data/cache/cache.goï¼Œåè€…ä» MongoDB è·å–æ•°æ®ã€‚ä»£ç å†™çš„çœ‹ä¸æ‡‚ï¼Œæœ‰è¿™ä¹ˆå¤æ‚ä¹ˆï¼Ÿç›´æ¥è®¿é—® <https://vm1/api/helm/api/assets/stable/anchore-engine/0.2.0/README.md> è¿™ä¸ªå¯ä»¥æœ‰å€¼ï¼Œä½†æ˜¯æ”¹ä¸º values.yaml æ²¡æœ‰ï¼Œè¿›å…¥ pod cmdï¼Œç›¸åº”ç›®å½•ä¸‹ç¡®å®æ²¡æœ‰ï¼ŒREADME.md æœ‰ï¼ŒåŸæ¥è¿™ä¸ªæ˜¯å…¶ç‰¹æ„ä» tar é‡Œé¢è§£å‹å‡ºæ¥çš„ã€‚chart meta ä¼¼ä¹ä¸æ˜¯ä» tar é‡Œé¢è§£å‹è·å–è€Œæ˜¯ä» chart repository index ç›´æ¥è·å–ï¼Ÿchart_package_helper.go é‡Œé¢æœ‰ var filesToKeep = []string{"README.mdâ€}ï¼Œè¿™è¡¨é¢åªè§£å‹è¿™ä¸ªæ–‡ä»¶ã€‚ç„¶å README.md è¿˜æœ‰å›¾æ ‡æ–‡ä»¶ä¼šä½œä¸º negroni çš„é™æ€æœåŠ¡å™¨çš„èµ„æºï¼Œè¿˜å•ç‹¬ç”¨ä¸€ä¸ªç¬¬ä¸‰æ–¹åº“ï¼Ÿæ‡’æƒ°ã€‚ 

src/api/vendor/k8s.io/helm/pkg/helm/client.go è¿™ä¸ªä¼šæœ€ç»ˆè°ƒç”¨ tiller serviceï¼Œservice å®šä¹‰ <https://github.com/helm/helm/blob/master/_proto/hapi/services/tiller.proto>  

gGPC hello world <https://grpc.io/docs/quickstart/go.html>  

å…ˆå®šä¹‰ proto æ¥å£å®šä¹‰æ–‡ä»¶ï¼ŒåŒ…å«æ¥å£æ–¹æ³•åã€è¾“å…¥æ ¼å¼å’Œè¾“å‡ºæ ¼å¼ã€‚ç„¶åæ ¹æ®å…¶ç”Ÿæˆä¸€ä¸ª .pb.go æ–‡ä»¶ï¼Œç±»ä¼¼ Java RMI stubã€‚ç„¶åä¾èµ–è¿™ä¸ªæ–‡ä»¶ï¼Œserver å®ç°æ¥å£ï¼Œclient è°ƒç”¨æ¥å£ã€‚ 

å·®ä¸å¤šäº†ï¼Œå¼€å§‹å†™ä»£ç ã€‚å…ˆç¡®å®šå¦‚ä½•æµ‹è¯•ã€‚å¦‚æœæ¯æ¬¡ä¿®æ”¹éƒ½æä¸€ä¸ª Docker Imageï¼Œè¿™ä¸ªå€’è¿˜å¥½ï¼Œä¸»è¦æ˜¯ monocular-api åˆå§‹åŒ–ä¼šæ‹‰å¾ˆå¤šé•œåƒï¼Œè¿™ä¸ªæœ‰ç‚¹éº»çƒ¦ï¼Œç²¾ç®€å…¶ repoï¼Ÿå¦å¤–ä¸€ç§åŠæ³•æ˜¯ gRPC ç›´è¿ Tillerï¼ŒTiller æ˜¯ ClusterIPï¼Œæ”¹ NodePort ä¸å¥½ï¼Œåæ¥æ‰¾åˆ°ä¸€ç§åŠæ³•ï¼š 
```
â€‹    kubectl port-forward pod-46r7w 27017:27017 -n kube-system
â€‹    kubectl port-forward service/monocular-api 27017:27017 -n kube-system è¿™ç§æ›´å¥½
```
Kubectl proxy åªèƒ½ HTTP å§ï¼Œè¿™ç§å¯ä»¥æ˜¯ TCPï¼ŒèŒƒå›´æ›´å¹¿ã€‚ 

**ä½†æ˜¯è¿™ç§æ–¹æ³•å› ä¸ºå¼€å‘ç¯å¢ƒæ²¡æœ‰è®¾ç½®DNS ä¸º k8s DNSï¼Œä»£ç é‡Œé¢å¿…é¡»æ‰‹å·¥åˆ‡æ¢ localhost å’Œ service hostnameï¼Œæ‰€ä»¥æš‚æ—¶åªèƒ½ä¿®æ”¹ /etc/hosts è¿™ç§ç¬¨æ–¹æ³•ï¼Œæˆ–è€…é€šè¿‡å‚æ•°ä¼ é€’ï¼šé»˜è®¤æ˜¯service nameï¼Œå¯ä»¥æ¥å—å‚æ•°ä¸º localhost**ã€‚ 

<https://github.com/helm/helm/blob/master/pkg/helm/helm_test.go> è¿™ä¸ªæœ‰å±•ç¤ºå¦‚ä½•æ“ä½œ helm apiï¼Œæˆ‘è¯•è¿‡å‘ç°å…¶æ¥å—çš„å­—ç¬¦ä¸²å…¶å®ä¸º json æ ¼å¼ï¼Œä¸æ˜¯ â€œaâ€=â€œaâ€, â€œbâ€=â€œbâ€ è¿™ç§ã€‚ 

æ·»åŠ å‚æ•°è¦ä¿®æ”¹æ¥å£å®šä¹‰ CreateReleaseBodyï¼Œè¿™ä¸ªæ˜¯ swagger äº§ç”Ÿå‡ºæ¥çš„ï¼Œä¿®æ”¹ swagger.xml ç„¶åè¿è¡Œ make swagger-serverstubï¼Œè¿™ä¸ªä¹Ÿæ˜¯ä¸ª docker è¿è¡Œï¼Œå¼ºå•Šï¼Œä¼šç”Ÿæˆä¸€å †æ–‡ä»¶ã€‚ 

Hashmap swagger å®šä¹‰ <https://swagger.io/docs/specification/data-models/dictionaries/>  

<https://kubernetes-charts.storage.googleapis.com/> è¿™é‡Œä¸‹è½½ä¸€ä¸ª stable chart tgz æ–‡ä»¶ä½œä¸ºæµ‹è¯•ä¾‹å­ 

æœ‰ä¸ªé—®é¢˜ï¼šä¼¼ä¹å¸¦ values æ—¶æ¯æ¬¡åˆ›å»ºçš„éƒ½æ˜¯åŒæ ·çš„ release nameï¼ˆUUID ä½¿ç”¨å®Œäº†ï¼Ÿï¼‰ï¼Œç»“æœå¯¼è‡´è¿”å›æˆåŠŸï¼Œä½†æ˜¯ helm ls çœ‹ä¸åˆ°ã€‚ 

[Proposal: Monocular 1.0 - improvements and redefining project focus](https://github.com/helm/community/blob/master/proposals/monocular-1.0-improvements.md) è¿™ä¸ªç‰ˆæœ¬ä¼šè§£å†³å¤§å¤šæ•°çš„é—®é¢˜ï¼Œä½¿ç”¨ Jobs æ¥åŒæ­¥ï¼Œåˆ‡åˆ†æ¨¡å—ï¼Œå»æ‰å®‰è£…åº”ç”¨è¿™ç§ in-cluster çš„åŠŸèƒ½ï¼Œæ¨èç”¨ kubeapp ä»£æ›¿ï¼Œchart ä»“åº“ç”¨ CRD ç®¡ç†ï¼ˆè¿™ç§éœ€è¦ä¿®æ”¹ååŠ¨æ€åŠ è½½çš„éƒ½é€‚åˆ CRDï¼‰ã€‚ 

å†™ Helm æ—¶å€™å¦‚æœè¦æ·»åŠ ä¸€ä¸ªå·²æœ‰çš„ Chartï¼Œæ¯”å¦‚ MySqlï¼Œå¦‚ä½•ç›´æ¥è°ƒç”¨æˆ–è€…æ·»åŠ å‘¢ï¼Ÿå¦‚æœèƒ½ç”¨ Juju å±•ç¤ºå‡ºæ¥æ›´ä½³ã€‚ 

monocular/deployment/monocular/requirements.yaml é‡Œé¢åŒ…å«ï¼š 
```
dependencies:
- name: mongodb
  version: 0.4.17
  repository: https://kubernetes-charts.storage.googleapis.com
```
è¿™ä¸ªè®¾è®¡å¯ä»¥ã€‚<https://github.com/kubernetes/helm/blob/master/docs/helm/helm_dependency.md> ä¾èµ–ä¹Ÿå¯ä»¥ä¸ºæœ¬åœ°çš„ç›®å½•ã€‚<https://github.com/kubernetes/helm/blob/master/docs/charts.md> è®²äº†ä¸å°‘ dependencyã€‚ 

<https://docs.bitnami.com/kubernetes/how-to/create-your-first-helm-chart/>

### Chartmuseum

<https://github.com/helm/chartmuseum/pull/53> Multi-Tenancy POC 

Chart å®‰è£…çš„æ—¶å€™é»˜è®¤æ˜¯ DISABLE_API=trueï¼Œè¿™å¯¼è‡´ api æ— æ³•è®¿é—®ï¼Œå¯èƒ½å®‰å…¨çš„è€ƒè™‘ã€‚ 

<https://github.com/caicloud/helm-registry> æ‰äº‘çš„ 

ä¸Šä¼ çš„ tgz çš„ç›®å½•åå¿…é¡»å’Œ name ä¿æŒä¸€è‡´ï¼Œå¦åˆ™ monocular ç›´æ¥åˆ° /tmp/chart187671202/aerospike-fan-zmy/README.md è¿™ç§ç›®å½•ä¸‹é¢æ‰¾ï¼Œä¼šæ‰¾ä¸åˆ°ã€‚ 
```
tar -cvzf <name of tarball>.tgz /path/to/source/folder 
```
å¦‚ä½•åˆ¤å®šè¿™ç§ä¸Šä¼ çš„æ˜¯å¯ä»¥åˆ é™¤çš„ chart å‘¢ï¼Ÿchartmuseum æ— æ³•æ›´æ”¹ chart keywordï¼Œè¦æ±‚ç”¨æˆ·çš„ tgz/Chart.yaml åŒ…å«æŸäº› keyword è‚¯å®šä¸ç°å®ã€‚æš‚æ—¶ä½¿ç”¨ repo.nameã€‚ 

ä¸Šä¼ å’Œåˆ é™¤æ•°æ®åå¦‚ä½•åŒæ­¥ monocular å‘¢ï¼Ÿå…¶æœ¬æ¥æœ‰ä¸ª /charts/{repo}/{chartName}/refresh REST APIï¼Œéœ€è¦æ”¹é€ æˆå½“ä¸åœ¨æ˜¯åˆ é™¤å·²æœ‰çš„ã€‚ä½†æ˜¯ chartmuseum ä¸Šä¼ æ¥å£å¹¶ä¸è¿”å› chart ä¿¡æ¯ï¼Œæ‰€ä»¥æ²¡æœ‰åŠæ³•æŒ‡å®šåˆ·æ–°ã€‚ 

å³ä¾¿è¿™ç§ REST åˆ·æ–°å¯ä»¥ç”¨ï¼Œä¹Ÿä¸æ˜¯å¥½çš„åŠæ³•ï¼šå…ˆå‘é€åˆ›å»º APIï¼Œç„¶åå‘é€åˆ·æ–° APIï¼Œè€Œè¿™ä¸ªç”±å‰ç«¯ä¿è¯äº‹åŠ¡å®Œæ•´æ€§å’Œå¯é æ€§ï¼Œå¦‚æœåç«¯åˆ™å°é¢˜å¤§åšäº†ã€‚ç›‘æ§çš„åœ°æ–¹ä¹Ÿæ˜¯ï¼šæ›´æ–° configmapï¼Œç„¶å reload ç›‘æ§æœåŠ¡ã€‚ä¼¼ä¹ MQ çš„è§£å†³åŠæ³•æ›´å¯é ä¸€ç‚¹ã€‚æˆ–è€…è¿˜æ˜¯ CRD æ¥ä¸€å¥—ï¼Ÿè¿™ä¸ªæ˜¯å¾®æœåŠ¡å¯¼è‡´çš„æ–°é—®é¢˜ï¼Œå’Œåˆ†å¸ƒå¼äº‹åŠ¡æœ‰å…³ç³»ã€‚ 

[0.8.0 æ›´æ–°](https://github.com/helm/chartmuseum/releases/tag/v0.8.0) å¸¦æ¥äº† auth tokenæ”¯æŒï¼Œè¿˜æœ‰[chartmuseum/ui](https://github.com/chartmuseum/ui)ï¼Œå¯ä»¥ä¸Šä¼ ï¼Œè·Ÿæˆ‘ä»¬åšçš„å¾ˆåƒå•ŠğŸ˜„ [beego](https://beego.me/) - The web framework usedï¼Œå›½äººäº§å“ï¼Œä¸€ç§æ¨¡æ¿è¯­è¨€ï¼Œç®€å•çš„ä½¿ç”¨äº† jQueryã€‚

é…ç½®çº¦æŸï¼š 

1. ä»“åº“ name å¿…é¡»æ˜¯ chartmuseumï¼Œå‰ç«¯ä¾æ® repo.name æ¥åˆ¤æ–­æ˜¯ä¸æ˜¯ç”¨æˆ·ä¸Šä¼ çš„
2. Helm è™½ç„¶ä¸åŠ Operator å¼ºå¤§ï¼Œä½†å¯¹äº k8s æ–°æ‰‹æ¥è¯´å¾ˆæœ‰å¸®åŠ©ï¼Œå› ä¸ºé©¬ä¸Šæœ‰äº† PaaS å¹³å°çš„æ„Ÿè§‰ï¼Œå®‰è£…åˆ é™¤ä¸€ä¸ªåº”ç”¨å¾ˆæ–¹ä¾¿ï¼Œå¯¹å¼€å‘å¸®åŠ©èƒœè¿‡è¿ç»´ã€‚

